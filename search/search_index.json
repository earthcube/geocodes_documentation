{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Geocodes Documentation","text":""},{"location":"#starting-points","title":"Starting points:","text":""},{"location":"#active-sites","title":"Active Sites","text":"<ul> <li>Geocodes</li> <li>Developement</li> <li>Community</li> <li>AddTo JSON-LD Editor</li> </ul>"},{"location":"#science-on-schema-metadata","title":"Science on Schema Metadata","text":"<ul> <li>Earthcube Documentation</li> <li>ESIP Guidance</li> <li>Earthcube Examples and Guidance</li> </ul>"},{"location":"#loading-and-indexing","title":"Loading and indexing","text":"<ul> <li>Gleaner- Harvesting JSONLD</li> <li>Nabu -- Converting to Graph</li> </ul>"},{"location":"#services-infrastructure-aka-production","title":"Services Infrastructure, aka Production","text":"<ul> <li>Services Infrastructure</li> </ul>"},{"location":"#repositories","title":"Repositories:","text":"<ul> <li>Search<ul> <li>This needs to become the start of documentation for facesearch</li> </ul> </li> <li>Earthcube Utilties</li> <li>JSON-LD Dataset Editor</li> </ul>"},{"location":"developers/addingdocumentation/","title":"notes on adding documentation","text":"<p>This Repository is based on MkDocs and uses the multirepo plugin The mkdocs.yml at the top level of the repository</p> <p>To add a new section, you add an import to the nav section based on: https://github.com/jdoiro3/mkdocs-multirepo-plugin#setup</p> <p>In your repo, you add a mkdocs.yml</p>"},{"location":"developers/addingdocumentation/#markup-cheat-sheet","title":"Markup cheat sheet","text":"<p>For good examples and editable pages see Geocodes</p> <p>Note</p> <p>We are using mkdocs, so admonitions and pymarkdown extensions but the pymarkdown stuff needs to be ebabled: https://pypi.org/project/mkdocs-callouts/</p> <p>https://squidfunk.github.io/mkdocs-material/reference/admonitions/#configuration</p>"},{"location":"developers/addingdocumentation/#mkdocs-references","title":"MKDOCS References","text":"<p>Mkdocs on github how tos:</p> <ul> <li>example<ul> <li>using this repo as starting point</li> </ul> </li> <li>https://blog.elmah.io/deploying-a-mkdocs-documentation-site-with-github-actions/</li> <li>https://blog.elmah.io/hosting-a-mkdocs-driven-documentation-site-on-github-pages/</li> </ul>"},{"location":"developers/communiity-info/","title":"About the NSF Earthcube Community Infrastructure","text":""},{"location":"developers/contributing/","title":"Contribution Guidelines","text":""},{"location":"examples/geocodes/","title":"Geocodes Instance","text":"Who Where What source production https://geocodes.earthcube.org/ Wifire Data Catalog Sheet <ul> <li> <p>Production</p> </li> <li> <p>Running on : Portainer.geocodes-1.earthcube.org </p> </li> <li>openstack: SDSC</li> </ul>"},{"location":"examples/community/","title":"Tennants and community examples","text":"Who Where What source wifire https://geocodes.wifire-data.sdsc.edu/ Wifire Data Catalog source"},{"location":"examples/community/#other-science-on-schema-users","title":"Other Science on Schema Users","text":""},{"location":"examples/developement/","title":"Developer Geocodes Instance","text":"Who Where What source geocodes https://geocodes.geocodes-dev.earthcube.org/ Staging/Test sheet <p>Geocodes:</p> <ul> <li>hosted on portainer.geocodes-dev.earthcube.org</li> <li>openstack: SDSC</li> </ul>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/","title":"Geocodes Science on Schema Metadata","text":""},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/#pages","title":"Pages:","text":"<ul> <li>Metadata on the Web</li> <li>Search Profile</li> <li>GeoCODES Dataset Metadata PDF</li> <li>GeoCODES Dataset-ECRR Metadata PDF</li> </ul>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/","title":"Considerations for publishing metadata for web harvesting.","text":""},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/#on-the-server-side","title":"On the server side:","text":"<p>The following are items of use to the GeoCODES harvest and indexing process.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/#sitemap-or-sitemap-index","title":"Sitemap or Sitemap index","text":"<p>You can leverage a sitemap (up to 50K resources) or a sitemap index to go beyond that.  An index will point to multiple sitemaps.  Note, you can use the index pattern to point to different resource types too.  For example, a sitemap index can point to various sitemaps for tools/software, people, dataset, etc.  This is also completely compatible with Google and other commercial site index systems.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/#leverage-sitemap-with-dates","title":"Leverage Sitemap with dates","text":"<p>Adding the dates node and updating it will be useful as we move to an automated indexing pattern with GeoCODES.   If present we will use this to guide the architecture to do faster and less burdensome indexing.   Sites that do not provide this date node will need to be periodically completely indexed to ensure updates to metadata records are found and indexed.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/#content-negotiate","title":"Content negotiate","text":"<p>Though note required, we do support content negotiation for accessing your resources.  This can be both faster and less stressful on servers.  You do not need to provide any guidance back to EarthCube if you implement this.</p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/MetadataPublicationOnTheWeb/#headless-vs-static","title":"Headless vs static","text":"<p>Both static and dynamic inclusion of the JSON-LD data graph into the page DOM is supported.  EarthCube (and all commercial indexes) support this \"headless\" rendering of pages for indexing.   At present we do not have a good standards based way to communicate this to EarthCube.  We are addressing approaches, both in the indexing code and via the web architecture to do this.   We will provide further guidance, if necessary, as it develops on this point.   Providers should feel free to select either approach as it addresses their own criteria.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/collections-ro-crate/","title":"Collections","text":"<p>A collection is intended to be a way to pass information to services</p> <p>We are proposing to use Research Object Crates Others: https://treecg.github.io/specification/#xyztiles</p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/collections-ro-crate/#thought-model","title":"Thought model:","text":"<ul> <li>collection</li> <li>queries<ul> <li>stored query that may be used</li> </ul> </li> <li>datsets<ul> <li>(link to query used to retrieve dataset?)</li> </ul> </li> <li>tools<ul> <li>dataset to input linkage, aak what url are we using in a dataset</li> </ul> </li> </ul>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/collections-ro-crate/#scenario","title":"Scenario:","text":"<ul> <li>search first (this is out present path)</li> <li>user queries, saves query, saves data from query, </li> <li>user creates collection, adds query</li> <li> <p>user adds datasets to collection</p> </li> <li> <p>tool/task first (from science perspective, this is the ideal path)</p> </li> <li>user searches for a tool to accomplish a task</li> <li>creates a collection, saves tool to collection</li> <li>for each input, a query is run to find datasets</li> <li>datasets saved to a tool input</li> </ul>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/collections-ro-crate/#examples","title":"Examples:","text":"<ul> <li>rocrate-placeholder.json is just a  file from the researchobject.org website</li> </ul>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/geocodesSearchProfile/","title":"ECO GeoCODES Profile","text":"<p>This document will evolve to provide guidance to groups on how to address elements of their metadata policy and procedures to address indexing by GeoCODES.  To provide some context we can look at other activities and see their relationships.   </p> <p></p> <ul> <li>Google Dev Guidance helps guide use in Google Dataset Search</li> <li>ESIP Science on Schema guides use of Type Dataset (https://schema.org/Dataset) </li> <li>EarthCube GeoCODES provides guidance on the application GeoCODES search</li> </ul> <p>An element of this is the leveraging of the graph to provide connections to other resources.  In particular the resources in the EarthCube Resource Registry graph.  However, links to other external resources like the EarthCube Throughput project are being developed. </p> <p>This work will be done in a way that can be applied to other groups (present and future) as well.  </p> <p>An example of the current approach for linking to the Resource Registry follows.  This is evolving at present as we explore how to connect this through to the user interface. </p> <p></p> <p>GeoCODES team will work to refine and publish this guidance along with machine workflows to validate JSON-LD data graphs with.  Likely via SHACL shape graphs.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/geocodesSearchProfile/#preview-of-these-recommendations-include","title":"Preview of these recommendations include:","text":""},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/geocodesSearchProfile/#on-the-server-side","title":"On the server side:","text":"<p>The following are items of use to the GeoCODES harvest and indexing process.  </p> <p>Sitemap or Sitemap index</p> <p>You can leverage a sitemap (up to 50K resources) or a sitemap index to go beyond that.  An index will point to multiple sitemaps.  Note, you can use the index pattern to point to different resource types too.  For example, a sitemap index can point to various sitemaps for tools/software, people, dataset, etc.  This is also completely compatible with Google and other commercial site index systems.  </p> <p>Leverage Sitemap with dates</p> <p>Adding the dates node and updating it will be useful as we move to an automated indexing pattern with GeoCODES.   If present we will use this to guide the architecture to do faster and less burdensome indexing.   Sites that do not provide this date node will need to be periodically completely indexed to ensure updates to metadata records are found and indexed.  </p> <p>Content negotiate</p> <p>Though note required, we do support content negotiation for accessing your resources.  This can be both faster and less stressful on servers.  You do not need to provide any guidance back to EarthCube if you implement this.</p> <p>Headless vs static</p> <p>Both static and dynamic inclusion of the JSON-LD data graph into the page DOM is supported.  EarthCube (and all commercial indexes) support this \"headless\" rendering of pages for indexing.   At present we do not have a good standards based way to communicate this to EarthCube.  We are addressing approaches, both in the indexing code and via the web architecture to do this.   We will provide further guidance, if necessary, as it develops on this point.   Providers should feel free to select either approach as it addresses their own criteria.  </p>"},{"location":"information-for-repositories-and-users/science-on-schema/earthcube-gudiance/geocodesSearchProfile/#in-the-graph","title":"In the graph:","text":"<p>The following are either important in the search UI or (in the case of variable measured) items we have talked about leveraging.   It is important to know that these are items used by the search application.  In many ways this is a subset of Science on Schema of importance to us in providing the GeoCODES search UI/UX.  </p> <p>ID</p> <p>When generating your JSON-LD be sure to include a top level @id that points to the metadata record of the resource you are describing.  This can be a DOI like a DataCite metadata record but it can also be the URL of the landing page hosting your JSON-LD and describing the resource.</p> <p>Descriptive text</p> <p>The initial search is focused mostly on text searches across the literal strings of the graph, then we conduct other graph connections as the query is processed into a set of results.   So, while it's important to publish the graph elements it is also important to provide descriptive text and keywords at every level of the JSON-LD graph.</p> <p>Distribution url</p> <p>Follow ESIP Science on Schema guidance here.  </p> <p>Ref: https://github.com/ESIPFed/science-on-schema.org/blob/master/guides/Dataset.md#distributions </p> <p>Encoding format</p> <p>Follow ESIP Science on Schema guidance here.  </p> <p>Ref: https://github.com/ESIPFed/science-on-schema.org/blob/master/guides/Dataset.md#distributions </p> <p>Variable measured </p> <p>This is a developing area and we will implement ESIP Science on Schema recommendations here.</p> <p>Spatial</p> <p>Many ways to do it, but GeoCODES may propose geosparql WKT approach</p>"},{"location":"developers/services-infrastructure/","title":"Documentation for the Geocodes Container Stack","text":"<pre><code>flowchart TB\nservices-- deployed by --&gt;portainer\ngeocodes-- deployed by  --&gt; portainer\ngleaner-- deployed by  --&gt; portainer\nfacetsearch-- routes --&gt; traefik\nfacetsearchservices-- routes--&gt;traefik\noss-- routes--&gt;traefik\ntriplestore-- routes --&gt; traefik\nsparqlgui-- routes --&gt; traefik\nsubgraph gleaner\nheadless\nend\nsubgraph geocodes\nfacetsearch--&gt;facetsearchservices\nend\nsubgraph services\noss[\"oss s3\"]\nsparqlgui\ntriplestore[\"graph -- triplestore\"]\nend\n\n        subgraph base\n           traefik&lt;-- routes --&gt;portainer\n        end\n\n</code></pre>"},{"location":"developers/services-infrastructure/#overview","title":"Overview:","text":"<ol> <li>Configure a base server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Intial setup of services and load data</li> <li>Setup Gepcodes UI containers,</li> </ol>"},{"location":"developers/services-infrastructure/#what-to-learn-from-deploying-the-stack-and-the-indexing-application","title":"What to learn from deploying the stack, and the indexing application","text":"<ul> <li>how to deploy containers</li> <li>how to run indexing using gleaner<ul> <li>initial setup to use a test daatase</li> <li>production setup to use the sources google spreadsheet</li> <li>to learn to renenerate the config files when you edit using glcon.</li> </ul> </li> <li>how to setup the UI</li> <li>how to reconfigure the UI </li> </ul> <p>Clean Machine</p> <p>These are instructions for a clean machine. Your mileage will vary if you are trying to install this stack on a developers workstation. If you are experienced, then you can probably deploy the docker stacks on a server with docker running. The stack uses treafik labels to manage the roures between the web server and the containers. It is not a task for the faint of heart, but IMOHO, it is more automatic that nginx or caddy reverse proxy routing. and allows us to deploy 'tenant' client stacks using configured data steores for each client in the services stack. Probably can be done with helm charts.</p>"},{"location":"developers/services-infrastructure/#requirements-prior-to-starting","title":"Requirements prior to starting","text":"<p>Three sections to know prior to installing * All * Production * Local Development</p>"},{"location":"developers/services-infrastructure/#all","title":"ALL","text":"<p>You need to be able to run <code>docker compose version</code></p> <p>Should be &gt; v2.13 <code>docker compose  --help</code> needs to show the -p --project flag</p> <p>Known issue</p> <p>with (at least) Ubuntu default docker package. Install the official docker package</p> <pre><code>docker compose version\nDocker Compose version v2.13.0</code></pre>"},{"location":"developers/services-infrastructure/#production","title":"Production","text":"<p>YOU NEED SETUP DNS.</p> <p>Setup DNS names for the aliases  so the treafik routing will work</p> Local(Tutorial)/Development <p>TODO: there are compose-local.yaml configurations. NOT WELL TESTED</p> <ul> <li>The run_local.sh works. There is no portainer for these. Managed by command line.</li> <li>minio (and maybe others) use ports. See \"-local path \" at this page</li> </ul>"},{"location":"developers/services-infrastructure/#creating-and-managing-containers","title":"Creating and managing CONTAINERS","text":"<ol> <li> <p>Configure a base server</p> <ul> <li>docker</li> <li>setup directory and groups for installing geocodes</li> <li>git clone https://github.com/earthcube/geocodes.git</li> <li>cd geocodes/deployment</li> <li>setup domain names</li> <li>create .env file</li> <li>add  (traefik and portainer ) build-machine-compose.yaml</li> <li>(add headless with larger shared memory) ./run_gleaner.sh   </li> </ul> </li> <li> <p>Use portainer to setup geocodes services </p> <ul> <li>setup and configure services<ul> <li>create env variables file for the services</li> <li>add stack services-compose.yaml to portainer</li> </ul> </li> </ul> </li> <li>Setup Gleaner containers</li> <li>run shell script <code>run_gleaner.sh</code></li> <li> <p>Initial Setup of datastores and loading of sample data</p> <ul> <li>Setup datastores for s3 and graph</li> <li>Install software glcon</li> <li>create configuration gctest <code>./glcon config init --cfgName gctest</code></li> <li>copy file with repository information (sitemap location and name)</li> <li>edit file tell ingest what services to utilize </li> <li>generate a configuration updated with the source and configuration <code>./glcon config generate --cfgName gctest</code></li> <li>run ingest <code>./glcon gleaner batch --cfgName gctest</code><ul> <li>check minioadmin to see that bucket gctest was populated</li> </ul> </li> <li>convert to triples and upload: <code>./glcon nabu prefix --cfgName gctest</code><ul> <li>run sparql query at graph service to see that triples got converted and uploaded</li> </ul> </li> <li>create a materilized view of the data using summarize (TB DOCUMENTED BY MBCODE)</li> </ul> </li> <li> <p>Use portainer to setup geocodes user interface (and services )</p> <ul> <li>setup and configure user infertace and it's services<ul> <li>create a facets config</li> <li>upload facets config to portainer/docker</li> <li>add  stack gecodes-compose.yaml to portainer</li> </ul> </li> </ul> </li> <li>Creating a community instance (aka tennant)</li> </ol>"},{"location":"developers/services-infrastructure/#data-loading","title":"Data Loading","text":"<ul> <li>Testing</li> <li>Production<ul> <li>Create a  'Production' config </li> <li>Production Fragments</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/#notes","title":"NOTES","text":"<ul> <li>Troubleshooting</li> </ul>"},{"location":"developers/services-infrastructure/development_notes/","title":"Notes on issues","text":""},{"location":"developers/services-infrastructure/development_notes/#creating-portainer-and-traefik","title":"Creating Portainer and Traefik","text":"<p>Initially thought to run the original stack in portainer.  Did not work well. </p>"},{"location":"developers/services-infrastructure/development_notes/#lets-encrypt","title":"Lets encrypt","text":"<p>When testing use the let's enxrypt developement server in the traefi.yml</p> <p>Hit a limit during development where the acme.json was being created as a directory Creating a config directory and telling let's encrypt/traefik proxy to store in that  directory solved the issue. Still had to wait seven days for the limit to clear</p>"},{"location":"developers/services-infrastructure/development_notes/#container-building","title":"Container Building","text":"<p>The docker container uses</p> <p><code>RUN npm build</code></p> <p>this calls</p> <p><code>vue-cli-service build</code></p> <p>Which run NODE_ENV production</p> <p>added</p> <p><code>\"geocodestest\": \"vue-cli-service build --mode geocodestest\"</code></p> <p>and changed Dockerfile to <code>RUN npm geocodestest</code></p> <p>This points out that the production container will need to just access a config.json  file from a remote url.. or have it published into the containers /public directory.</p> <p>Headless:</p> <p>Improving performance:  I tried to replicated the headless container, but that make headless queries fail.</p>"},{"location":"developers/services-infrastructure/development_notes/#healthcheck","title":"healthcheck","text":"<p>https://medium.com/geekculture/how-to-successfully-implement-a-healthcheck-in-docker-compose-efced60bc08e</p>"},{"location":"developers/services-infrastructure/development_notes/#notes-on-updating-a-config-from-the-command-line","title":"Notes on updating a config from the command line","text":"<pre><code>ubuntu@geocodes-dev:~$ docker stack services geocodes\nID             NAME                      MODE         REPLICAS   IMAGE                                      PORTS\nqb2dg6kb3nsw   geocodes_notebook-proxy   replicated   1/1        nsfearthcube/mknb:latest                   \nlo93ax3l4qp8   geocodes_vue-client       replicated   1/1        nsfearthcube/ec_facets_client:latest       \nd1g5vckmivnj   geocodes_vue-services     replicated   1/1        nsfearthcube/ec_facets_api_nodejs:latest   </code></pre> <pre><code>ubuntu@geocodes-dev:~$ docker stack services  --format=\"{{.ID}} {{.Name}}\"  geocodes\nqb2dg6kb3nsw geocodes_notebook-proxy\nlo93ax3l4qp8 geocodes_vue-client\nd1g5vckmivnj geocodes_vue-services</code></pre> <p>How to stop a service docker service scale [servicename]=0</p> <pre><code>buntu@geocodes-dev:~$ docker service scale geocodes_vue-client=0\ngeocodes_vue-client scaled to 0\noverall progress: 0 out of 0 tasks \nverify: Service converged \n</code></pre>"},{"location":"developers/services-infrastructure/install_minio_client/","title":"minio client","text":"<p>Minio Client will help push and manage items in minio/s3 from the command line. basically, we can remove old datasets, empty buckets, etc.</p>"},{"location":"developers/services-infrastructure/install_minio_client/#install-client","title":"install client","text":"<p>minio client</p> <pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n--create-dirs \\\n-o $HOME/minio-binaries/mc\n\nchmod +x $HOME/minio-binaries/mc\nexport PATH=$PATH:$HOME/minio-binaries/\n\nmc --help</code></pre>"},{"location":"developers/services-infrastructure/install_minio_client/#if-midnight-commander-is-installed","title":"if midnight commander is installed","text":"<ul> <li>add file .bash_aliases <pre><code>alias mc=' $HOME/minio-binaries/mc'</code></pre></li> </ul>"},{"location":"developers/services-infrastructure/install_minio_client/#configure-client","title":"configure client","text":"<p>Note the single quotes around the password... some passwords are   not command line friendly <pre><code>set +o history\nmc config host add dev https://oss.geocodes.earthcube.org  {miniouser} '{miniopassword}'\n\nset -o history</code></pre></p>"},{"location":"developers/services-infrastructure/install_minio_client/#some-commands","title":"Some commands:","text":"<p>Note these are using a dev alias Use mc alias set to set an alias</p>"},{"location":"developers/services-infrastructure/install_minio_client/#what-buckets","title":"What Buckets","text":"<p><code>mc ls dev</code></p> <pre><code>[2022-07-28 09:52:51 PDT]     0B citesting/\n[2023-04-17 14:44:09 PDT]     0B deepoceans/\n[2023-01-11 09:03:29 PST]     0B dv-testing/\n[0000-12-31 16:07:02 LMT]     0B gleaner/\n[2022-10-18 05:30:41 PDT]     0B gleaner-wf/\n[2023-01-20 11:35:22 PST]     0B mb-testing/\n[2022-09-19 09:35:23 PDT]     0B mbci2/\n[2023-04-19 09:45:10 PDT]     0B opencore/\n[2023-03-11 16:13:11 PST]     0B public/</code></pre>"},{"location":"developers/services-infrastructure/install_minio_client/#bucket-disk-usage","title":"Bucket Disk usage","text":"<p><code>mc du --depth 2 dev/citesting</code></p> <pre><code>107KiB  9 objects   citesting/milled\n1.2KiB  1 object    citesting/orgs\n68KiB   40 objects  citesting/prov\n30B 2 objects   citesting/reports\n107KiB  1 object    citesting/results\n58KiB   9 objects   citesting/summoned\n341KiB  62 objects  citesting</code></pre>"},{"location":"developers/services-infrastructure/stack_machines/","title":"Stack Containers","text":""},{"location":"developers/services-infrastructure/stack_machines/#containers-and-routes","title":"\"Containers and Routes\"","text":"<pre><code>flowchart TB\nservices-- deployed by --&gt;portainer\ngeocodes-- deployed by  --&gt; portainer\ngleaner-- deployed by  --&gt; portainer\nfacetsearch-- routes --&gt; traefik\nfacetsearchservices-- routes--&gt;traefik\noss-- routes--&gt;traefik\ntriplestore-- routes --&gt; traefik\nsparqlgui-- routes --&gt; traefik\nsubgraph gleaner\nheadless\nend\nsubgraph geocodes\nfacetsearch--&gt;facetsearchservices\nend\nsubgraph services\noss[\"oss s3\"]\nsparqlgui\ntriplestore[\"graph -- triplestore\"]\nend \n        subgraph base\n           traefik&lt;-- routes --&gt;portainer\n        end \n</code></pre>"},{"location":"developers/services-infrastructure/stack_machines/#this-is-a-list-of-the-stack-containers","title":"This is a list of the stack containers.","text":"<p>NOTE, for production stacks, DNS Names listed need to be cnamed. REPEAT, so to setup a test machine for production, you need to request DNS. The local stacks do no use the traefik routing, and the followign ports need to be available: 3000, 3031,8080, 8888, 9000, 9001, 9999</p> container name stack -local path notes traekfik admin.{HOST} base n/a http router portainer portainer.{HOST} base n/a container management s3system oss.{HOST} services http://localhost:9000 s3 store s3system minioadmin.{HOST} services http://localhost:9001 s3 store triplestore graph.{HOST} services http://localhost:9999/blazegraph/ sparqlgui sparqlui.{HOST} services http://localhost:8888/sparqlgui sparql ui headless {none} gleaner_via_shell headless:9000 (internal route) start with ./run_gleaner.sh vue-client geocodes.{HOST} geocodes http://localhost:8080/ facetsearch ui vue-services geocodes.{HOST} geocodes http://localhost:3000/ec/api api ,at geocodes/ec/api notebook-proxy geocodes.{HOST} geocodes http://localhost:3031/notebook notebook proxy, at geocodes/notebook"},{"location":"developers/services-infrastructure/stack_machines/#docker","title":"Docker","text":"<p>When you run local, these should be created</p> <p>Networks: traefik_proxy Volumes: * graph * s3 * logs configs: * configs will be locally mounted, See the docker files.</p>"},{"location":"developers/services-infrastructure/troubleshooting/","title":"Troubleshooting","text":""},{"location":"developers/services-infrastructure/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Some topics (sorry disorganized notes)</p> <ul> <li>Containers<ul> <li>can't seem to connect</li> <li>Complaints about bad certificate</li> <li>cannot connect to (minioadmin,graph,sparqlui)</li> <li>Minoadmin/Graph/etc seem to be there, but do not connect</li> </ul> </li> <li>glcon/gleaner application</li> <li>Blazegraph<ul> <li>journal truncation</li> </ul> </li> <li>updating portainer</li> <li>OS issues<ul> <li>Ubuntu docker</li> <li>Ubuntu 16. glcon</li> </ul> </li> <li>issue with a repository</li> <li></li> </ul>"},{"location":"developers/services-infrastructure/troubleshooting/#cant-seem-to-connect","title":"can't seem to connect;","text":"<p>are containers running <code>docker ps</code></p> <p>can you connect to portainer and traefik</p> <p>traefik:  <pre><code>https://admin.{HOST}</code></pre></p> <p>portainer:  <pre><code>https://portainer.{host}</code></pre></p> <p>In traefik, are  errors on the https://admin.{host}</p>"},{"location":"developers/services-infrastructure/troubleshooting/#complaints-about-bad-certificate","title":"Complaints about bad certificate","text":"<p>Initially, we are using letsencypt dev services. THis message will show until you rebuild the base containers with the dev line commented out</p>"},{"location":"developers/services-infrastructure/troubleshooting/#but-that-line-is-commented-out","title":"But that line is commented out.","text":"<p>need to delete the acme.json and restart the container.</p> <p>log onto console via portainer, use /bin/sh <pre><code>/ # ls\nacme.json      dev            etc            lib            mnt            proc           run            srv            tmp            var\nbin            entrypoint.sh  home           media          opt            root           sbin           sys            usr\n/ # cat acme.json \n{\n  \"httpresolver\": {\n    \"Account\": {\n      \"Email\": \"dwvalentine@ucsd.edu\",\n      \"Registration\": {\n        \"body\": {\n          \"status\": \"valid\",\n          \"contact\": [\n            \"mailto:dwvalentine@ucsd.edu\"\n          ]\n        },\n        \"uri\": \"https://acme-v02.api.letsencrypt.org/acme/acct/1030461777\"\n</code></pre></p> <pre><code>rm acme.json</code></pre> <p>some docker command also...</p>"},{"location":"developers/services-infrastructure/troubleshooting/#cannot-connect-to-minioadmingraphsparqlui","title":"cannot connect to (minioadmin,graph,sparqlui)","text":"<ul> <li>is the traefik proxy a SCOPE swarm <code>docker network ls</code></li> <li>portainerui, networks</li> <li>is the DNS correct?</li> <li>need a dig example, since editing host and NS looku</li> <li>are they running?</li> <li><code>docker ps</code><ul> <li>do you see portainer stack </li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/troubleshooting/#minoadmingraphetc-seem-to-be-there-but-do-not-connect","title":"Minoadmin/Graph/etc seem to be there, but do not connect","text":"<p>If minioadmin complains that it cannot connect with a 10.0.0.x message, then there  may be two</p> <ul> <li>open portainer</li> <li>select containers</li> <li>sort by name</li> <li>look and see if two of the s3 stack are running.</li> <li>if yes, delete both, and one should restart.</li> </ul> <p>have also seen something similar for graph.</p>"},{"location":"developers/services-infrastructure/troubleshooting/#glcon","title":"glcon","text":""},{"location":"developers/services-infrastructure/troubleshooting/#setup-failure","title":"setup failure","text":"<p><code>./glcon gleaner setup --cfgName {name}</code></p> <pre><code>{\"file\":\"/github/workspace/pkg/gleaner.go:63\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"error\",\"msg\":\"Connection issue, make sure the minio server is running and accessible.The Access Key Id you provided does not exist in our records.\",\"time\":\"2022-07-22T19:08:01Z\"}</code></pre> <p>You probably have environment variables set. <pre><code>ubuntu@geocodes-dev:~/indexing$ env\n[snip]\nMINIO_SECRET_KEY=MySecretSecretKeyforMinio\nMINIO_ACCESS_KEY=MySecretAccessKey\n[snip]\nubuntu@geocodes-dev:~/indexing$ unset MINIO_SECRET_KEY\nubuntu@geocodes-dev:~/indexing$ unset MINIO_ACCESS_KEY</code></pre></p>"},{"location":"developers/services-infrastructure/troubleshooting/#blazegraph-journal-truncation","title":"Blazegraph journal truncation:","text":""},{"location":"developers/services-infrastructure/troubleshooting/#for-a-container","title":"for a container","text":"<p>in newer container, the command is available, but the service needs to be stopped. guess running an container with an exec command in a different container might work.</p> <pre><code>cd /var/lib/blazegraph ;java -jar /usr/bin/blazegraph.jar com.bigdata.journal.CompactJournalUtility blazegraph.jnl blazegraph.jnl.compact\n\n</code></pre>"},{"location":"developers/services-infrastructure/troubleshooting/#count-quads","title":"count quads","text":"<pre><code>SELECT (COUNT(*) as ?Triples) WHERE {graph ?g {?s ?p ?o}}</code></pre>"},{"location":"developers/services-infrastructure/troubleshooting/#updating-portainer-or-treafik","title":"updating Portainer, or treafik","text":"<p>the latest image needs to be pulled</p> <p><code>docker pull portainer/portainer-ce:latest</code></p> <p>then <code>./run_base.sh</code></p>"},{"location":"developers/services-infrastructure/troubleshooting/#os-issues","title":"OS Issues","text":"<p>place where  os issues may be </p>"},{"location":"developers/services-infrastructure/troubleshooting/#ubuntu-docker","title":"Ubuntu Docker","text":"<p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run</p> <pre><code>```shell\ndocker compose version\nDocker Compose version v2.13.0\n```\n\nIf you cannot run `docker compose` then update to the docker.com version\nThis is the version we are presently running.\n\n```    \nClient: Docker Engine - Community\n     Version:           20.10.21\n     API version:       1.41\n```</code></pre>"},{"location":"developers/services-infrastructure/troubleshooting/#ubuntu-18-and-glcon","title":"Ubuntu 18 and glcon","text":"<p>there appears to be issues with Ubuntu 18 and the latest versions of the golang library viper. If you run on Ubuntu 20.X it works. Solution is to 'do-realse-upgrade' and wait ;)</p> <p><code>shell ubuntu@geocodes-dev:~$ uname -a Linux geocodes-dev 4.15.0-194-generic #205-Ubuntu SMP Fri Sep 16 19:49:27 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux ubuntu@geocodes-dev:~$ cd indexing/ ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName ci version:  v3.0.8-ec {\"file\":\"/Users/valentin/development/dev_earthcube/gleanerio/gleaner/pkg/cli/gleaner.go:71\",\"func\":\"github.com/gleanerio/gleaner/pkg/cli.initGleanerConfig\",\"level\":\"fatal\",\"msg\":\"error reading config file While parsing config: yaml: unmarshal errors:\\n  line 1: cannot unmarshal !!str `\\u003c?xml v...` into map[string]interface {}\",\"time\":\"2023-02-14T15:58:00Z\"} ubuntu@geocodes-dev:~/indexing$</code></p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/","title":"Data Loading Trouble shooting","text":""},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#intial-places-to-look","title":"Intial places to look:","text":"<p>When you have issues there are multiple starting points</p> <ul> <li>console... </li> <li>logs</li> <li>minio</li> <li>run reports</li> </ul>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#console-and-logs","title":"Console and Logs","text":"<p>There will be errors in the console and the logs. We have not correctly identified what is an actual data error, a network  error, or a code error.</p> <p>Need to add common errors with descriptions:</p> <p>Robots.txt</p> <p>These are normal. It just says hey cannot find the robots.txt. Being lowered to info or debug <code>{\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:35\",\"func\":\"github .com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\" msg\":\"error parsing robots.txt at https://ecl.earthchem.org/home.php/robots.txtP arse error(s): \\nAllow before User-agent at token #31.\\n\",\"time\":\"2023-05-03T14: 01:19-05:00\"}</code></p> <p>** Headless **</p> <p>There are normal. If gleaner cannot find a jsonld it calls headless. And then if headless cannot find one,  it presently throws an error.</p> <p>{\"file\":\"/github/workspace/internal/summoner/acquire/headlessNG.go:340\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.PageRender\",\"issue\":\"Headless Evaluate\",\"level\":\"error\",\"msg\":\"cdp.Runtime: Evaluate: context deadline exceeded\",\"time\":\"2023-05-04T14:00:16-05:00\",\"url\":\"https://earthref.org/MagIC/13413\"}</p> <p>headless timeput</p> <p>Normal, but you may need to check it out</p> <p>{\"file\":\"/github/workspace/internal/summoner/acquire/headlessNG.go:340\",\"func\":\" github.com/gleanerio/gleaner/internal/summoner/acquire.PageRender\",\"issue\":\"Head less Evaluate\",\"level\":\"error\",\"msg\":\"cdp.Runtime: Evaluate: rpc error: Executio n context was destroyed. (code = -32000)\",\"time\":\"2023-05-03T14:01:24-05:00\",\"ur l\":\"https://ssdb.iodp.org/dataset\"}</p> <p>** error **</p> <p>This may mean that a page does not have JSON, or that a page that gleaner thinks should be json, is not json. You may need to look in postman, and send a Accept header to see what gleaner is getting from the website</p> <p><code>{\"contentType\":\"script[type='application/ld+json']\",\"file\":\"/github/workspace/in ternal/summoner/acquire/acquire.go:228\",\"func\":\"github.com/gleanerio/gleaner/int ernal/summoner/acquire.FindJSONInResponse.func1\",\"level\":\"error\",\"msg\":\"Error pr ocessing script tag in http://get.iedadata.org/doi/100505error checking for vali d json: Error in unmarshaling json: invalid character '}' looking for beginning of object key string\",\"time\":\"2023-05-03T14:01:23-05:00\",\"url\":\"http://get.iedad ata.org/doi/100505\"}</code></p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#data-loading-stops-when-my-terminal-disconnects","title":"Data Loading stops when my terminal disconnects","text":"<p>Yes, the process dies when you are disconnected... if you do not nohup the process,  or use tmux or screen to allow the process  to be detaached.</p> <p>There are insructions for screen, since that is what the write of this documentation uses:</p> <p>If source is large,  use screen e.g. <code>screen -S gleaner</code></p> <p>Did we suggest running in a screen</p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#missing-jsonld","title":"Missing JSONLD","text":"<p>from open topo one id</p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#duplicate-id","title":"duplicate id","text":""},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#r2r","title":"r2r","text":"<p>@id is set to doi:null</p> <p>same id generated, only one file will be maintained. <pre><code>level=debug issue=\"Uploaded JSONLD to object store\" sha=00a6b5eec951ac51065f0f2485c3406a4c260fb0 url=\"https://dev.rvdata.us/search/fileset/149018\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=d26f68908c3d75d7705f78518beb19c325d32ac9 url=\"https://dev.rvdata.us/search/fileset/149018\"\nlevel=debug msg=\"&lt;nil&gt;\" issue=\"Multiple JSON\" url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Multiple JSON\" url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=00a6b5eec951ac51065f0f2485c3406a4c260fb0 url=\"https://dev.rvdata.us/search/fileset/149019\"\nlevel=debug issue=\"Uploaded JSONLD to object store\" sha=d26f68908c3d75d7705f78518beb19c325d32ac9 url=\"https://dev.rvdata.us/search/fileset/149019</code></pre></p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#data-catalog","title":"Data Catalog","text":"<p>we need to deal with these</p>"},{"location":"developers/services-infrastructure/data_loading/dataloading_troubleshooting/#no-datasets","title":"No datasets","text":"<p>run the graph and no datasets</p>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/","title":"Loading Data for Testing and Validation","text":"<p>Goal: Load data from GeocodesMetadata Repository for testing</p> <p>The stories should be on the Geocodes Documentation Wiki. data validation and loading story</p> <p>This will load data into  buckets that is for testing. Aka not in gleaner</p> <p>let's use:</p> <ul> <li>ci</li> <li>ci2 </li> </ul> <p>Testing Matrix</p> Tests config s3 Bucket graph namespace notes ReporitoryMeta gctest gctest gctest samples of actual datasets TestingMeta ci citesting citesting Good Dataset multiple ci2 citesting2 citesting2 two repositories DoubleLoad ci citesting citesting Run Nabu a second time to try to load duplicates <p>gctest</p> <p>gctest configuration and setup is described in Setup</p>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#install-glcon","title":"Install glcon","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <p>Install glcon</p>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#create-a-configuration-for-continuous-integration","title":"Create a configuration for Continuous Integration","text":"<code>./glcon config init --cfgName ci</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon config init --cfgName ci\n    2022/07/21 23:27:31 EarthCube Gleaner\n    init called\n    make a config template is there isn't already one\n    ubuntu@geocodes-dev:~/indexing$ ls configs/ci\n    README_Configure_Template.md  localConfig.yaml  sources.csv\n    gleaner_base.yaml             nabu_base.yaml\n    ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#edit-files","title":"edit files:","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/ci/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: citesting\n  # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: citesting\n  # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv \n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestDatasetSources\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#generate-configs-glcon-config-generate-cfgname-ci","title":"Generate configs <code>./glcon config generate --cfgName ci</code>","text":"<code>.</code>/glcon config generate --cfgName ci` <pre><code>./glcon config generate --cfgName ci\n2022/07/21 23:37:46 EarthCube Gleaner\ngenerate called\n{SourceType:sitemap Name:geocodes_demo_datasets Logo:https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources URL:https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml Headless:false PID:https://www.earthcube.org/datasets/ ProperName:Geocodes Demo Datasets Domain:0 Active:true CredentialsFile: Other:map[] HeadlessWait:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#flightest","title":"flightest","text":"<code>./glcon gleaner setup --cfgName ci</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName ci\n   2022/07/21 23:42:54 EarthCube Gleaner\n   Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner\n   Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu\n   setup called\n   2022/07/21 23:42:54 Validating access to object store\n   2022/07/21 23:42:54 Connection issue, make sure the minio server is running and accessible. The specified bucket does not exist.\n   ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#run-batch","title":"run batch","text":"<p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> \"./glcon gleaner batch --cfgName ci <pre><code>    ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName ci\n    INFO[0000] EarthCube Gleaner                            \n    Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner\n    Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu\n    batch called\n    {\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/pkg/gleaner.go:35\",\"func\":\"github.com/gleanerio/gleaner/pkg.Cli\",\"level\":\"info\",\"msg\":\"Sitegraph(s) processed\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:17\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner start time:2022-07-22 19:16:53.451745139 +0000 UTC m=+0.182100234\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:189\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"info\",\"msg\":\"Getting robots.txt from 0/robots.txt\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:23\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\"msg\":\"error fetching robots.txt at 0/robots.txtGet \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:192\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for 0:Get \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:63\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasetscontinuing without it.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:127\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getSitemapURLList\",\"level\":\"info\",\"msg\":\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml is not a sitemap index, checking to see if it is a sitemap\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:32\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResRetrieve\",\"level\":\"info\",\"msg\":\"Queuing URLs for geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:74\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getConfig\",\"level\":\"info\",\"msg\":\"Thread count 5 delay 0\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    12% |\u2588\u2588\u2588\u2588\u2588\u2588                                                 | (2/16, 2 it/s) [0s:7s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    43% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                | (7/16, 6 it/s) [1s:1s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n    68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | (11/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | (12/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (16/16, 9 it/s)        \n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:37\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner end time:2022-07-22 19:16:55.660367672 +0000 UTC m=+2.390721648\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/summoner/summoner.go:38\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner run time:0.0368103569\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:27\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller start time2022-07-22 19:16:55.661434567 +0000 UTC m=+2.391819553\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:44\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to milling list:summoned/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to prov building list:prov/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (15/15, 51 it/s)        \n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:82\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Assembling result graph for prefix:summoned/geocodes_demo_datasetsto:milled/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:83\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Result graph will be at:results/runX/geocodes_demo_datasets_graph.nq\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/graph/graphng.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Pipe copy for graph done\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:84\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller end time:2022-07-22 19:16:56.387639969 +0000 UTC m=+3.117994225\",\"time\":\"2022-07-22T19:16:56Z\"}\n    {\"file\":\"/github/workspace/internal/millers/millers.go:85\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller run time:0.0121029112\",\"time\":\"2022-07-22T19:16:56Z\"}</code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#push-to-graph","title":"push to graph","text":"<code>./glcon nabu prefix --cfgName ci</code> <p>```json lines ./glcon nabu prefix --cfgName ci INFO[0000] EarthCube Gleaner                           Using gleaner config file: /home/ubuntu/indexing/configs/ci/gleaner Using nabu config file: /home/ubuntu/indexing/configs/ci/nabu check called 2022/07/22 19:23:16 Load graphs from prefix to triplestore {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:41\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"[milled/geocodes_demo_datasets prov/geocodes_demo_datasets org]\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:61\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"gleaner:milled/geocodes_demo_datasets object count: 15\\n\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:79\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.PipeLoad\",\"level\":\"info\",\"msg\":\"Loading milled/geocodes_demo_datasets/11316929f925029101493e8a05d043b0ae829559.rdf \\n\",\"time\":\"2022-07-22T19:23:16Z\"} [snip] {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:197\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Status: 200 OK\",\"time\":\"2022-07-22T19:23:21Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:198\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Headers: map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Headers:[Authorization,Origin,Content-Type,Accept] Access-Control-Allow-Origin:[*] Content-Length:[449] Content-Type:[text/html;charset=utf-8] Date:[Fri, 22 Jul 2022 19:23:21 GMT] Server:[Jetty(9.4.z-SNAPSHOT)] Vary:[Origin] X-Frame-Options:[SAMEORIGIN]]\",\"time\":\"2022-07-22T19:23:21Z\"} 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (1/1, 15 it/s)</p> <p>```</p>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#test-in-graph","title":"Test in Graph","text":"<p><code>https://graph.geocodes-dev.earthcube.org/blazegraph/#query</code></p>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#returns-all-triples","title":"returns all triples","text":"<pre><code>select * \nwhere {\n  ?s ?p ?o\n     }\nlimit 1000</code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#query-for-amgeo","title":"query for amgeo","text":"<pre><code>PREFIX bds: &lt;http://www.bigdata.com/rdf/search#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?g ?resourceType ?name ?description  ?pubname\n(GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n(GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw)\n?datep  (GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl) (MAX(?score1) as ?score)\n(MAX(?lat) as ?maxlat) (Min(?lat) as ?minlat) (MAX(?lon) as ?maxlon) (Min(?lon) as ?minlon)\nWHERE {\n?lit bds:search \"amgeo\" .\n?lit bds:matchAllTerms false .\n?lit bds:relevance ?score1 .\n?lit bds:minRelevance 0.14 .\n?subj ?p ?lit .\n#filter( ?score1 &gt; 0.14).\ngraph ?g {\n?subj schema:name|sschema:name ?name .\n?subj schema:description|sschema:description ?description .\n#Minus {?subj a sschema:ResearchProject } .\n# Minus {?subj a schema:ResearchProject } .\n# Minus {?subj a schema:Person } .\n# Minus {?subj a sschema:Person } .\n}\n#BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a sschema:Dataset .} , \"data\", \"tool\" ) AS ?resourceType).\nvalues (?type ?resourceType) {\n(schema:Dataset \"data\")\n(sschema:Dataset \"data\")\n(schema:ResearchProject \"Research Project\") #BCODMO- project\n(sschema:ResearchProject  \"Research Project\")\n(schema:SoftwareApplication  \"tool\")\n(sschema:SoftwareApplication  \"tool\")\n(schema:Person  \"Person\") #BCODMO- Person\n(sschema:Person  \"Person\")\n(schema:Event  \"Event\") #BCODMO- deployment\n(sschema:Event  \"Event\")\n(schema:Award  \"Award\") #BCODMO- Award\n(sschema:Award  \"Award\")\n(schema:DataCatalog  \"DataCatalog\")\n(sschema:DataCatalog  \"DataCatalog\")\n#(UNDEF \"other\")  # assume it's data. At least we should get  name.\n} ?subj a ?type .\noptional {?subj schema:distribution/schema:url|sschema:subjectOf/sschema:url ?url .}\nOPTIONAL {?subj schema:datePublished|sschema:datePublished ?datep .}\nOPTIONAL {?subj schema:publisher/schema:name|sschema:publisher/sschema:name|sschema:sdPublisher|sschema:provider/schema:name ?pubname .}\nOPTIONAL {?subj schema:spatialCoverage/schema:name|sschema:spatialCoverage/sschema:name ?placename .}\n\nOPTIONAL {?subj schema:keywords|sschema:keywords ?kwu .}\n\n}\nGROUP BY ?subj ?pubname ?placenames ?kw ?datep ?disurl ?score ?name ?description  ?resourceType ?g ?minlat ?maxlat ?minlon ?maxlon\nORDER BY DESC(?score)\nLIMIT 100\nOFFSET 0\n</code></pre>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#test-in-client","title":"test in client","text":"<p><code>https://geocodes.geocodes-dev.earthcube.org</code></p> <ul> <li>terms</li> <li>amgeo</li> <li>bcodmo</li> </ul>"},{"location":"developers/services-infrastructure/data_loading/indexing_with_gleanerio_for_testing/#detailed-testing","title":"Detailed Testing","text":"<p>TODO Need to ses the datavalidaton story:</p>"},{"location":"developers/services-infrastructure/data_loading/install_glcon/","title":"Install Glcon","text":""},{"location":"developers/services-infrastructure/data_loading/install_glcon/#install-glcon","title":"Install glcon","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <ul> <li>create a directory <code>cd ~ ; mkdir indexing</code></li> <li>download and install: We will try to keep this updated, but for the latest release. <code>wget https://github.com/gleanerio/gleaner/releases/download/{{RELASE}}}}</code></li> </ul> <p><code>tar xf glcon-v3.0.8-linux-amd64.tar.gz</code></p> OS download linux intel glcon-{{VERSION}}-linux-amd64.tar.gz linux arm glcon-{{VERSION}}-linux-arm64.tar.gz mac glcon-{{VERSION}}-darwin-arm64.tar.gz windows  intel glcon-{{VERSION}}-windows-amd64.zip downloading glcon <pre><code>    3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    --2022-07-21 23:04:55--  https://github.com/gleanerio/gleaner/releases/download/v3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    Resolving github.com (github.com)... 140.82.113.4\n    Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream [following]\n    --2022-07-21 23:04:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream\n    Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n    Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 13826668 (13M) [application/octet-stream]\n    Saving to: \u2018glcon-v3.0.4-dev-linux-amd64.tar.gz\u2019\n\nglcon-v3.0.4-dev-linux- 100%[=============================&gt;]  13.19M  12.6MB/s    in 1.0s</code></pre> <p>See that it is installed <pre><code>ubuntu@geocodes-dev:~/indexing$ tar xf glcon-v3.0.4-dev-linux-amd64.tar.gz\nubuntu@geocodes-dev:~/indexing$ ls\nREADME.md  docs   glcon-v3.0.4-dev-linux-amd64.tar.gz  scripts\nconfigs    glcon  schemaorg-current-https.jsonld</code></pre></p> <ul> <li>test</li> </ul> <code>./glcon --help</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon --help\nINFO[0000] EarthCube Gleaner                            \nThe gleaner.io stack harvests JSON-LD from webpages using sitemaps and other tools\nstore files in S3 (we use minio), uploads triples to be processed by nabu (the next step in the process)\nconfiguration is now focused on a directory (default: configs/local) with will contain the\nprocess to configure and run is:\n* glcon config init --cfgName {default:local}\n  edit files, servers.yaml, sources.csv\n* glcon config generate --cfgName  {default:local}\n* glcon gleaner Setup --cfgName  {default:local}\n* glcon gleaner batch  --cfgName  {default:local}\n* run nabu (better description)\n\nUsage:\n  glcon [command]\n\nAvailable Commands:\n  completion  generate the autocompletion script for the specified shell\n  config      commands to intialize, and generate tools: gleaner and nabu\n  gleaner     command to execute gleaner processes\n  help        Help about any command\n  nabu        command to execute nabu processes\n\nFlags:\n      --access string        Access Key ID (default \"MySecretAccessKey\")\n      --address string       FQDN for server (default \"localhost\")\n      --bucket string        The configuration bucket (default \"gleaner\")\n      --cfg string           compatibility/overload: full path to config file (default location gleaner in configs/local)\n      --cfgName string       config file (default is local so configs/local) (default \"local\")\n      --cfgPath string       base location for config files (default is configs/) (default \"configs\")\n      --gleanerName string   config file (default is local so configs/local) (default \"gleaner\")\n  -h, --help                 help for glcon\n      --nabuName string      config file (default is local so configs/local) (default \"nabu\")\n      --port string          Port for minio server, default 9000 (default \"9000\")\n      --secret string        Secret access key (default \"MySecretSecretKeyforMinio\")\n      --ssl                  Use SSL boolean\n\nUse \"glcon [command] --help\" for more information about a command.</code></pre>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/","title":"Oboarding a Dataseource/Repository","text":"<p>If we have an issue with a repository, let's test it, independently. If we have a new datasource/community, let's test it, independently.</p> <p>We can do this because:</p> <ul> <li>s3 paths are repository based, so we can load to an s3 bucket</li> <li>blazegraph, we can create two namespaces in our openstack blazegraph stores</li> <li>We can setup 'tenant' instances of the UI that connect to  (s3 bucket, and project namespaces) services</li> <li>we have some source reports to help evaluate the data loading.</li> </ul> <p>So, basically, This assumes we have some basic information about a data source, aka sitemap, and something we want to  name this repository.</p> <p>Note</p> <p>This is a high level overview that assumes you have loaded data, and do not need any deep details. Over time put the troubleshooting an gotchas below the steps.</p> <p>Note</p> <p>if source is large, run in a screen In fact, it is suggested to always run in a screen</p> <p>Please put any issues/notes in the production/repos google docs</p>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/#some-steps","title":"Some steps.","text":"<ol> <li>Grab some urls from the sitemap, evaluate in validator.schema.org</li> <li>run check_sitemap to see it url's are good</li> <li>setup datastores<ul> <li>any s3</li> <li>independent project and project_summary namespaces</li> </ul> </li> <li>create gleaner config for repo</li> <li>if source is large,  use screen e.g. <code>screen -S gleaner</code></li> <li><code>glcon gleaner batch</code> Summon to an s3 location. Repos are independent at this point.<ul> <li>did we suggest runing in a screen</li> </ul> </li> <li>evaluate summon. Look at jsonld. Do they seem like they got loaded?<ul> <li>thought: do we need a tool to pull a specific url from s3? could filter the listSummonedUrls, we do have getOringalUrl</li> <li>run missing stats... may need an option to just check the sitemap&gt;summon portion</li> </ul> </li> <li><code>glcon nabu prefix</code> if these look good, then <ul> <li>did we suggest runing in a screen</li> <li>there needs to be a note about using <code>glcon nabu release --cfgName CONFIG</code>, and how to upload the quads</li> </ul> </li> <li>run <code>graph_stats</code> and <code>missing...</code><ul> <li>graph stats report needs to be updated to include [all/repo]_count_types_top_level.sparql, </li> </ul> </li> <li>check reports</li> <li>feel free to run repo_urn_w_types_toplevel.sparql</li> <li>run summarize_* to populate summary</li> <li>create a facets configuration for the project, upload to portainer, and create stack of tenant containers to run against the project and project_summary namespaces</li> <li>via UI run queries to see that it works.<ul> <li>humm, add a simple query tester to scripts</li> </ul> </li> <li>Review with team</li> <li>review with datasource</li> <li>add to production sources</li> </ol>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/#evaluating-with-validator","title":"Evaluating with validator","text":"<p>what to look for</p>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/#evaluating-summon","title":"evaluating summon","text":"<p>What to look for</p>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/#evaluating-graph-and-missing-reports","title":"Evaluating graph and missing reports","text":"<p>What to look for</p>"},{"location":"developers/services-infrastructure/data_loading/onboarding_or_testing_a_datasource/#how-to-test-the-ui","title":"How to test the UI","text":"<p>keywords: * data * repository keywords</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/","title":"Loading Data for The Initial Installation","text":"<p>This is step 4 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol> <p>Step Overview:</p> <ol> <li>create data stores in minioadmin and graph</li> <li>install glcon, if not installed</li> <li>create a configuration file to install a small set of data<ol> <li><code>./glcon config init --cfgName gctest</code></li> <li>edit</li> <li><code>./glcon config generate --cfgName gctest</code></li> </ol> </li> <li>setup and summon data using 'gleaner' <ol> <li><code>./glcon gleaner setup --cfgName gctest</code></li> <li><code>./glcon gleaner batch --cfgName gctest</code></li> </ol> </li> <li>load data to graph using 'nabu' <ol> <li><code>./glcon nabu prefix --cfgName gctest</code></li> <li><code>./glcon nabu prune --cfgName gctest</code></li> </ol> </li> <li>Test data in Graph </li> <li>Example of how to edit the source</li> <li>edit gctest.csv</li> <li>regenerate configs</li> <li>rerun batch</li> <li>Run Summarize task. This is performance related.</li> </ol> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#step-details","title":"Step Details","text":""},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#setup-datastores","title":"Setup Datastores","text":"<p>There are several datastores required to enable data summoning(harvesting), converting to a graph. While the production presently uses the earthcube repository convention, we suggest that  tutorial and communities setting up an instance to use the geocodes repository pattern. Earthcube/Decoder staff should use the A Community pattern when setting up an instance for a community.</p> Repository config s3 Bucket graph namespaces notes GeocodesTest gctest gctest gctest, gctest_summary samples of actual datasets geocodes geocodes geocodes geocodes, geocodes_summary suggested standalone repository earthcube geocodes gleaner earthcube, summary DEFAULT PRODUCTION NAME A COMMUNITY eg {acomm} {acomm} {acomm}, {acomm}_summary A communities tenant repository <p>Initial Setup</p> <p>we will be setting up both the gctest and gecodes repositories.</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#setup-minio-buckets","title":"Setup Minio buckets","text":"<p>Gleaner extracts JSONLD from a web apge, and stores it in an s3 system (Minio) in </p> <p>go to https://minioadmin.{youhost}/</p> <p>create buckets gctest, and geocodes</p> <p>go to settings for the bucket and make  public.</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#setup-graph-stores","title":"Setup Graph stores.","text":"<p>Nabu pulls from the s3 system, converts to RDF quads, and uploads to a graph store.</p> <p>go to https://graph.{your host}</p> <p>namespace tab, create  a mode 'quads' namespace with full text index,  \"gctest\", and \"geocodes\"</p> <p>namespace tab, create mode 'triples' namespace with full text index, \"gctest_summary\", and \"geocodes_summary\"</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#install-indexing-software","title":"Install Indexing Software","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <p>Install glcon</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#harvest-and-load-data","title":"Harvest and load data","text":"<p>Goal is to create a configuration file to load gctest data. The sitemap is here:</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#create-a-configuration-and-load-sample-data","title":"Create a configuration and load sample data","text":""},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#create-a-configuration-for-continuous-integration","title":"Create a configuration for Continuous Integration","text":"<code>./glcon config init --cfgName gctest</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon config init --cfgName gctest\n    2022/07/21 23:27:31 EarthCube Gleaner\n    init called\n    make a config template is there isn't already one\n    ubuntu@geocodes-dev:~/indexing$ ls configs/gctest\n    README_Configure_Template.md  localConfig.yaml  sources.csv\n    gleaner_base.yaml             nabu_base.yaml\n    ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#copy-sources-list-to-configsgctest","title":"Copy sources list to configs/gctest","text":"<p>Note</p> <p>assumes you are in indexing, and have put the geocodes at ~/geocodes aka your home directory</p> <p><code>cp ~/geocodes/deployment/ingestconfig/gctest.csv configs/gctest/</code></p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#edit-files","title":"edit files:","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/gctest/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{YOU HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: gctest # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.{YOU HOST}/blazegraph/namespace/gctest/sparql\ns3:\n  bucket: gctest # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1 \n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location: gctest.csv \n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestDatasetSources</code></pre> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#generate-configs","title":"Generate configs","text":"<code>./glcon config generate --cfgName gctest</code> <pre><code>./glcon config generate --cfgName gctest\n2022/07/21 23:37:46 EarthCube Gleaner\ngenerate called\n{SourceType:sitemap Name:geocodes_demo_datasets Logo:https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources URL:https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml Headless:false PID:https://www.earthcube.org/datasets/ ProperName:Geocodes Demo Datasets Domain:0 Active:true CredentialsFile: Other:map[] HeadlessWait:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#flightest","title":"flightest","text":"<p>Run setup to see if you can connect to the minio store</p> `./glcon gleaner setup --cfgName gctest <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName gctest\n   2022/07/21 23:42:54 EarthCube Gleaner\n   Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\n   Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\n   setup called\n   2022/07/21 23:42:54 Validating access to object store\n   2022/07/21 23:42:54 Connection issue, make sure the minio server is running and accessible. The specified bucket does not exist.\n   ubuntu@geocodes-dev:~/indexing$ </code></pre> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#load-data","title":"Load Data","text":"<p>Gleaner will harvest jsonld from the URL's listed in the sitemap.</p> <p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect </li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> <code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName gctest\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\nbatch called\n{\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:35\",\"func\":\"github.com/gleanerio/gleaner/pkg.Cli\",\"level\":\"info\",\"msg\":\"Sitegraph(s) processed\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:17\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner start time:2022-07-22 19:16:53.451745139 +0000 UTC m=+0.182100234\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:189\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"info\",\"msg\":\"Getting robots.txt from 0/robots.txt\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:23\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\"msg\":\"error fetching robots.txt at 0/robots.txtGet \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:192\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for 0:Get \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:63\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasetscontinuing without it.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:127\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getSitemapURLList\",\"level\":\"info\",\"msg\":\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml is not a sitemap index, checking to see if it is a sitemap\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:32\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResRetrieve\",\"level\":\"info\",\"msg\":\"Queuing URLs for geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:74\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getConfig\",\"level\":\"info\",\"msg\":\"Thread count 5 delay 0\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n12% |\u2588\u2588\u2588\u2588\u2588\u2588                                                 | (2/16, 2 it/s) [0s:7s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n43% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                | (7/16, 6 it/s) [1s:1s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | (11/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | (12/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (16/16, 9 it/s)        \n{\"file\":\"/github/workspace/internal/summoner/summoner.go:37\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner end time:2022-07-22 19:16:55.660367672 +0000 UTC m=+2.390721648\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:38\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner run time:0.0368103569\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:27\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller start time2022-07-22 19:16:55.661434567 +0000 UTC m=+2.391819553\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:44\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to milling list:summoned/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to prov building list:prov/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (15/15, 51 it/s)        \n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:82\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Assembling result graph for prefix:summoned/geocodes_demo_datasetsto:milled/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:83\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Result graph will be at:results/runX/geocodes_demo_datasets_graph.nq\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Pipe copy for graph done\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:84\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller end time:2022-07-22 19:16:56.387639969 +0000 UTC m=+3.117994225\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:85\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller run time:0.0121029112\",\"time\":\"2022-07-22T19:16:56Z\"}</code></pre> <p>See files in Minio</p> <p>You can open the minioadmin console (https://minioadmin.{your host}/) and look to see that file are uploaded into the bucket, in this case gctest.. summon/gecodes_demo_data</p> <p>(NEED IMAGE HERE)</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#push-to-graph","title":"Push to graph","text":"<p>Nabu will read files from the bucket, and push them to the graph store.</p> <code>./glcon nabu prefix --cfgName gctest</code> <p>```json lines ./glcon nabu prefix --cfgName gctest INFO[0000] EarthCube Gleaner                           Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu check called 2022/07/22 19:23:16 Load graphs from prefix to triplestore {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:41\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"[milled/geocodes_demo_datasets prov/geocodes_demo_datasets org]\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:61\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"gleaner:milled/geocodes_demo_datasets object count: 15\\n\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:79\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.PipeLoad\",\"level\":\"info\",\"msg\":\"Loading milled/geocodes_demo_datasets/11316929f925029101493e8a05d043b0ae829559.rdf \\n\",\"time\":\"2022-07-22T19:23:16Z\"} [snip] {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:197\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Status: 200 OK\",\"time\":\"2022-07-22T19:23:21Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:198\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Headers: map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Headers:[Authorization,Origin,Content-Type,Accept] Access-Control-Allow-Origin:[*] Content-Length:[449] Content-Type:[text/html;charset=utf-8] Date:[Fri, 22 Jul 2022 19:23:21 GMT] Server:[Jetty(9.4.z-SNAPSHOT)] Vary:[Origin] X-Frame-Options:[SAMEORIGIN]]\",\"time\":\"2022-07-22T19:23:21Z\"} 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (1/1, 15 it/s)</p> <p>```</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#test-in-graph","title":"Test in Graph","text":"<p>One the data is loaded into the graph store <code>https://graph.{your host}/blazegraph/#query</code></p> <ol> <li>go to namespace tab, select gctest, </li> <li>go to query tab, input the </li> </ol> returns all triples <pre><code>select * \nwhere {\n?s ?p ?o\n }\nlimit 1000</code></pre> <p>A more complex query can be ran:</p> what types are in the system <pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n       ?s a ?type .\n       }\n} \nGROUP By ?type\nORDER By DESC(?scount)</code></pre> <p>A more complex query can be ran:</p> Show me just datasets <pre><code>SELECT (count(?g ) as ?count) \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}</code></pre> <p>More SPARQL Examples</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#example-of-how-to-edit-the-source","title":"Example of how to edit the source","text":"<p>This demonstrates a feature where if you have duplicate identifiers, then you can ensure all data get loaded. It's a bad idea to have the same ID, but it happens.</p> <p>There are two lines in gctest csv.  The second dataset is [actual data] (https://github.com/earthcube/GeoCODES-Metadata/tree/main/metadata/Dataset/actualdata).  There are three files, the two earthchem files have the same @id, 1 2 The identifierType is set to 'filesha' which generates a sha based on the entire file.</p> gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,FALSE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#edit-gctestcsv","title":"edit gctest.csv","text":"<p>Set the second line active to TRUE</p> edited gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,TRUE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#regenerate-configs","title":"regenerate configs","text":"<p><code>./glcon config generate --cfgName gctest</code></p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#rerun-batch","title":"rerun batch","text":"<code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes:~/indexing$ ./glcon gleaner batch --cfgName gctest\nversion:  v3.0.8-fix129\nbatch called\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/actual:Robots.txt unavailable at https://www.earthcube.org/datasets/actual/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_actual_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (3/3, 10 it/s)        \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 25 it/s)        \nRunStats:\n  Start: 2023-01-30 21:09:49.120833598 -0600 CST m=+0.105789938\n  Repositories:\n    - name: geocodes_demo_datasets\n      SitemapCount: 9 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n      SitemapSummoned: 9 \n      SitemapStored: 9 \n    - name: geocodes_actual_datasets\n      SitemapSummoned: 3 \n      SitemapStored: 3 \n      SitemapCount: 3 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 168 it/s)\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (2/2, 123 it/s)</code></pre>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#create-a-materialized-view-of-the-data-using-summarize-to-the-repo_summary-namespace","title":"Create  a materialized view of the data using summarize to the  repo_summary namespace","text":"<p>DOCUMENTATION NEEDED </p> <p>(TBD assigned to Mike Bobak)</p>"},{"location":"developers/services-infrastructure/data_loading/setup_indexing_with_gleanerio/#go-to-step-5","title":"Go to step 5.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/data_loading/using_screen_for_manual_loading/","title":"Manual Loading with <code>screen</code>","text":"<p>If a source has many URLs/records, it suggest that one runs the  <code>glcon</code> \\ <code>gleaner</code> command in a <code>screen</code></p>"},{"location":"developers/services-infrastructure/data_loading/using_screen_for_manual_loading/#start-a-screen","title":"start a screen","text":"<p>Since this is a long running process, it is suggested that this be done in <code>screen</code> There is also a possibility of using tmux, if a user has experience with it.</p> <p>But someonee else needs to write that up.</p>"},{"location":"developers/services-infrastructure/data_loading/using_screen_for_manual_loading/#notes","title":"Notes:","text":"<p>To create a 'named' screen named gleaner</p> <p><code>screen -S gleaner</code></p> <p>to detach from screen, use: <code>ctl-a-d</code></p> <p>to find running screens: <code>screen -ls</code></p> <pre><code>There are screens on:\n    7187.gleaner    (07/28/22 17:43:48) (Detached)\n    6879.pts-3.geocodes-dev (07/28/22 17:33:25) (Detached)\n2 Sockets in /run/screen/S-ubuntu.</code></pre> <p>to attach to a screen  in this case you use the name</p> <p><code>screen -r gleaner</code></p> <p>or</p> <p><code>screen -r pts-3</code></p> <pre><code>screen -S gleaner\nubuntu@geocodes-dev:~/indexing$ screen -ls\nThere is a screen on:\n    7187.gleaner    (07/28/22 17:43:48)   (Attached)\n1 Socket in /run/screen/S-ubuntu.</code></pre>"},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/","title":"Data Loading - Watching CLI/Manual Loading","text":""},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/#when-you-are-doing-a-manual-load-things-you-can-observe","title":"When you are doing a manual load, things you can observe","text":"<ul> <li>console</li> <li>logs</li> <li>Minio/s3</li> </ul> <p>Note</p> <p>Remember to use <code>screen</code> for long data loads. This will make sure the process  does not die when a terminal disconnccts. <code>screen -S {SOME_NAME}</code></p>"},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/#console","title":"console","text":"<ul> <li><code>screen -ls</code></li> <li><code>screen r {SOME_NAME}</code> You should see a set of json records being reported.</li> </ul> <p>Note</p> <p>Nabu will provide a progress bar, but not anything into the main gleaner log.</p>"},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/#logs","title":"Logs","text":"<ul> <li><code>cd indexing</code> or whereever you ran glcon from</li> <li><code>ls -l logs</code> You will see a set of logs. initally, this will be there:</li> </ul> <pre><code>gleaner-2023-04-27-21-56-46.log</code></pre> <p>Then when sitemaps are loaded, reposiories will appear:</p> <pre><code>repo-magic-issues-2022-12-20-18-54-58.log\nrepo-magic-loaded-2022-10-06-15-40-07.log\nrepo-opentopography-issues-2022-10-05-22-04-03.log\nrepo-opentopography-loaded-2022-10-05-22-04-03.log</code></pre> <p>Note</p> <p>Headless repositories run after the not headless repositories. They also run serially, so it can take a long time to run headless. You can just run the headless as separate runs using the --source {SOURCE/REPO}</p> <pre><code>\ngleaner-runstats-2023-04-27-05-38-59.log</code></pre> <p>You can <code>tail -f logs/{file}</code></p>"},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/#minioadmins","title":"Minioadmin/s","text":"<p>In minioadmin you can see a bucket loading. Go to a minioadmin https://minioadmin.geocodes.ncsa.illinois.edu/</p> <p>Go to the bucket you are loading, summoned path select a repo, Sort by date to see what is the latest loaded (click twice)</p>"},{"location":"developers/services-infrastructure/data_loading/watching_manual_gleanerio_data_loading/#run-a-missing-report","title":"Run a missing report","text":"<p>If you run the missing_report you can do a quick idea of what did not make it in...  but there is still alot to go the there will be a lot of missing.</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/","title":"Configuration Examples","text":""},{"location":"developers/services-infrastructure/data_loading/configuration/#basics","title":"Basics","text":"<p>At present, there are two similar but different configuration files that are used by the two core applications: <code>gleaner</code> and <code>nabu</code> These can be generated using a command line tool: <code>glcon</code> when generating using<code>glcon</code>, a file called <code>localConfig.yaml</code> is edited, and a command generate generates  the two configuraiton files.</p> <p>Plans for the future are to refactor into two files, core services and sources</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/#services-and-sources","title":"Services and Sources","text":"<p>To load data you need to know the services and the sources. The services can be a remote cloud based, or  local usually running in a container (warn local is not always easy.)</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/#using-glcon-to-generate-configurations","title":"Using glcon to generate configurations","text":"<p>Step overview:</p> <ul> <li><code>./glcon config init --cfgName {projectname}</code></li> <li>edit configs/projectname/localConfig.yaml</li> <li><code>./glcon config generate --cfgName {projectname}</code></li> </ul> <pre><code>{%\n   include './template/localConfig.yaml'\n\n%}</code></pre>"},{"location":"developers/services-infrastructure/data_loading/configuration/#examples","title":"Examples","text":""},{"location":"developers/services-infrastructure/data_loading/configuration/#demo","title":"Demo","text":"<p>This is configured as a local</p> <pre><code>{%\n   include './demo/localConfig.yaml'\n\n%}</code></pre>"},{"location":"developers/services-infrastructure/data_loading/configuration/#flight-test","title":"Flight Test","text":"<p>This </p>"},{"location":"developers/services-infrastructure/data_loading/configuration/demo/readme/","title":"Readme","text":"<p>Demonstration to run against samples.earth</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/demo/readme/#demoyaml-is-a-hand-generated-file","title":"demo.yaml is a hand generated file","text":"<p>run demo using gleaner <pre><code>gleaner -cfg configs/demo/demo</code></pre> Note: no .yaml extension  at the end of the file</p> <p>run demo using glcon <pre><code>glcon gleaner batch -cfgFile  configs/demo/demo</code></pre></p>"},{"location":"developers/services-infrastructure/data_loading/configuration/demo/readme/#gleaner-configuration-directory-mode","title":"gleaner configuration directory mode","text":"<p>After running glcon config</p> <pre><code>glcon config generate  -cfgName  myDemo</code></pre> <p>gleaner: <pre><code>gleaner -cfg configs/demo/gleaner</code></pre> glcon: <pre><code>glcon gleaner batch -cfgName  demo</code></pre> Note: No filename needed. assumed to be gleaner</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/","title":"Configure Using glcon and Templates","text":"<p>You do not need to have a container stack running to run    <code>glcon config</code> But run <code>glcon gleaner</code> and <code>glcon nabu</code>, you will need to. </p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#overview-glcon-configuration-generation","title":"OVERVIEW glcon Configuration generation","text":"<p><code>glcon config</code> is used to create configuration files for gleaner and nabu The pattern is to intiialize a configuration directory, edit files, and generate new  configuration files for gleaner and nabu. Inside a configuration, you will need to edit a localConfiguration file Edit/add sources in a csv listing, and generate the configurations.</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#initialize-a-configuration-directory","title":"initialize a configuration directory","text":"<p>use  glcon command can intialize a configuration directory, and allow for the generation of gleaner and nabu configurations</p> <pre><code>glcon config init -cfgName test</code></pre> <p>initializes a configuration in configs with name of 'test' Inside you will find <pre><code>test % ls\ngleaner_base.yaml   readme.txt      sources.csv\nnabu_base.yaml      localConfig.yaml \nREADME_Configure_Template.md</code></pre></p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#edit-the-files","title":"EDIT the files","text":"<p>Usually, you will only need to edit the localConfig.yaml and sources.csv The localConfig.yaml</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#localconfigyaml","title":"localConfig.yaml","text":"<pre><code>---\nminio:\n  address: 0.0.0.0 # can be overridden with MINIO_ADDRESS\n  port: 9000 # can be overridden with MINIO_PORT\n  accessKey: worldsbestaccesskey # can be overridden with MINIO_ACCESS_KEY\n  secretKey: worldsbestsecretkey # can be overridden with MINIO_SECRET_KEY\n  ssl: false # can be overridden with MINIO_SSL\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: http://localhost/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n   type: csv\n   location: sources.csv</code></pre> <p>First, in the \"mino:\" section make sure the accessKey and secretKey here match the access keys for your minio. These can be overridden with the environent variables: * \"MINIO_ACCESS_KEY\" * \"MINIO_SECRET_KEY\"</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#sourcessource","title":"sourcesSource","text":"<p>sources.csv is utilized to generate the information needed to retrieve (summon), process (mill), upload (prefix), and cull old records (prune) This is done in order to facilitate the managing a list of sources, in a spreadsheet, rather than a yamil file. a csv file with the fields below</p> <p>This is designed to be edited in a spreadsheet, or exported by url as csv from a google spreadsheet</p> <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,Domain,PID,Logo\n1,sitegraph,FALSE,aquadocs,AquaDocs,https://oih.aquadocs.org/aquadocs.json ,FALSE,https://aquadocs.org,http://hdl.handle.net/1834/41372,\n3,sitemap,TRUE,opentopography,OpenTopography,https://opentopography.org/sitemap.xml,FALSE,http://www.opentopography.org/,https://www.re3data.org/repository/r3d100010655,https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n,sitemap,TRUE,iris,IRIS,http://ds.iris.edu/files/sitemap.xml,FALSE,http://iris.edu,https://www.re3data.org/repository/r3d100010268,http://ds.iris.edu/static/img/layout/logos/iris_logo_shadow.png</code></pre> <p>Fields:  1. hack:a hack to make the fields are properly read. 2. SourceType : [sitemap, sitegraph, googledrive, api] type of source 3. Active: [TRUE,FALSE] is source active.  4. Name: short name of source. It should be one word (no space) and be lower case. 5. ProperName: Long name of source that will be added to organization record for provenance 6. URL: URL of sitemap or sitegraph. 7. Headless: [FALSE,TRUE] should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page.  This is true of some sites and it is supported but not currently auto-detected.  So you will need to know this and set it.  For most place, this will be false.    if the json-ld is generated in a page dynamically, then use , TRUE 8. Domain:  9. PID: a unique identifier for the source. Perfered that is is a research id. 10. Logo: while no longer used, logo of the source 11. googleapikeyenv: (ONLY NEEDED FOR type:googledrive) environment variable pointing to a google api key. 12. any additional feilds you wish. This might be used to generate information about sources for a website.</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#configuration-of-your-source","title":"Configuration of your source","text":"<p>You configure the source in the localConfig.yaml (or override via the command line) <pre><code># looks for Mysources.csv in configuration directory\nsourcesSource:\n   type: csv\n   location: Mysources.csv</code></pre> This can also be a remote url starting with http:// or https:// <pre><code># pulls from a google sheet\nsourcesSource:\n   type: csv\n   location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}</code></pre></p> <p>If you start the name with a '/' a full path to the file is assumed. <pre><code># file outside of local configuration directory\nsourcesSource:\n   type: csv\n   location: /home/user/ourSources.csv</code></pre></p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#override-sources-via-the-cli","title":"Override Sources via the CLI","text":"<p>pass --sourcemaps to generate:</p> <p><code>glcon config generate --cfgName test --sourcemaps \"My Sources.csv\"</code></p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#generate-the-configuration-files","title":"GENERATE the configuration files","text":"<p><pre><code>glcon generate -cfgName test</code></pre> This will generate files  'gleaner' and  'nabu' and make copies of the existing configuration files</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#some-gleaner-configuration-details","title":"Some Gleaner Configuration details","text":"<p>This is a summary of a few portions of the configuration files generated. More details are at: GleanerConfiguration.md </p> <p>Open the file 'gleaner', and you will see  is actually quite a bit information this file, but for this starting demo only a few things we need to worry about.  The default file will look like:</p> <pre><code>---\nminio:\n  address: 0.0.0.0\n  port: 9000\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestsecretkey\n  ssl: false\n  bucket: gleaner\ngleaner:\n  runid: runX # this will be the bucket the output is placed in...\n  summon: true # do we want to visit the web sites and pull down the files\n  mill: true\ncontext:\n  cache: true\ncontextmaps:\n  - prefix: \"https://schema.org/\"\n    file: \"./configs/schemaorg-current-https.jsonld\"\n  - prefix: \"http://schema.org/\"\n    file: \"./configs/schemaorg-current-https.jsonld\"\nsummoner:\n  after: \"\"      # \"21 May 20 10:00 UTC\"   \n  mode: full  # full || diff:  If diff compare what we have currently in gleaner to sitemap, get only new, delete missing\n  threads: 5\n  delay:  # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) \n  headless: http://127.0.0.1:9222  # URL for headless see docs/headless\nmillers:\n  graph: true\n# will be built from sources.csv\nsources:\n  - sourcetype: sitegraph\n    name: aquadocs\n    logo: \"\"\n    url: https://oih.aquadocs.org/aquadocs.json\n    headless: false\n    pid: http://hdl.handle.net/1834/41372\n    propername: AquaDocs\n    domain: https://aquadocs.org\n    active: false\n  - sourcetype: sitemap\n    name: opentopography\n    logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n    url: https://opentopography.org/sitemap.xml\n    headless: false\n    pid: https://www.re3data.org/repository/r3d100010655\n    propername: OpenTopography\n    domain: http://www.opentopography.org/\n    active: false</code></pre> <p>A few things we need to look at.</p> <p>First, in the \"mino:\" section make sure the accessKey and secretKey here are the ones you utlize. Note: blank these out, and used environment variables (TODO:Need to describe them)</p> <p>Next, lets look at the \"gleaner:\" section.  We can set the runid to something.  This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized.  It can be set to any lower case string with no spaces. </p> <p>The miller and summon sections are true and we will leave them that way.  It means we want Gleaner to both fetch the resources and process (mill) them.  </p> <p>Now look at the \"miller:\"  section when lets of pick what milling to do.   Currently it is set with only graph set to true.  Let's leave it that way for now.  This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process.  </p> <p>The final section we need to look at is the \"sources:\" section.  Here is where the fun is.  While there are two types, sitegraph and sitemaps we will normally use sitemap type.  There is a third type that involves configuring and pulling from a</p> <p>A standard sitemap is below: <pre><code>sources:\n  - sourcetype: sitemap\n      name: opentopography\n      logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\n      url: https://opentopography.org/sitemap.xml\n      headless: false\n      pid: https://www.re3data.org/repository/r3d100010655\n      propername: OpenTopography\n      domain: http://www.opentopography.org/\n      active: true</code></pre></p> <p>A sitegraph  <pre><code>sources:\n  - sourcetype: sitegraph\n    name: aquadocs\n    logo: \"\"\n    url: https://oih.aquadocs.org/aquadocs.json\n    headless: false\n    pid: http://hdl.handle.net/1834/41372\n    propername: AquaDocs\n    domain: https://aquadocs.org\n    active: false</code></pre> A google drive <pre><code>sources:\n   - sourcetype: googledrive\n     name: ecrr_submitted\n     logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\n     url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\n     headless: false\n     pid: \"\"\n     propername: Earthcube Resource Registry\n     domain: http://www.earthcube.org/resourceregistry/\n     active: true\n     GoogleServiceJsonEnv: GOOGLEAPIAUTH\n     # see below. Enviroment variable name that contains filename of service_account.json authentication</code></pre> These are the sources we wish to pull and process.  Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value.  It was used in the past to provide a page showing all the sources and  a logo for them.  However, that's really just out of scope for what we want to do.  You can leave it blank or set it to any value, it wont make a difference.  </p> <p>The name is what you want to call this source.  It should be one word (no space) and be lower case. </p> <p>The url value needs to point to the URL for the site map XML file.  This will be created and served by the data provider. </p> <p>The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page.  This is true of some sites and it is supported but not currently auto-detected.  So you will need to know this and set it.  For most place, this will be false. </p> <p>You can have as many sources as you wish.  For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml</p>"},{"location":"developers/services-infrastructure/data_loading/configuration/template/README_Configure_Template/#google-drive","title":"Google Drive","text":"<p>The key GoogleServiceJson Env points to a env variable name that contains the path to service_account.json file <code>setenv GOOGLEAPIAUTH \"configs/credentials/gleaner-331805-030e15e1d9c4.json\"</code></p> <p>Create service credentials https://developers.google.com/workspace/guides/create-credentials</p> <p>save in credentials/{filename}</p> <p>The URL will be the folder you see in google drive.</p> <p>A google drive <pre><code>sources:\n   - sourcetype: googledrive\n     name: ecrr_submitted\n     logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\n     url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\n     headless: false\n     pid: \"\"\n     propername: Earthcube Resource Registry\n     domain: http://www.earthcube.org/resourceregistry/\n     active: true\n     GoogleServiceJsonEnv: GOOGLEAPIAUTH\n     # see below. Enviroment variable name that contains filename of service_account.json authentication</code></pre></p>"},{"location":"developers/services-infrastructure/development/","title":"Developing local UI","text":""},{"location":"developers/services-infrastructure/development/#user-interface","title":"User Interface","text":"<p>Presently the local development is used for the UI</p> <p>grab the  facetsearch repository</p> <p>run the server</p> <p>run the client</p> <p>We do development in Jetbrains Webstorm</p>"},{"location":"developers/services-infrastructure/development/#full-stack","title":"full stack","text":"<p>see Local Stack</p>"},{"location":"developers/services-infrastructure/development/local_stacks/","title":"Local or Developer Stacks","text":"<p>TODO GRAPH OF STACK... or say see index ;)</p> <p>Container Stacks: * services * geocodes</p>"},{"location":"developers/services-infrastructure/development/local_stacks/#starting","title":"starting.","text":"<ul> <li>copy env.local.example to .env  <code>./run_local.sh</code></li> <li>or copy env.local.example to yourconfig.env</li> <li><code>./run_local.sh -e yourconfig.env</code></li> </ul> <p>Ports/Services see Stack Machines</p> <p>====</p>"},{"location":"developers/services-infrastructure/development/local_stacks/#is-it-running","title":"is it Running?","text":"<p>traefik admin</p> <p>Graph: http://localhost:9999/blazegraph</p>"},{"location":"developers/services-infrastructure/development/localdeveloper_configs/","title":"Production configuration settings.","text":"<p>This is just a list of the customized sections of the 'production' configurations You should be able to 'glean' information needed about what servers and sources are being utilized.</p> <p>Note</p> <p>You will need to customize these for each server.</p> service config servers source production geocodes geocodes-1 production from  sources geocodes-dev geocodes_all geocodes-dev sources from sources sheet beta geocodes_all geocodes-dev sources from sources sheet alpha geocodes_all geocodes-1 sources from sources sheet wifire wifire geocodes-dev wifire from sources sheet ** BETA AND ALPHA NEED TO BE UPDATED  to the latest tenant with updated configs and config/facet_search_{project} ** service servers notes production geocodes-1 Runs vetted Data geocodes-dev geocodes-dev All sources beta geocodes-dev config/facets_serarch_beta point at geocodes-dev services? alpha geocodes-1 config/facets_serarch_alpha pointed at geocodes-1 services wifire gecodes-dev tenant <p>** Alpha and Beta ** are user interface testing clients, so while tenants, they are using  the data sources for production and gecodes-dev (all sources). These can be changed as needed.</p> <p>Production service naming logic</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p>"},{"location":"developers/services-infrastructure/development/localdeveloper_configs/#production-geocodesearthcubeorg","title":"Production geocodes.earthcube.org","text":"<p>docker config: geocodes-1 configs/onfigs/facet_search</p> <p>Production is a subset of the sources that have been vetted.</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nGLEANER_PORTAINER_DOMAIN=portainer.geocodes-1.earthcube.org\nGLEANER_ADMIN_DOMAIN=admin.geocodes-1.earthcube.org\nGLEANER_OSS_DOMAIN=oss.geocodes-1.earthcube.org\nGLEANER_OSS_CONSOLE_DOMAIN=minioadmin.geocodes-1.earthcube.org\nGLEANER_GRAPH_DOMAIN=graph.geocodes-1.earthcube.org\nGLEANER_WEB_DOMAIN=web.geocodes-1.earthcube.org\nGLEANER_SPARQLGUI_DOMAIN=sparqlui.geocodes.earthcube.org\nGLEANER_GRAPH2_DOMAIN=graph2.geocodes-1.earthcube.org\nGC_CLIENT_DOMAIN=geocodes.earthcube.org\nGEODEX_BASE_DOMAIN=geodex.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY=gleaner\nMINIO_SERVICE_SECRET_KEY=addtoearthcube\nFUSEKI_ADMIN_PASSWORD=earthcubeAdmin1!\nGLEANER_TRAEFIK_YML=traefik_data\nGLEANER_TRAEFIK=traefik_data\nGLEANER_OBJECTS=minio\nGLEANER_GRAPH=graph\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nTRAEFIK_AUTH={snip}\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=gleaner\nS3SECRET=adtoearthcube\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE='${bucketpath}/${reponame}/${sha}.jsonld'\nTOOLTEMPLATE='${bucketpath}/${reponame}/${ref}.json'\nTOOLBUCKET=ecrr\nTOOLPATH=summoned</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre></p> <p>config/facets_search <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/summary2/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-1.earthcube.org/ec/api/${o}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/development/localdeveloper_configs/#-","title":"----------","text":""},{"location":"developers/services-infrastructure/development/localdeveloper_configs/#geocodes-devstaging-geocodesgeocodes-devearthcubeorg","title":"Geocodes-dev/staging  geocodes.geocodes-dev.earthcube.org","text":"<p>docker config: geocodes-dev configs/facet_search</p> <p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  #  location: sources.csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.geocodes-dev.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-dev.earthcube.org/ec/api/${o}\n# oauth issues. need to add another auth app for additional 'proxies'\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/development/localdeveloper_configs/#wifire","title":"wifire","text":"<p>docker config: geocodes-dev   configs/wifire</p> <p>environment <pre><code>HOST=geocodes-dev.earthcube,org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=geocodes.wifire-data.sdsc.edu\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/wifire/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET=wifire\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET=OAUTH SECRET\nGC_GITHUB_CLIENTID=OAUTH APP ID\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: wifire # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\ns3:\n  bucket: wifire # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>config/facets_config_wifire  <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.wifire-data.sdsc.edu/ec/api\n#TRIPLESTORE_URL: https://graph.geodex.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire_summary/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/development/localdeveloper_configs/#alpha-needs-to-be-updated-alphageocodesearthcubeorg","title":"Alpha needs to be updated - alpha.geocodes.earthcube.org","text":"<p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code></code></pre></p>"},{"location":"developers/services-infrastructure/ecrr/","title":"notes on ECRR","text":"<p>Original ECCR Google Drive  * Resource Registry Project  * ECRR Files</p>"},{"location":"developers/services-infrastructure/ecrr/#rlcone-to-copy-from-google-drive-to-s3","title":"Rlcone to copy from google drive to s3.","text":"<ul> <li>install rclone</li> </ul> <p><code>rclone config</code></p> <p>add a ecrr_gdrive</p> <p>make a link to the ECRR in your google drive.</p> <p>configure advanced to all for access to shared</p> <p>rclone ls ecrr_gdrive:RegistryProject2019/ECRR_Resources --max-depth 1</p>"},{"location":"developers/services-infrastructure/production/","title":"Production","text":"Layout of Geocodes Stacks and Containers <pre><code>flowchart TB\n    subgraph Base Machine Stacks\n      subgraph base\n         traefik[traefik routing]\n         portainer[portainer container admin]\n      end\n      subgraph services\n         oss[\"oss s3\"]\n         sparqlgui\n         triplestore[\"graph -- triplestore\"]\n      end\n      subgraph geocodes\n         facetsearch--&gt;facetsearchservices\n      end\n      subgraph gleaner\n         headless\n      end\n  end\n</code></pre>"},{"location":"developers/services-infrastructure/production/#pages","title":"Pages:","text":"<ul> <li>Prodcution UI DNS/Server changes</li> <li>Creating Configurations for Production<ul> <li>Production Configuration Fragments</li> </ul> </li> <li>Managing Services</li> <li>Testing/Onboarding a Datasource</li> <li>Maanging Geocodes UI Containers</li> <li>Some Sparql queries</li> <li>Notebook Proxy Container</li> </ul>"},{"location":"developers/services-infrastructure/production/#machines","title":"Machines:","text":"<ul> <li>geodex.org</li> <li>geocodes.earthcube.org</li> </ul>"},{"location":"developers/services-infrastructure/production/#builds","title":"Builds:","text":"<ul> <li>Containers<ul> <li>dockerhub nsfearthcube</li> </ul> </li> <li>actions</li> <li>github</li> </ul>"},{"location":"developers/services-infrastructure/production/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developers/services-infrastructure/production/#i-refreshed-and-i-get-a-404","title":"I refreshed and i get a 404:","text":"<ol> <li>See if container is running in portainer</li> <li>Look at the portainer container log</li> <li>See if the treafik is mangled.</li> </ol> <p><code>https://admin.geodex.org/dashboard/#/</code>    (admin:password)</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/","title":"Production Configuration","text":"<p>setting up the 'production' config files This example is using geocodes-dev.earthcube.org Should this be an actual production configuration, the names need to be changed to protect the innocent</p> <p>Fragments from production configs for cribbing are in production_configs.md:</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#overview","title":"Overview","text":"<ol> <li>Setup Datastores</li> <li>install glcon</li> <li>create a new configuration directory</li> <li>edit the local config for the configuration</li> <li>Generate the configuration files for gleaner and nabu</li> <li>setup minio using glcon gleaner setup</li> <li>start a screen (adds ability to run long running processes)</li> <li>run <code>gleaner batch</code> ||  <code>glcon gleaner batch</code></li> <li>run <code>nabu prefix</code> ||  <code>glcon nabu prefix</code></li> <li>there needs to be a note above can be replaced with  <code>glcon nabu release --cfgName CONFIG</code>, and how to upload the quads</li> </ol>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#reconfiguration","title":"Reconfiguration","text":"<ol> <li>Changes to the gleaner/nabu configuration or the sources spreadsheet, other other items<ol> <li>make a change</li> <li>regenerate configs  <code>glcon config generate</code></li> <li>run <code>gleaner batch</code> ||  <code>glcon gleaner batch</code></li> </ol> </li> <li>run <code>nabu prefix</code> ||  <code>glcon nabu prefix</code></li> <li>run <code>nabu prune</code> ||  <code>glcon nabu prune</code><ol> <li>if files were removed from a repo, then this should prune them.</li> </ol> </li> </ol> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#setup-datastores","title":"Setup Datastores","text":"<p>If you followed the (setup indexing)[../setup_indexing_with_gleanerio.md] steps, you should have the datastores needed already created.  The production presently uses the earthcube repository convention, and that is what this document will use</p> Repository config s3 Bucket graph namespaces notes GeocodesTest gctest gctest gctest, gctest_summary samples of actual datasets geocodes geocodes geocodes geocodes, geocodes_summary suggested standalone repository earthcube geocodes gleaner earthcube, summary DEFAULT PRODUCTION NAME A COMMUNITY eg {acomm} {acomm} {acomm}, {acomm}_summary A communities tenant repository"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#install-glcon","title":"Install glcon","text":"<pre><code>cd \nls indexing</code></pre> <p>If glcon does not exist Install glcon</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#create-a-new-configuration-directory","title":"create a new configuration directory","text":"<pre><code>cd indexing\n./glcon config init --cfgName geocodes</code></pre> <p>See that it is created.</p> <p><code>ls configs/</code></p> <p><code>ls configs/geocodes</code></p> <p>note there a only a few files.</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#edit-the-local-config-for-the-configuration","title":"Edit the local config for the configuration","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/ci/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: gleaner\n  # bucket: can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.{HOST}/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner\n  #  sync with above... bucket: can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv \n# this is a remote csv, of the Vetted sources (though it's called Test)\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n# this has all sources\n#      location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n</code></pre> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p> <p>values need to match your {myhost}.env file</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#generate-the-configuration-files-for-gleaner-and-nabu","title":"Generate the configuration files for gleaner and nabu","text":"<code>./glcon config generate --cfgName geocodes</code> <pre><code>./glcon config generate --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \ngenerate called\n{SourceType:sitemap Name:balto Logo:http://balto.opendap.org/opendap/docs/images/logo.png URL:http://balto.opendap.org/opendap/site_map.txt  Headless:false PID:http://balto.opendap.org ProperName:Balto Domain:http://balto.opendap.org Active:false CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:neotomadb Logo:https://www.neotomadb.org/images/site_graphics/Packrat.png URL:http://data.neotomadb.org/sitemap.xml Headless:true PID:http://www.re3data.org/repository/r3d100011761 ProperName:Neotoma Domain:http://www.neotomadb.org/ Active:false CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n#{SNIP]\n{SourceType:sitemap Name:usap-dc Logo:https://www.usap-dc.org/ URL:https://www.usap-dc.org/view/dataset/sitemap.xml Headless:true PID:https://www.re3data.org/repository/r3d100010660 ProperName:U.S. Antarctic Program Data Center Domain:https://www.usap-dc.org/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:cchdo Logo:https://cchdo.ucsd.edu/static/svg/logo_cchdo.svg URL:https://cchdo.ucsd.edu/sitemap.xml Headless:false PID:https://www.re3data.org/repository/r3d100010831 ProperName:CLIVAR and Carbon Hydrographic Data Office Domain:https://cchdo.ucsd.edu/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\n{SourceType:sitemap Name:amgeo Logo:https://amgeo.colorado.edu/static/img/amgeosmall.svg URL:https://amgeo-dev.colorado.edu/sitemap.xml Headless:false PID: ProperName:Assimilative Mapping of Geospace Observations Domain:https://amgeo.colorado.edu/ Active:true CredentialsFile: Other:map[] HeadlessWait:0 Delay:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre> <p>Check: <code>ls configs/geocodes</code></p> <p>Now there will be at least a 'gleaner', a 'nabu' and a 'nabu_prov' files.</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#setup-minio","title":"setup minio","text":"<code>./glcon gleaner setup --cfgName geocodes</code> <p>This only needs to be done once <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/geocodes/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/geocodes/nabu\nsetup called\n{\"file\":\"/github/workspace/pkg/gleaner.go:60\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Validating access to object store\",\"time\":\"2022-07-28T17:32:51Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:67\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Setting up buckets\",\"time\":\"2022-07-28T17:32:51Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:78\",\"func\":\"github.com/gleanerio/gleaner/pkg.Setup\",\"level\":\"info\",\"msg\":\"Buckets generated.  Object store should be ready for runs\",\"time\":\"2022-07-28T17:32:51Z\"}</code></pre></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#start-a-screen","title":"Start a screen","text":"<p>Since this is a long running process, it is suggested that this be done in <code>screen</code> There is also a possibility of using tmux, if a user has experience with it.</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#notes","title":"Notes:","text":"<p>To create a 'named' screen named gleaner </p> <p><code>screen -S gleaner</code></p> <p>to detach from screen, use: <code>ctl-a-d</code> </p> <p>to find running screens: <code>screen -ls</code></p> <pre><code>There are screens on:\n    7187.gleaner    (07/28/22 17:43:48) (Detached)\n    6879.pts-3.geocodes-dev (07/28/22 17:33:25) (Detached)\n2 Sockets in /run/screen/S-ubuntu.</code></pre> <p>to attach to a screen  in this case you use the name </p> <p><code>screen -r gleaner</code></p> <p>or</p> <p><code>screen -r pts-3</code></p> <pre><code>screen -S gleaner\nubuntu@geocodes-dev:~/indexing$ screen -ls\nThere is a screen on:\n    7187.gleaner    (07/28/22 17:43:48)   (Attached)\n1 Socket in /run/screen/S-ubuntu.</code></pre>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-gleaner-to-pull-jsonld-from-sitemaps","title":"Run Gleaner to pull JSONLD from sitemaps","text":"<p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> <p>** Are you running, like suggested in a screen**</p> <code>./glcon gleaner batch --cfgName geocodes</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName geocodes\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/geocodes/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/geocodes/nabu\nbatch called\n{\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:40\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Processing sitegraph file (this can be slow with little feedback):https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:41\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Downloading sitegraph file:https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:18Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:53\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Sitegraph file downloaded. Uploading togleanerhttps://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:57Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/sitegraph.go:60\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.GetGraph\",\"level\":\"info\",\"msg\":\"Sitegraph file uploaded togleanerUploaded :https://oih.aquadocs.org/aquadocs.json\",\"time\":\"2022-07-28T17:34:59Z\"}\n\n[SNIP]</code></pre> <p>You can now detach,</p> <p><code>ctl-a-d</code></p> <p>watch the logs <code>tail -f  logs/gleaner{somedate pattern}log</code></p> <p>to attach to a screen  in this case you use the name</p> <p><code>screen -r gleaner</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-nabu-prefix-to-upload-data-to-graph-store","title":"run nabu prefix  to upload data to graph store","text":"<p>when gleaner is complete</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prefix --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-nabu-prefix-to-prov-to-graph-store","title":"Run nabu prefix to prov to graph store","text":"<p>This uses a separate config, for now.</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prefix --cfg configs/geocodes/nabuprov</code></p> <p>Note</p> <p>The above can be replaced with  <code>glcon nabu release --cfgName CONFIG</code>,   Need Note on how to do this and how to upload the quads</p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-nabu-prune-to-cullremove-data-to-graph-store","title":"run nabu prune  to cull/remove data to graph store","text":"<p>when gleaner is complete</p> <p>IF detached,  attach to a screen  in this case you use the name <code>screen -r gleaner</code></p> <p><code>./glcon nabu prune --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#changes-that-will-be-needed-for-the-client-configuration","title":"Changes that will be needed for the client configuration","text":"<p>production model for post step 4</p> <p>Portions of deployment/facets/config.yaml that might be changed. This is for production. IF you completed the initial data load using gctest, then you can modify and rebuild the geecodes stack using Updating a GEOCODES CLIENT Configuration production configuration in Manging Geocodes UI containers</p> production section of deployment/facets/config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/earthcube/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\"\n\nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#reconfiguration_1","title":"Reconfiguration","text":""},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#changes-to-the-gleanernabu-configuration-or-the-sources-spreadsheet","title":"Changes to the gleaner/nabu configuration or the sources' spreadsheet","text":"<p>If you change the {config}/localConfig.yaml or you update the source google sheet, then you need to regenerate the config files</p> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#regenerate-configs","title":"regenerate configs","text":"<p><code>./glcon config generate --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-batch","title":"run batch","text":"<p><code>./glcon gleaner batch --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-nabu-prefix","title":"run nabu prefix","text":"<p><code>./glcon nabu prefix --cfgName geocodes</code></p>"},{"location":"developers/services-infrastructure/production/creatingAndLoadingProduction/#run-nabu-prune","title":"run nabu prune.","text":"<p><code>./glcon nabu prune --cfgName geocodes</code></p> <p>if files were removed from a repo, then this should prune them.</p>"},{"location":"developers/services-infrastructure/production/fuseki/","title":"Fuseki","text":"<p>Fuseki service uses a CONFIG in portainer. * upload the earthcube.ttl as a config file * open the graph2 seervice, add the config to: /fuseki-base/configuration/earthcube.ttl</p> <p>https://graph2.geocodes-dev.earthcube.org/$/stats</p> <p>https://graph2.geocodes-dev.earthcube.org/$/metrics</p>"},{"location":"developers/services-infrastructure/production/geocodes_notebook_proxy_notes/","title":"notebook Proxy","text":"<p>Env variables</p> <pre><code>AUTH_MODE=server\nGITHUB_SECRET={KEY}\nGITHUB_CLIENTID=b5a5494cf21096a99e37</code></pre> <p>Note: In portainer, you may need to go into the service to update the ENV keys, there.</p>"},{"location":"developers/services-infrastructure/production/geocodes_notebook_proxy_notes/#github-authorization","title":"Github authorization","text":"<p>The app is setup to run in a service.</p> <p>The app is here: https://github.com/organizations/earthcube/settings/applications/1768280</p> <p>Authorization callback URL: https://geocodes.earthcube.org/notebook/auth</p>"},{"location":"developers/services-infrastructure/production/geodex.org/","title":"Geodex.org","text":"<p>log into geodex.org</p> <p>notes: update the version on the on facetsearch for it to know you got the correct one. <code>cd code/geodex</code></p> <p><code>./refresh_containers.sh</code></p> <p><code>./restart_all.sh</code></p> <p>to find an issue</p> <p><code>cat restart_all.sh</code></p> <p>grab the command line</p> <p>change up -d to</p> <p>log {service2view}</p> <p>eg</p> <p><code>docker-compose --env-file env.beta -f headless-only.yml -f geocodes-compose.yaml  -f geodex-compose.yml   logs vue-services</code></p> <p>You canfigure out the running endpoints from the /config route.</p> <p>http://localhost:3000/config https://geocodes.earthcube.org/ec/api/config</p> <p>NOTES: Things borked... match to staging or dev config... be careful, dev config points to localhost, so do not blindly copy</p> <p>Inside the geocodes.yaml there is a machine path.. <pre><code>vue-services:\nimage: nsfearthcube/ec_facets_api_nodejs:latest\n#build: ./server\nrestart: unless-stopped\nenvironment:\n- NODE_ENV=production\n#- S3ADDRESS=s3system:9000\n- S3ADDRESS=oss.geocodes.earthcube.org\n- S3BUCKET=sites\n- S3PREFIX=alpha\n- DOMAIN=https://${HOST:?HOST environment varaible is required}/\n- S3KEY=${S3KEY}\n- S3SECRET=${S3SECRET}</code></pre></p>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/","title":"Manging Geocodes UI containers","text":""},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#updating-a-geocodes-client-configuration","title":"Updating a GEOCODES CLIENT Configuration","text":"<p>You can modify the facets_config config, in order to do this, stop the stack, delete the config and recreate the config.</p> <ol> <li>go to portainer,</li> <li>select geocodes_geocodes, stop</li> <li>select config, facets_config, copy content, select delete</li> <li>create a new config with name 'facets_config', paste in content</li> <li>modify content, save</li> <li>restart stack</li> <li>update the service<ol> <li>services, geocodes_vue-client or geocodes_xxx_vue-client</li> <li>udate the service    *** NOTE: TRY A SECOND BROWSER... and/or Clear browser cache ****</li> <li>If that does not work, check to see in services if the correct container image is being pulled.</li> </ol> </li> <li>Then go to containers, geocodes_vue-client or geocodes_xxx_vue-client<ol> <li>remove container. It will rebuild if it is not stopped</li> </ol> </li> </ol>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#developers-testing-a-ui-branch-in-portainerdocker","title":"DEVELOPERS: Testing a UI Branch in Portainer/Docker","text":"<p>:memo: An Update May be needed. You can now deploy a tennant configuration, which many mean that geocodes repo changes may not be needed</p> <p>:memo: you should do local development before deployment testing</p> <p>To do this we will need to do two branches, one on the facet search, and one on the services stack geocodes. Or, you can disconnect your development services</p>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#facetsearch-repository-changes","title":"Facetsearch repository changes","text":"<ul> <li>create a branch<ul> <li>on that branch edit the github workflows/docker_xxx add your branch</li> </ul> </li> </ul> <pre><code>on:\n  push:\n    branches:\n    - master\n    - feat_summary</code></pre> <ul> <li>make changes and push</li> </ul>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#geocodes-repository-changes","title":"geocodes repository changes","text":"<ul> <li>create a branch</li> <li>modify   deployment/geocodes-compose.yaml</li> </ul> <pre><code>vue-services:\n  image: nsfearthcube/ec_facets_api_nodejs:{{BRANCH NAME}}</code></pre> <pre><code>vue-client:\n  image: nsfearthcube/ec_facets_client:{{BRANCH NAME}}</code></pre>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#deployment-in-in-portainer","title":"Deployment in in portainer","text":"<ul> <li>create a new stack</li> <li>under advanced configuration   ??? example \"stack deploy from a branch\"   </li> <li>save</li> <li>pull and deploy</li> </ul>"},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"developers/services-infrastructure/production/managing_geocodes_ui_containers/#seems-like-the-container-is-not-getting-updated","title":"seems like  the container is not getting updated","text":"<p>occasionally, a branch is being used for a stack. This will  be true of alpha/beta/tennant containers.</p> <ul> <li>open stack</li> <li>user Redeploy from Git: select advanced configuration</li> <li>change the branch information</li> </ul> stack deploy from a branch <p></p> <p>Occasionally, the latest will not be pulled, Seen  when I  change a branch,</p> <ul> <li>open services,</li> <li>select a service,</li> <li>go down to Change container image</li> <li>set to the appropriate container path.</li> </ul> stack container path  <p></p>"},{"location":"developers/services-infrastructure/production/managing_services/","title":"notes on managing Services","text":"<ul> <li>minio</li> <li>blazegraph</li> </ul>"},{"location":"developers/services-infrastructure/production/managing_services/#minio","title":"Minio","text":""},{"location":"developers/services-infrastructure/production/managing_services/#what-servers-exist-in-the-minio-configuration","title":"what servers exist in the minio configuration","text":"<p><code>mc alias ls</code></p>"},{"location":"developers/services-infrastructure/production/managing_services/#minio-add-configuration-for-a-server","title":"minio add configuration for a server","text":"<p>Note the single quotes around the password... some passwords are   not command line friendly <pre><code>set +o history\nmc config host add dev https://oss.geocodes.earthcube.org  {miniouser} '{miniopassword}'\n\nset -o history</code></pre></p>"},{"location":"developers/services-infrastructure/production/managing_services/#test","title":"test","text":"<pre><code>mc ls dev</code></pre>"},{"location":"developers/services-infrastructure/production/managing_services/#mino-sync-between-servers","title":"Mino sync between servers","text":"<pre><code>mc cp --recursive dev/ecrr/ gc1/ecrr/</code></pre>"},{"location":"developers/services-infrastructure/production/managing_services/#copy-log-files-to-minio","title":"copy log files to minio","text":"<pre><code>cd indexing/logs\nmc cp  --recursive . dev/gleaner/logs/{date}\nmc share download --recursive dev/gleaner/logs/{date}\n</code></pre>"},{"location":"developers/services-infrastructure/production/managing_services/#links-for-a-bucket","title":"links for a bucket:","text":"<p>(IGNORE: only works for top level of bucket... may need a better policy)</p> <pre><code>mc anonymous links --recursive dev/gleaner/summoned/amgeo</code></pre>"},{"location":"developers/services-infrastructure/production/managing_services/#statistics-for-a-bucket","title":"statistics for a bucket","text":"<pre><code>mc ls dev/gleaner/summoned/{repo} --summarize --recursive\n\nmc ls dev/gleaner/summoned/amgeo --summarize --recursiv\n\n\nmc stat --recursive  dev/gleaner/summoned/{repo}</code></pre> <p>eg</p> <p>mc stat --recursive  dev/gleaner/summoned/amgeo</p>"},{"location":"developers/services-infrastructure/production/managing_services/#remove-old-records","title":"Remove old records:","text":"<pre><code>mc rm dev/gleaner/results --recursive --older-than 365d00h00m00s\nmc rm dev/gleaner/summoned --recursive --older-than 365d00h00m00s\nmc rm dev/gleaner/milled --recursive --older-than 365d00h00m00s</code></pre>"},{"location":"developers/services-infrastructure/production/managing_services/#blazegraph","title":"BLAZEGRAPH","text":""},{"location":"developers/services-infrastructure/production/managing_services/#cleaning-up-the-journal","title":"CLEANING UP THE JOURNAL","text":"<p>Blazegraph (and fuseki) will grow as data is added, its a journaled file system so it's not  cleaned up.</p> <p>The steps  are originally from (Medium.com)[https://medium.com/@nvbach91/how-to-reclaim-disk-space-in-blazegraph-95a47575f8a8]</p> <p>(MIKE WILL INSERT INSTRUCTIONS HERE)</p>"},{"location":"developers/services-infrastructure/production/managing_services/#deleting","title":"deleting:","text":"<p>https://www.w3.org/TR/sparql11-update/#clear <pre><code>`CLEAR ALL`\n## Clear a graph\n`CLEAR GRAPH earthcube:{iri}`</code></pre></p>"},{"location":"developers/services-infrastructure/production/portainer_docker_swarm/","title":"Notes on setting up Portainer to connect to a docker swarm.","text":""},{"location":"developers/services-infrastructure/production/portainer_docker_swarm/#notes","title":"Notes:","text":"<p>Docker Swarm wants port 9001. Not truely needed since it's the minioadminport, that can be routed via trafik</p> <p>Created an edge agent https://docs.portainer.io/start/agent/edge</p>"},{"location":"developers/services-infrastructure/production/portainer_docker_swarm/#openstack","title":"Openstack","text":"<pre><code>&gt;docker info\nubuntu@geocodes-1:~$ docker info\nClient:\n Context:    default\n Debug Mode: false\n Plugins:\n  app: Docker App (Docker Inc., v0.9.1-beta3)\n  buildx: Docker Buildx (Docker Inc., v0.9.1-docker)\n  compose: Docker Compose (Docker Inc., v2.12.2)\n  scan: Docker Scan (Docker Inc., v0.21.0)\n\nServer:\n Containers: 38\n  Running: 12\n  Paused: 0\n  Stopped: 26\n Images: 47\n Server Version: 20.10.21\n Storage Driver: overlay2\n  Backing Filesystem: extfs\n  Supports d_type: true\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog\n Swarm: active\n  NodeID: ct9p7o7wrf5m1aq4njzh74ekh\n  Is Manager: true\n  ClusterID: j45b7si0xn5o1kmegv3zhza8l\n  Managers: 1\n  Nodes: 1\n  Default Address Pool: 10.0.0.0/8  \n  SubnetSize: 24\n  Data Path Port: 4789\n  Orchestration:\n   Task History Retention Limit: 5\n  Raft:\n   Snapshot Interval: 10000\n   Number of Old Snapshots to Retain: 0\n   Heartbeat Tick: 1\n   Election Tick: 10\n  Dispatcher:\n   Heartbeat Period: 5 seconds\n  CA Configuration:\n   Expiry Duration: 3 months\n   Force Rotate: 0\n  Autolock Managers: false\n  Root Rotation In Progress: false\n  Node Address: 10.128.57.9\n  Manager Addresses:\n   10.128.57.9:2377\n Runtimes: runc io.containerd.runc.v2 io.containerd.runtime.v1.linux\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 1c90a442489720eec95342e1789ee8a5e1b9536f\n runc version: v1.1.4-0-g5fd4c4d\n init version: de40ad0\n Security Options:\n  apparmor\n  seccomp\n   Profile: default\n  cgroupns\n Kernel Version: 5.15.0-52-generic\n Operating System: Ubuntu 22.04.1 LTS\n OSType: linux\n Architecture: x86_64\n CPUs: 8\n Total Memory: 31.35GiB\n Name: geocodes-1\n ID: YNXV:NTF4:5APE:UF2U:DTAN:CLW3:QQII:4NWH:FAFP:WRCA:N5GG:I5Q3\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Registry: https://index.docker.io/v1/\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\n\n</code></pre>"},{"location":"developers/services-infrastructure/production/production_configs/","title":"Production configuration settings.","text":"<p>This is just a list of the customized sections of the 'production' configurations You should be able to 'glean' information needed about what servers and sources are being utilized.</p> <p>Note</p> <p>You will need to customize these for each server.</p> service config servers source production geocodes geocodes-1 production from  sources geocodes-dev geocodes_all geocodes-dev sources from sources sheet beta geocodes_all geocodes-dev sources from sources sheet alpha geocodes_all geocodes-1 sources from sources sheet wifire wifire geocodes-dev wifire from sources sheet ** BETA AND ALPHA NEED TO BE UPDATED  to the latest tenant with updated configs and config/facet_search_{project} ** service servers notes production geocodes-1 Runs vetted Data geocodes-dev geocodes-dev All sources beta geocodes-dev config/facets_serarch_beta point at geocodes-dev services? alpha geocodes-1 config/facets_serarch_alpha pointed at geocodes-1 services wifire gecodes-dev tenant <p>** Alpha and Beta ** are user interface testing clients, so while tenants, they are using  the data sources for production and gecodes-dev (all sources). These can be changed as needed.</p> <p>Production service naming logic</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p>"},{"location":"developers/services-infrastructure/production/production_configs/#production-geocodesearthcubeorg","title":"Production geocodes.earthcube.org","text":"<p>docker config: geocodes-1 configs/onfigs/facet_search</p> <p>Production is a subset of the sources that have been vetted.</p> <p>In order to better handle the ability to point the 'client' at different endpoints and services The new pattern is that the main server has a basename  that is not only 'geocodes' 'geocodes-1' is the production service and it's services are affixed with geocodes-1.earthcube.org</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nGLEANER_PORTAINER_DOMAIN=portainer.geocodes-1.earthcube.org\nGLEANER_ADMIN_DOMAIN=admin.geocodes-1.earthcube.org\nGLEANER_OSS_DOMAIN=oss.geocodes-1.earthcube.org\nGLEANER_OSS_CONSOLE_DOMAIN=minioadmin.geocodes-1.earthcube.org\nGLEANER_GRAPH_DOMAIN=graph.geocodes-1.earthcube.org\nGLEANER_WEB_DOMAIN=web.geocodes-1.earthcube.org\nGLEANER_SPARQLGUI_DOMAIN=sparqlui.geocodes.earthcube.org\nGLEANER_GRAPH2_DOMAIN=graph2.geocodes-1.earthcube.org\nGC_CLIENT_DOMAIN=geocodes.earthcube.org\nGEODEX_BASE_DOMAIN=geodex.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY=gleaner\nMINIO_SERVICE_SECRET_KEY=addtoearthcube\nFUSEKI_ADMIN_PASSWORD=earthcubeAdmin1!\nGLEANER_TRAEFIK_YML=traefik_data\nGLEANER_TRAEFIK=traefik_data\nGLEANER_OBJECTS=minio\nGLEANER_GRAPH=graph\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nTRAEFIK_AUTH={snip}\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=gleaner\nS3SECRET=adtoearthcube\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE='${bucketpath}/${reponame}/${sha}.jsonld'\nTOOLTEMPLATE='${bucketpath}/${reponame}/${ref}.json'\nTOOLBUCKET=ecrr\nTOOLPATH=summoned</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestSources202210\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre></p> <p>config/facets_search <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/summary2/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-1.earthcube.org/ec/api/${o}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/production/production_configs/#-","title":"----------","text":""},{"location":"developers/services-infrastructure/production/production_configs/#geocodes-devstaging-geocodesgeocodes-devearthcubeorg","title":"Geocodes-dev/staging  geocodes.geocodes-dev.earthcube.org","text":"<p>docker config: geocodes-dev configs/facet_search</p> <p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  #  location: sources.csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.geocodes-dev.earthcube.org/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: https://geocodes.geocodes-dev.earthcube.org/ec/api/${o}\n# oauth issues. need to add another auth app for additional 'proxies'\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/production/production_configs/#wifire","title":"wifire","text":"<p>docker config: geocodes-dev   configs/wifire</p> <p>environment <pre><code>HOST=geocodes-dev.earthcube,org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=geocodes.wifire-data.sdsc.edu\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/wifire/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET=wifire\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET=OAUTH SECRET\nGC_GITHUB_CLIENTID=OAUTH APP ID\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: wifire # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\ns3:\n  bucket: wifire # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>config/facets_config_wifire  <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.wifire-data.sdsc.edu/ec/api\n#TRIPLESTORE_URL: https://graph.geodex.org/blazegraph/namespace/earthcube/sparql\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/wifire_summary/sparql\n#SUMMARYSTORE_URL: https://graph.geodex.org/blazegraph/namespace/summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></p>"},{"location":"developers/services-infrastructure/production/production_configs/#alpha-needs-to-be-updated-alphageocodesearthcubeorg","title":"Alpha needs to be updated - alpha.geocodes.earthcube.org","text":"<p>This would be a list of all sources and sitemaps.</p> <p>environment <pre><code>HOST=geocodes-1.earthcube.org\nFACET_SERVICES_FILE=./config/services.js\nGC_CLIENT_DOMAIN=alpha.geocodes.earthcube.org\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/earthcube/sparql\nS3ADDRESS=oss.geocodes-1.earthcube.org\nS3KEY=worldsbestaccesskey\nS3SECRET=worldsbestsecretkey\nS3SSL=true\nS3PORT=443\nBUCKET=gleaner\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=gcalpha</code></pre></p> <p>localConfig.yaml <pre><code>---\nminio:\n  address: oss.geocodes-1.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: gleaner # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/earthcube/sparql\ns3:\n  bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n#  location: sources.csv\n# this can be a remote csv\n#  type: csv\n  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=sources\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml\n</code></pre></p> <p>facets_search.yaml <pre><code></code></pre></p>"},{"location":"developers/services-infrastructure/production/production_ui_deployment/","title":"Production UI Deployment and Server Switches","text":""},{"location":"developers/services-infrastructure/production/production_ui_deployment/#requirements","title":"Requirements:","text":"<ul> <li>DNS Pointing at geocodes.earthcube.org  a</li> <li>Docker<ul> <li>secrets need to be set</li> <li>deploy configs facet_config_production</li> <li>create file env variables</li> <li>deploy deployment/geocodes-compose-production.yaml to portainer</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/production/production_ui_deployment/#gotchas","title":"GOTCHAS","text":"<p>Let's Encrypt has limits on production changes, so if you deploy the stack before the DNS, then you can lock y out changes for 7 days.</p>"},{"location":"developers/services-infrastructure/production/production_ui_deployment/#proposed-steps","title":"Proposed Steps","text":"<ul> <li>ask for NDS TIL (time to live) to be lowered.</li> <li>Create docker config files, secrets, and configs</li> <li>change DNS.</li> <li>create a production stack</li> <li>STOP PREVIOUS PRODUCTION STACK</li> </ul>"},{"location":"developers/services-infrastructure/production/production_ui_deployment/#notes","title":"Notes:","text":""},{"location":"developers/services-infrastructure/production/production_ui_deployment/#lets-encypt","title":"Let's encypt","text":""},{"location":"developers/services-infrastructure/production/production_ui_deployment/#dns","title":"DNS","text":"<p>We will now use a single DNS record for just the UI: geocodes.earthcube.org. This is fixed in the deployment/geocodes-compose-production.yaml</p> <p>Previously, before external file configuration (facet_config_production),  we had services at (graph|oss|minioadmin, etc).geocodes.earthcube.org.  It is easier to configure the client to point at server services (graph|oss|minioadmin, etc).host eg. (graph|oss|minioadmin, etc).geocodes-aws.earthcube.org where a wildcard DNS is setup.</p>"},{"location":"developers/services-infrastructure/production/production_ui_deployment/#docker-secrets","title":"Docker Secrets","text":"<p>We are begining to use docker secrets, so these need to be configured</p>"},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/","title":"Reconfigure Geocodes Services Containers for Production:","text":""},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/#setup-and-start-geocodes-client-using-portainer-ui","title":"Setup and start GeoCodes Client using portainer ui","text":"<p>Steps:</p> <ul> <li>Stop the geocodes stack</li> <li>copy, edit, delete, recreate the configuration file</li> <li>test</li> <li>instructions for Updating a GEOCODES CLIENT Configuration if things do not work<ul> <li>or delete stack and reload</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/#modify-the-facet-search-configuration","title":"Modify the Facet Search Configuration","text":"<ul> <li>edit in deployment/facets/config.yaml</li> <li>this file is mounted on the container as a docker config file<ul> <li>run the run_add_configs.sh</li> </ul> </li> </ul> <p>Portions of deployment/facets/config.yaml that might be changed.</p> portainer configs/facets_config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/gctest/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/gctest/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\" \nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/#create-geocodes-stack","title":"Create Geocodes Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>click add stack button <pre><code>Name: geocodes\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nreference: refs/heads/main\nCompose path: deployment/geocodes-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.geocodes.env</li> </ul> </li> <li>Actions:<ul> <li>Click: Deploy This Stack</li> </ul> </li> </ul> </li> </ul> Geocodes Stack"},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/#test-geocodes-client","title":"Test Geocodes Client","text":"<p>Issues</p> <p>IF things are not working in the UI, it is probably the facet search configuration You can take down the geocodes stack, and delete the config/facets_search or you can possibly just stop the gecodes_vue_ui service, and edit the facets_search config as noted here: See Managing Geocodes UI Containers</p> <ol> <li>Got to https://geocodes.{your host}/</li> <li>Got to configuration: https://geocodes.{your host}/#/config</li> <li>Two sections, one is the facests/config.yaml and the second is the API configuration (sanitized, we hope)</li> </ol> <p>Done</p> <p>This is the end of the deployment steps.</p>"},{"location":"developers/services-infrastructure/production/reconfigure_geocodes_ui_containers/#if-the-deployment-is-working-you-can-now","title":"If the deployment is working, you can now","text":"<ul> <li>(Setup a Production Configuration)[./production/creatingProductionConfigs.md]</li> <li>(Reconfigure Geocodes UI)[./production/reconfigure_geocodes_ui_containers.md]</li> </ul>"},{"location":"developers/services-infrastructure/production/sparql/","title":"Some SPARQL Queries","text":""},{"location":"developers/services-infrastructure/production/sparql/#is-there-data-in-the-namespace","title":"Is there data in the namespace:","text":""},{"location":"developers/services-infrastructure/production/sparql/#count","title":"count","text":"<pre><code>SELECT (count(*) as ?count) \nWHERE     { ?s ?p ?o}\n</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#triples","title":"triples","text":"<p>select all from all graphs (?g) <pre><code>SELECT *\nWHERE     {     GRAPH ?g {?s ?p ?o}}\nLIMIT 100</code></pre></p> <pre><code>SELECT *\nWHERE     {     GRAPH ?g {?s ?p ?o}}\nLIMIT 100</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#what-types-are-in-the-system","title":"what types are in the system","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n\n       ?s a ?type .\n\n       }\n}\n\nGROUP By ?type\nORDER By DESC(?scount)```\n\n## Dataset \n### count\nsubject is a rdf:type schema.org/Dataset\n```sparql\nSELECT (count(?g ) as ?count) \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}\n</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#triples_1","title":"triples","text":"<pre><code>SELECT *  \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}\nLIMIT 100</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#keyword-count","title":"Keyword count","text":""},{"location":"developers/services-infrastructure/production/sparql/#count-of-keywords","title":"count of keywords","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  (count(distinct ?keyword ) as ?scount)\nWHERE {\n  {\n\n       ?s schema:keywords ?keyword .\n\n       }\n}\n\nORDER By DESC(?scount)</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#keyword-counts","title":"Keyword counts","text":"<pre><code># needs work... keywords can be an array.\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?keyword (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:keywords ?keyword .\n\n       }\n}\nGROUP By ?keyword\nORDER By DESC(?scount)</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#publisher","title":"Publisher","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?pubname (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:publisher/schema:name|schema:sdPublisher ?pubname .\n       }\n}\nGROUP By ?pubname\nORDER By DESC(?scount)</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#variable-name","title":"Variable Name","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?variableName (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:variableMeasured ?variableMeasured .\n    ?variableMeasured schema:name ?variableName\n\n       }\n}\nGROUP By ?variableName\nORDER By DESC(?scount)</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#dataset-with-versions","title":"Dataset with versions","text":""},{"location":"developers/services-infrastructure/production/sparql/#list-of-version-numbers","title":"list of version numbers","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?version (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:version ?version .\n\n       }\n}\nGROUP By ?version\nORDER By DESC(?scount)</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#datasets-with-a-version-number","title":"datasets with a version number","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url where {\n    {SELECT distinct  ?sameAs (MAX(?version2) as ?version  )\n    where {\n       ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:version|sschema:version ?version2 .\n\n    }\n        GROUP BY ?sameAs\n}\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url\norder by ?sameAs ?version\nlimit 1000</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#datasets-with-multiple-versions","title":"Datasets with multiple versions","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url where {\n    {SELECT distinct  ?sameAs (MAX(?version2) as ?version  )\n    where {\n       ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:version|sschema:version ?version2 .\n    filter (?version2 &gt;1)\n    }\n        GROUP BY ?sameAs\n}\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url\norder by ?sameAs ?version</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#versions-for-earthreforg","title":"Versions for Earthref.org","text":"<pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url ?g where {\n   graph ?g {\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:sdPublisher|sschema:sdPublisher \"EarthRef.org\".\n    }\n        ?subj2 schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj2 schema:version|sschema:version ?version2 .\n\n    FILTER (?version &lt; ?version2).\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url ?g\n</code></pre>"},{"location":"developers/services-infrastructure/production/sparql/#latest-version-for-a-steens-query","title":"Latest version for a steens query","text":"<p>this includes a max version  <pre><code>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\nprefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\n# no longer works as expected. Identifier changed. Try to use sameAs as in other version examples.\nSELECT distinct ?subj  ?citation ?version ?pubname ?resourceType ?name  (GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n        (GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw) (MAX(?version) as ?latestVersion)\n        ?datep  (GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl) (MAX(?score1) as ?score) ?description ?g\n        WHERE {\n            ?lit bds:search \"steens\" .\n            ?lit bds:matchAllTerms false .\n            ?lit bds:relevance ?score1 .\n            ?subj ?p ?lit .\n            BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a sschema:Dataset .} , \"data\", \"tool\") AS ?resourceType).\n            filter( ?score1 &gt; 0.04).\n          graph ?g {\n            Minus {?subj a sschema:ResearchProject } .\n            Minus {?subj a schema:ResearchProject } .\n            Minus {?subj a schema:Person } .\n            Minus {?subj a sschema:Person } .\n             ?subj schema:name|sschema:name ?name .\n             ?subj schema:description|sschema:description ?description .\n             }\n             ?subj schema:citation|sschema:citation ?citation .\n             ?subj schema:version|sschema:version ?version .\n            optional {?subj schema:distribution/schema:url|schema:subjectOf/schema:url ?url .}\n            OPTIONAL {?subj schema:datePublished|sschema:datePublished ?date_p .}\n            OPTIONAL {?subj schema:publisher/schema:name|sschema:publisher/sschema:name|sschema:sdPublisher|schema:provider/schema:name ?pub_name .}\n            OPTIONAL {?subj schema:spatialCoverage/schema:name|sschema:spatialCoverage/sschema:name ?place_name .}\n            OPTIONAL {?subj schema:keywords|sschema:keywords ?kwu .}\n            BIND ( IF ( BOUND(?date_p), ?date_p, \"No datePublished\") as ?datep ) .\n            BIND ( IF ( BOUND(?pub_name), ?pub_name, \"No Publisher\") as ?pubname ) .\n            BIND ( IF ( BOUND(?place_name), ?place_name, \"No spatialCoverage\") as ?placename ) .\n            ?subj schema:version|sschema:version ?version .\n        }\n        GROUP BY ?subj ?pubname ?placenames ?kw ?datep ?disurl ?score ?name ?description  ?resourceType ?g  ?citation ?version\n        ORDER BY DESC(?score)\n ```\n\n## Resource Registry\n\n(lot of blank nodes)\n### count\n```sparql\nSELECT (count(?g ) as ?count)\nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/CreativeWork&gt;}}\n</code></pre></p>"},{"location":"developers/services-infrastructure/production/sparql/#triples_2","title":"Triples","text":"<pre><code>SELECT * \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/CreativeWork&gt;}}\nLIMIT 100</code></pre>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/","title":"Setting up Geocodes Services on AWS","text":"Note <p>we decided against using both s3 and Neptune. each neptune instnace is one graph namespace, so at $95/month for an instance, that is a bit high. s3 works, but for testing we take buckets up and down, easier to manage in minio that AWS.</p> <p>To do this, we are going to utilize four pieces: * s3, replaces Minio Container * neptune, replaces Graph Container * single virutal machine setup as a docker storm. * Portainer setup on in Our Openstack.</p> <p>Major differences in the docker: * uses config to store traefik config * use a volume to store the trafik * using secrets</p>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#notes-on-setting-up-s3-and-neptune","title":"Notes on Setting up s3 and Neptune","text":""},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#docker-stack","title":"Docker stack","text":"<p>Install docker stack setup as the base machine install where  <code>docker stack init --address--</code></p>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#from-console","title":"From Console","text":"<ol> <li>setup netowrk</li> <li>setup volumes</li> <li>setup config</li> <li>setup secrets</li> <li></li> </ol> <p><code>volume create traefik_data</code> <code>volume create logs</code></p> <p><code>docker network create -d overlay --attachable traefik_proxy</code></p> <p><code>docker config create   traefik_yml ./traefik-aws/traefik.yml</code></p> <p>docker secret create </p> <p>S3SSL=true S3PORT=443</p> <p>MINIO_ROOT_ACCESS_KEY=worldsbestaccesskey MINIO_ROOT_SECRET_KEY=worldsbestsecretkey</p>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#minio_service_access_keyworldsbestaccesskey","title":"MINIO_SERVICE_ACCESS_KEY=worldsbestaccesskey","text":""},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#minio_service_secret_keyworldsbestsecretkey","title":"MINIO_SERVICE_SECRET_KEY=worldsbestsecretkey","text":"<p>GC_GITHUB_CLIENTID_OR_USER_GITHUB_TOKEN=OAUTH SECRET GC_GITHUB_SECRET_OR_USER=OAUTH APP ID</p>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#setup-stack-in-portainer","title":"setup stack in portainer","text":"<ol> <li>edit the portainer_aws.env</li> <li> <p>copy over the base-aws-compose.yaml</p> </li> <li> <p>or use the git version <pre><code>Name: base\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nReference: refs/heads/main\nCompose path: dbase-aws-compose.yaml</code></pre></p> </li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setting_up_aws/#notes","title":"Notes:","text":"<p>https://enginaltay.medium.com/how-to-use-traefik-as-ingress-router-on-aws-fc559f87f4d8</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_aws_neptune/","title":"Setup aws neptune","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_aws_neptune/#setting-up-aws-services","title":"setting up AWS services","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_aws_neptune/#setup-up-aws-neptune","title":"setup up AWS-Neptune","text":"<p>Start with cloudbank login</p> <p>cloudbank.org/billing-account-access</p> <p>click on login</p> <p>go to upper left services, and star/favorite  Neptune and s3, iam, ec2</p> <p>click on Neptune to provision</p> <p>you can take all the defaults</p> <p>including making a decoder notebook to look at it</p> <p>more to come as we sort out access</p> <p>For now getting crawl from radiant to s3, for later Neptune load</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/","title":"Setup Machine:","text":"<p>This is step 1 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#base-machine-to-run-docker-containers-treafik-and-portainer","title":"Base Machine to run Docker Containers Treafik and Portainer:","text":"<p>This is what will be needed to create a production server</p> <ul> <li>base virtual machine for containers</li> <li>ability to request DNS,</li> </ul> <p>SUMMARY</p> <p>These are a summary of the steps, The Step Details are below.</p> <p>DOCKER REQUIREMENT</p> <p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run </p> <pre><code>docker compose version\nDocker Compose version v2.13.0</code></pre> <p>If you cannot run <code>docker compose</code> then update to the docker.com version This is the version we are presently running.</p> <pre><code>Client: Docker Engine - Community\n     Version:           20.10.21\n     API version:       1.41</code></pre> <p>DOCKER SWARM</p> <p>Docker swarm needs to be init'd with the public ip address.</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#step-overview","title":"Step Overview:","text":"<ul> <li>create a machine in openstack (if production)<ul> <li>select size</li> <li>associate floating IP<ul> <li>ask for DNS for that ip to be configured with needed names</li> </ul> </li> </ul> </li> <li> <p>ssh to machine. You do not need to have the DNS's to install the software. But it will be needed.</p> <ul> <li>update apt<ul> <li><code>sudo apt update</code></li> </ul> </li> <li> <p>update base software</p> <ul> <li><code>sudo apt upgrade</code></li> </ul> </li> <li> <p>install docker</p> </li> </ul> </li> </ul> Use Official Docker for Ubuntu <ul> <li> <p>use these docker install instructions</p> </li> <li> <p>add ubuntu (or other users) to docker group</p> <ul> <li><code>sudo groupadd docker</code></li> <li><code>sudo usermod -aG docker ubuntu</code></li> </ul> </li> <li>reboot</li> <li><code>sudo reboot now</code></li> </ul> <ul> <li>create a directory for geocodes, set up permissions and groups<ul> <li><code>sudo mkdir /data/decoder</code></li> <li><code>ln -s /data/decoder/ decoder</code></li> <li><code>ln -s /data/decoder/ geocodes</code></li> <li><code>sudo addgroup geocodes</code></li> <li><code>usermod -a -G geocodes {user}</code></li> <li><code>sudo chgrp geocodes /data/decoder</code></li> <li><code>sudo chmod g+rwx /data/decoder</code></li> </ul> </li> <li>init docker swarm<ul> <li> <p>DOCKER SWARM</p>   Docker swarm needs to be init'd with the public ip address. </li> <li><code>nslookup {HOSTNAME}</code></li> <li><code>sudo docker swarm init --advertise-addr {PUBLIC_IP}</code></li> <li>save the token to a file (I use NOTES)</li> </ul> </li> <li>verify proper base configuration<ul> <li><code>docker compose --help</code> shows a -p flag</li> </ul> </li> <li>SNAPSHOT and creaate an image<ul> <li></li> </ul> </li> <li>clone geocodes<ul> <li><code>cd decoder</code> or <code>cd /data/decoder</code></li> <li><code>git clone https://github.com/earthcube/geocodes.git</code></li> </ul> </li> <li>configure a base server</li> <li>base-machine-compose.yaml is the full stack with a portainer, treafik</li> <li>base-swarm-compose.yaml is just a treakfit. connect with your existing portainer.</li> <li>take a break and wait for the DNS entries.<ul> <li>if you cannot wait for the DNS, you can go to the no cert port <ul> <li>https://{HOST}}:9443/</li> <li>use chrome, click advanced, and go to the port.</li> </ul> </li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#step-details","title":"Step Details:","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#create-a-machine-in-openstack","title":"create a machine in openstack","text":"<p>Suggested size:</p> <p>SDSC Openstack:</p> <ul> <li>ubuntu 22</li> <li>100 gig<ul> <li>m1.2xlarge (8 CPU, 32 gig)</li> <li>network: earthcube</li> </ul> </li> <li>Security groups:<ul> <li>remote ssh (22)</li> <li>geocodes (http/https; 80:443)</li> <li>portainer (temporary need: 9443)</li> <li>minio (optional: 9000/9001)</li> </ul> </li> <li>Keypair: earthcube (or any)</li> </ul> <p>Ports Pre-DNS</p> <p>minio ports do not need to be open, we are proxying on 80 and 443 Portainer port (9443)  can be opended temporarily if you want to play a bit pre-DNS.</p> <p>Associate a Public IP</p> <p>After the machine is created, we can change the IP to the one associated with geocodes.earthcube.org</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#setup-domain-names","title":"setup domain names","text":"<ul> <li>Machines</li> <li>Name for remote DNS</li> </ul> <p>ESSENTIAL for PRODUCTION</p> <p>It is ESSENTIAL for PRODUCTION that the names are defined in a DNS. This allows for https for all services and some services (aka s3/minio) do not play well with a proxy.</p> <p>You might be able to run production stack using localhost, with these DNS... but that mucks with the lets encrypt HTTPS certs... if you control your own DNS, these are the  entries needed.  Name for local DNS</p> <p>Local testing and development can be using  the local compose configuration. This use http, and  local ports for services that cannot be proxied</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#ssh-to-machine-and-verify","title":"ssh to machine and verify","text":"<p><code>ssh -i ~/.ssh/earthcube.pem ubuntu@{public IP}</code></p> add your ssh key so you can log in as main user (eg. ubuntu) <p>SSH Keys</p> <p>for production, we recommend that you use a group account/main account</p> <p>to do this you will need to create and copy a public/private key</p> <p>Generate an ssh-key:</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"comment\"</code></pre> <p>copy it to your remote server:</p> <pre><code>ssh-copy-id user@ip</code></pre> <p>or you can manually copy the <pre><code>~/.ssh/id_rsa.pub to ~/.ssh/authorized_keys.</code></pre></p> <p>Edit</p> <p>It can be done through ssh command as mentioned @chepner:</p> <p><pre><code>ssh user@ip 'mkdir ~/.ssh'\nssh user@ip 'cat &gt;&gt; ~/.ssh/authorized_keys' &lt; ~/.ssh/id_rsa.pub</code></pre> (Above based on: stackexchange)</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#configure-a-base-server","title":"configure a base server","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#update-os","title":"update OS","text":"<ul> <li>update apt<ul> <li><code>sudo apt update</code></li> </ul> </li> <li>update base software<ul> <li>`sudo apt upgrade</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#add-docker-git","title":"add docker, git","text":"Offical Docker for Ubuntu <p>use these docker install instruction</p> <ul> <li>add ubuntu (or other users) to docker group</li> <li><code>sudo groupadd docker</code></li> <li><code>sudo usermod -aG docker ubuntu</code></li> <li>reboot</li> <li><code>sudo reboot now</code></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#create-a-directory-for-geocodes-set-up-permissions-and-groups","title":"create a directory for geocodes, set up permissions and groups","text":"<pre><code>* `sudo mkdir /data/decoder`\n* `ln -s /data/decoder/ decoder`\n* `sudo addgroup geocodes`\n* `usermod -a -G geocodes {user}`\n* `sudo chgrp geocodes /data/decoder`\n* `sudo chmod g+rwx /data/decoder`</code></pre>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#clone-geocodes-stack","title":"clone geocodes stack","text":"<ul> <li><code>cd decoder</code> or <code>cd /data/decoder</code></li> <li><code>git clone https://github.com/earthcube/geocodes.git</code></li> <li><code>cd geocodes/deployment</code></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#copy-base_machineexampleenv-to-env","title":"copy  base_machine.example.env, to .env","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#option-1-production-server-use-env","title":"Option 1. production server use .env","text":"<ul> <li><code>cp base_machine.example.env .env</code></li> <li>modify the file<ul> <li>note: you can also copy the full portainer.env. </li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#option-2-testing-playing-developer","title":"Option 2. testing, playing, developer","text":"<ul> <li><code>cp base_machine.example.env {myproject}.env</code></li> <li>modify the file<ul> <li>note: you can also copy the full portainer.env.</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#modify-the-treafik-datatraefikyml","title":"modify the treafik-data/traefik.yml","text":"treafik-data/traefik.yml <p><pre><code>acme:\n# using staging for testing/development\n#     caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: example@earthcube.org\n    storage: acme.json\n    httpChallenge:\n        entryPoint: http</code></pre> If production, comment the line as shown. Developers see Lets Encypt Notes </p> Let Encrypt Notes <p>lets encrypt, </p> <p>(developers) set to use staging environment server while testing If you are doing development, then leave the caServer uncommented.</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#start-the-base-containers","title":"start the base containers","text":"<ul> <li>new machine or developer</li> <li> <p><code>./run_base.sh -e  {myproject}.env</code></p> </li> <li> <p>production: this uses the default .env (cp  portainer.env .env)</p> </li> </ul> <code>./run_base.sh</code> <pre><code>      ubuntu@geocodes-dev:~/geocodes/deployment$ ./run_base.sh -e geocodes-1.env\n      Error response from daemon: network with name traefik_proxy already exists\n      NETWORK ID     NAME              DRIVER    SCOPE\n      ad6cbce4ec60   bridge            bridge    local\n      2f618fa7da6d   docker_gwbridge   bridge    local\n      f8048bc7a3d9   host              host      local\n      kibdi510bt0x   ingress           overlay   swarm\n      12c01a2186b0   none              null      local\n      u4d4oxfy7olc   traefik_proxy     overlay   swarm\n      Verify that the traefik_proxy network SCOPE is swarm\n      traefik_data\n      portainer_data\n      true\n      [+] Running 2/2\n      \u283f Container portainer  Started                                           13.7s\n      \u283f Container traefik    Started</code></pre>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#testing-setup","title":"Testing Setup","text":"<code>Are containers running</code> <p><code>docker ps</code> <pre><code>    * ubuntu@geocodes-dev:~/geocodes/deployment$ docker ps\n      CONTAINER ID   IMAGE                           COMMAND                  CREATED         STATUS         PORTS                                                                      NAMES\n      09a5d8683cce   traefik:v2.4                    \"/entrypoint.sh trae\u2026\"   2 minutes ago   Up 2 minutes   0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp, :::443-&gt;443/tcp   traefik\n      d3e2333ade6f   portainer/portainer-ce:latest   \"/portainer\"             2 minutes ago   Up 2 minutes   8000/tcp, 9000/tcp, 9443/tcp                                               portainer</code></pre></p> Is network setup correctly? <p><code>docker network ls</code> <pre><code>docker network ls\n      NETWORK ID     NAME              DRIVER    SCOPE\n      ad6cbce4ec60   bridge            bridge    local\n      2f618fa7da6d   docker_gwbridge   bridge    local\n      f8048bc7a3d9   host              host      local\n      kibdi510bt0x   ingress           overlay   swarm\n      12c01a2186b0   none              null      local\n      u4d4oxfy7olc   traefik_proxy     overlay   swarm</code></pre></p> Note <p>NAME:traefik_proxy needs to exist, and be DRIVER:overlay, SCOPE:swarm</p> Are volumes available <p><code>docker volumes</code> <pre><code>ubuntu@geocodes-dev:~$ docker volume ls\n      DRIVER    VOLUME NAME\n      local     graph\n      local     minio\n      local     portainer_data\n      local     traefik_data</code></pre></p> <p>are Traefik and Portainer available via the web?</p> <ul> <li>Treafik https://admin.{host}<ul> <li>login is admin:iforget</li> </ul> </li> </ul> image <p></p> <ul> <li>Portainer https://portainer.{host}/<ul> <li>this will ask you to setup and admin password</li> </ul> </li> </ul> image <p></p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#go-to-step-2","title":"Go to step 2.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#how-totroubleshooting","title":"How to/Troubleshooting","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#updating-portainer-or-treafik","title":"updating Portainer, or treafik","text":"<p>the latest image needs to be pulled</p> <p><code>docker pull portainer/portainer-ce:latest</code></p> <p>then <code>./run_base.sh</code></p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_base_machine_configuration/#how-tos-needed","title":"How tos needed:","text":"<ul> <li>LOCAL DNS SETUP</li> <li>editing your local machine /etc/hosts file does not work with letsencrypt. </li> <li>If user has a local name server they control, that might work.</li> <li>setup a new password for traefik</li> <li>lets encrypt</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/","title":"intial notes on setting up a docker swarm on aws","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/#create-virutal-machine","title":"create virutal machine","text":"<ul> <li>Kevin did this ** user earthcube: sent key</li> <li></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/#login-install-docker","title":"login, install docker","text":"<p>earthcube@ip-172-31-9-0:~$ docker <pre><code>Command 'docker' not found, but can be installed with:\nsnap install docker         # version 20.10.17, or\napt  install docker.io      # version 20.10.21-0ubuntu1~22.04.2\napt  install podman-docker  # version 3.4.4+ds1-1ubuntu1`\nSee 'snap info docker' for additional versions.</code></pre></p> <p>This is the wrong command</p> <p>If you are running on Ubuntu, you need to remove the provided docker.com version. Official docker package We suggest that for others, confirm that you can run</p> <p>Follow the official docker as noted in the docs and the linux post isntall</p> <pre><code>sudo groupadd docker\nsudo usermod -aG docker earthcube\n</code></pre> <p>Log in and out</p> <pre><code>docker info </code></pre> <p>sudo systemctl enable docker.service sudo systemctl enable containerd.service</p> <p>docker swarm init --advertise-addr 54.244.44.10</p> <p>from the portainter.geocodes-dev.earthcube.org/ add environment Docker Swarm</p> <p>USE THE INTERNAL AMAZON IP ADDRESS, else you will need to open port 9001... </p> <p>docker network create \\ --driver overlay \\ portainer_agent_network</p> <p>docker service create \\ --name portainer_agent \\ --network portainer_agent_network \\ -p 9001:9001/tcp \\ --mode global \\ --constraint 'node.platform.os == linux' \\ --mount type=bind,src=//var/run/docker.sock,dst=/var/run/docker.sock \\ --mount type=bind,src=//var/lib/docker/volumes,dst=/var/lib/docker/volumes \\ portainer/agent:2.18.4</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/#configure-portainer","title":"configure portainer","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/#test","title":"Test...","text":"<p>do you have a the DNS setup properly. try</p> <p><code>nslookup admin.{HOST}</code></p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_docker_swarm/#add-in-portainer-interface","title":"Add in portainer interface","text":"<ul> <li>network :  </li> <li>traefik_proxy (overlay)</li> <li>headless_gleanerio(overlay)</li> <li>volumes: log,   traefik_data</li> <li>configuration: </li> <li>gleaner</li> <li>nabu</li> <li>treafik.yml    please change the email address </li> </ul> <pre><code>api:\n  dashboard: true\n\nentryPoints:\n  http:\n    address: \":80\"\n  https:\n    address: \":443\"\n\nproviders:\n  docker:\n    endpoint: \"unix:///var/run/docker.sock\"\n    exposedByDefault: false\n\ncertificatesResolvers:\n  httpresolver:\n    acme:\n      # using staging for testing/development\n      #caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n  httpresolver_staging:\n    acme:\n      # using staging for testing/development\n      caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n  httpresolver_production:\n    acme:\n      # using staging for testing/development\n      #caServer: https://acme-staging-v02.api.letsencrypt.org/directory\n      email: dwvalentine@ucsd.edu\n      storage: /etc/traefik/acme.json\n      httpChallenge:\n        entryPoint: http\n</code></pre> <p>Warn</p> <p>this needs to deploy a headleass, and the configs for gleaner and nabu, .</p> <ul> <li> <p>stack: headless</p> <ul> <li>https://github.com/earthcube/geocodes.git</li> <li>deployment/gleaner-compose.yaml</li> <li>env variables <pre><code>     HOST=</code></pre></li> </ul> </li> <li> <p>Then configure dagit in portainer hosting dagster. get the porteiner endpoint correct <pre><code>PORTAINER_URL=https://portainer.geocodes-aws-dev.earthcube.org:443/api/endpoints/9/docker/</code></pre></p> </li> </ul> <p>will need notes on securing it to a user, with that users API key</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/","title":"Setup Geocodes Services  Containers:","text":"<p>This is step 2 of 4 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#services-stack","title":"Services Stack","text":"<p>The services stack includes the graph, storage (s3) and sparql gui containers. JSON-LD files are 'summoned' by gleaner to the s3 storage, and nabu convert jsonld to rdf quads and pushes the results to the graph.  After uploading, a step to produce a materialized view is required to improve performance The summarize step is undocoumented at present.</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#create-a-new-env-file","title":"create a new env file","text":"<ul> <li>cd deployment</li> <li>Edit files containing env variables</li> <li>copy portainer.services.env to new file.<code>cp portainer.env {myhost}.services.env</code></li> <li>copy portainer.geocodes.env to new file.<code>cp portainer.geocodes.env {myhost}.geocodes.env</code></li> <li>(option) use single file copy portainer.env to new file.<code>cp portainer.env {myhost}.env</code></li> <li>edit {myhost}.{geocodes|services}.env<ul> <li>change</li> </ul> </li> </ul> env <pre><code>HOST=geocodes-dev.mydomain.org\nPRODUCTION=geocodes.mydomain.org\nGC_CLIENT_DOMAIN=geoccodes.geocodes-dev.mydomain.org\nS3ADDRESS=oss.geocodes-dev.mydomain.org</code></pre>"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#setup-and-start-services-using-portainer-ui","title":"Setup and start services using portainer ui","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#create-services-stack","title":"Create Services Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>Select stack tab</li> <li>click add stack button <pre><code>Name: services\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nReference: refs/heads/main\nCompose path: deployment/services-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.services.env</li> </ul> </li> <li>Actions: <ul> <li>Click: Deploy This Stack </li> </ul> </li> </ul> </li> </ul> Services Stack"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#go-to-step-3","title":"Go to step 3.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> <li></li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setup_geocodes_services_containers/#testing-services-stack","title":"Testing Services Stack","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_gleaner_container/","title":"Setup Gleaner   Containers:","text":"<p>This is step 3 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/setting_up_services/setup_gleaner_container/#gleaner-container-stack","title":"Gleaner Container Stack","text":"<p>The gleaner-compose.yaml contains a single headless chrome container. Headless chrome needs to be run with extra shared memory    <code>` shm_size: \"2gb\"</code>, which was not fully supported by portainer prior to v 2.16. A script, run_gleaner.sh, that is run outside of portainer allows for this setting to be configured.</p> <p>Future</p> <p>in the future, this stack will most likely be a stack with a workflow system based on dagster</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_gleaner_container/#gleaner-ingest-containers","title":"Gleaner Ingest Containers","text":""},{"location":"developers/services-infrastructure/setting_up_services/setup_gleaner_container/#create-gleaner-via","title":"Create Gleaner (via)","text":"<code>./run_gleaner.sh</code> <p>this will run a headless chrome container  for gleaner summoning.  It will only be available locally via http://localhost:9222/</p>"},{"location":"developers/services-infrastructure/setting_up_services/setup_gleaner_container/#go-to-step-4","title":"Go to step 4.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup </li> </ol>"},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/","title":"Setup Geocodes Services Containers for Test Data load GCTEST:","text":"<p>This is step 5 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/#step-overview-setup-and-start-geocodes-client-using-portainer-ui","title":"Step Overview: Setup and start GeoCodes Client using portainer ui:","text":"<ul> <li>modify the configuration file</li> <li>create stack in portainer</li> <li>test</li> <li>instructions for Updating a GEOCODES CLIENT Configuration if things do not work<ul> <li>or delete stack and reload</li> </ul> </li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/#step-details","title":"Step Details:","text":""},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/#modify-the-facet-search-configuration","title":"Modify the Facet Search Configuration","text":"<ul> <li>edit in deployment/facets/config.yaml</li> <li>this file is mounted on the container as a docker config file<ul> <li>run the run_add_configs.sh</li> </ul> </li> </ul> <p>Portions of deployment/facets/config.yaml that might be changed.</p> section of deployment/facets/config.yaml <pre><code>API_URL: https://geocodes.{your host}/ec/api/\nSPARQL_NB: https:/geocodes.{your host}/notebook/mkQ?q=${q}\nSPARQL_YASGUI: https://geocodes.{your host}/sparqlgui?\n#API_URL: \"${window_location_origin}/ec/api\"\n#TRIPLESTORE_URL: https://graph.geocodes-1.earthcube.org/blazegraph/namespace/gctest/sparql\nTRIPLESTORE_URL: https://graph.{your host}/blazegraph/namespace/gctest/sparql\nBLAZEGRAPH_TIMEOUT: 20\n## ECRR need to use fuseki source, for now.\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query \n# ECRR_TRIPLESTORE_URL:   http://{your host}/blazegraph/namespace/ecrr/sparql \nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n# JSONLD_PROXY needs qoutes... since it has a $\nJSONLD_PROXY: \"https://geocodes.{your host}/ec/api/${o}\" \nSPARQL_YASGUI: https://sparqlui.{your host}/?</code></pre>"},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/#create-geocodes-stack","title":"Create Geocodes Stack","text":"<ul> <li>log into portainer<ul> <li>if this is a first login, it will ask you for a password.</li> <li>click add stack button <pre><code>Name: geocodes\nBuild method: git repository\nRepository URL: https://github.com/earthcube/geocodes\nreference: refs/heads/main\nCompose path: deployment/geocodes-compose.yaml</code></pre></li> <li>Environment variables: click 'load variables from .env file'<ul> <li>load {myhost}.geocodes.env</li> </ul> </li> <li>Actions:<ul> <li>Click: Deploy This Stack</li> </ul> </li> </ul> </li> </ul> Geocodes Stack"},{"location":"developers/services-infrastructure/setting_up_user_interface/setup_geocodes_ui_containers/#test-geocodes-client","title":"Test Geocodes Client","text":"<p>Issues</p> <p>IF things are not working in the UI, it is probably the facet search configuration You can take down the geocodes stack, and delete the config/facets_search or you can possibly just stop the gecodes_vue_ui service, and edit the facets_search config as noted here: See Managing Geocodes UI Containers</p> <ol> <li>Got to https://geocodes.{your host}/</li> <li>Got to configuration: https://geocodes.{your host}/#/config</li> <li>Two sections, one is the facests/config.yaml and the second is the API configuration (sanitized, we hope)</li> </ol> <p>Done</p> <p>This is the end of the deployment steps.</p> <p>If the deployment is working, you can now</p> <ul> <li>(Setup a Production Configuration)[./production/creatingProductionConfigs.md]</li> <li>(Reconfigure Geocodes UI)[./production/reconfigure_geocodes_ui_containers.md]</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/","title":"Setting up A Tenant","text":"<p>A 'tenant' will allow us to host a project with a separate UI, using common services</p> <p>Assumptions</p> <p>this assumes that the user is familiar with how the stack works</p> <ul> <li>how to get a DNS name for the tenant</li> <li>how to create datasource for minio and graph,</li> <li>how to create a gleaner config file using glcon</li> <li>how to  load data using glcon</li> <li>how to create a facet_search.yaml  and load to docker/portainer config at config/facet_search+{project}</li> <li>how to add a stack \"geocodes_{project} in  portainer/docker</li> </ul> <ul> <li>a namepace for the graph and oss in the geocodes.services</li> <li>a client at thier DNS, eg. geoocdes.project.org</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#outline-of-the-steps","title":"Outline of the Steps:","text":"<ul> <li>Setup</li> <li>Load Data</li> <li>Summarize</li> <li>Create Geocodes Client UI Stack</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#setup","title":"Setup","text":"<ul> <li>Choose a 'project' identifier, This will be an ENV variable ${GC_BASE} set in local environment, or portainer</li> <li>setup two namespaces in graph (see table below)</li> <li><code>project</code> - a quad store</li> <li><code>project_summary</code> - a triple store, full text index</li> <li>setup bucket in minio</li> <li><code>project</code></li> <li>ask project to setup a DNS name for the client:</li> <li><code>geocodes.project.org CNAME geocodes-dev.earthcube.org</code></li> </ul> Item Name This instance project GC_BASE if you are testing locally you can use an env variable, export GC_BASE={project} DNS DNS HOST geocodes.{dns} project graph graph.XXXX.XXX {project}        (quad with full text) project summary graph.XXXX.XXX {project}_summary   (triples with full text) project bucket oss.xxxx.xxx {project} config {on docker/portainer server xxx} facets_config_{project}"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#load-data","title":"Load data","text":"<p>Load data Steps Overview:</p> <ul> <li>setup source</li> <li>glcon/gleaer/nabu</li> <li>init config</li> <li>edit localconfig.yaml</li> <li>generate config</li> <li>run gleaner</li> <li>run nabu</li> <li>run summary</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#create-a-config-for-the-project-run-and-load-data-to-the-namespace-and-graph","title":"create a config for the project, run and load data to the namespace and graph","text":"<ul> <li>data<ul> <li>add tab to the sources spreadsheet, use that location url</li> <li>OR just use a local csv</li> </ul> </li> <li><code>glcon config init --cfgName {project}</code></li> <li>edit localConfig.yaml<ul> <li><code>nano configs/{project}/localConfig.yaml</code></li> </ul> </li> <li><code>glcon config generate --cfgName {project}</code></li> </ul> localConfig.yaml <pre><code>minio:\n  address: oss.geocodes-dev.earthcube.org\n  port: 443\n  accessKey: {snip}\n  secretKey: {snip}\n  ssl: true\n  bucket: {PROJECT} # can be overridden with MINIO_BUCKET\nsparql:\n#  endpoint: http://localhost/blazegraph/namespace/wifire/sparql\n  endpoint: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\ns3:\n  bucket: {PROJECT}  # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1\n\n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location:  https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=wifire\n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\n# TBD -- Just use the sources in the gleaner file.\n#  type: yaml\n#  location: gleaner.yaml</code></pre>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#run-glcon","title":"Run Glcon","text":"<ul> <li><code>glcon gleaner batch --cfgName {project}</code></li> <li><code>glcon nabu prefix --cfgName {project}</code></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#run-summarize","title":"Run Summarize","text":"<p>summarize materializes a flattend the graph  *  * Note this is new, undetested... take a look at the source if something breaks</p> <ul> <li>install earthcube summarize</li> <li><code>pip3 install earthcube_summarize</code></li> <li>run summarize (if installed via package, there should be a command line)</li> <li><code>summarize_from_graph--repo {repo} --graphendpoint {endppiont} --summary_namespace {earthcube_summary}</code></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#configure-client","title":"Configure Client","text":"<p>If you may want to initially test with a local instance in an IDE. After that this is the possible instructions for creating a tennant.</p> FACETS_CONFIG_CONFIG <p>This variable now tells the client which configuraiton to utilize</p>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#add-a-config-in-portainer-facets_config_project","title":"add a config in portainer (facets_config_{project})","text":"<pre><code>* using namespaces, minio and dns from above</code></pre> config/$FACETS_CONFIG_CONFIG <pre><code>---\n#API_URL: http://localhost:3000\nAPI_URL: https://geocodes.{HOST}/ec/api\nTRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\nSUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}_summary/sparql\nECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\nECRR_GRAPH: http://earthcube.org/gleaner-summoned\nTHROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\nSPARQL_QUERY: queries/sparql_query.txt\nSPARQL_HASTOOLS: queries/sparql_hastools.txt\nSPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\nSPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\nJSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n# oauth issues. need to add another auth app for additional 'proxies'\n# This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\nSPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n####\nSPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#setup-tenant-stack","title":"setup tenant stack","text":"<pre><code>* create a new configutatio with a name\n* add a stack with project name  using  geocodes-compose_named.yaml\n* Before saving,  env var GC_BASE with project name</code></pre> <pre><code>HOST=geocodes-dev.earthcube.org\nFACETS_CONFIG_CONFIG=facets_config_name_you_created\n\nGC_CLIENT_DOMAIN=geocodes.{dns}\nMINIO_ROOT_ACCESS_KEY={snip}\nMINIO_ROOT_SECRET_KEY={snip}\nMINIO_SERVICE_ACCESS_KEY={snip}\nMINIO_SERVICE_SECRET_KEY={snip}\nSPARQL_DEFAULT_SPARQL_ENDPOINT_PATH=/blazegraph/namespace/{PROJECT}/sparql\nS3ADDRESS=oss.geocodes-dev.earthcube.org\nS3KEY={snip}\nS3SECRET={snip}\nS3SSL=true\nS3PORT=443\nBUCKET={PROJECT}\nBUCKETPATH=summoned\nPATHTEMPLATE={{bucketpath}}/{{reponame}}/{{sha}}.jsonld\nTOOLTEMPLATE={{bucketpath}}/{reponame}}/{{ref}}.json\nTOOLBUCKET=ecrr\nTOOLPATH=summoned\nGC_GITHUB_SECRET={snip}\nGC_GITHUB_CLIENTID={snip}\nGC_NB_AUTH_MODE=service\nGC_BASE=wifire</code></pre> <ul> <li>add a config in portainer (facets_config_{project}) (go to Configs on portainer)</li> <li>using namespaces, minio and dns from above</li> <li>example for \"config/facets_config_PROJECT\" <pre><code>    #API_URL: http://localhost:3000\n    API_URL: https://geocodes.{HOST}/ec/api\n    TRIPLESTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}/sparql\n    SUMMARYSTORE_URL: https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/{PROJECT}_summary/sparql\n    ECRR_TRIPLESTORE_URL: http://132.249.238.169:8080/fuseki/ecrr/query\n    ECRR_GRAPH: http://earthcube.org/gleaner-summoned\n    THROUGHPUTDB_URL: https://throughputdb.com/api/ccdrs/annotations\n    SPARQL_QUERY: queries/sparql_query.txt\n    SPARQL_HASTOOLS: queries/sparql_hastools.txt\n    SPARQL_TOOLS_WEBSERVICE: queries/sparql_gettools_webservice.txt\n    SPARQL_TOOLS_DOWNLOAD: queries/sparql_gettools_download.txt\n    JSONLD_PROXY: \"${window.location.origin}/ec/api/${o}\"\n    # oauth issues. need to add another auth app for additional 'proxies'\n    # This is the one that will work: SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n    SPARQL_NB: https://geocodes.earthcube.org/notebook/mkQ?q=${q}\n    ####\n    SPARQL_YASGUI: https://sparqlui.geocodes-dev.earthcube.org/?</code></pre></li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#important-changes","title":"important changes","text":"<ul> <li>host is machine host</li> <li>GC_CLIENT_DOMAIN</li> <li>TRIPLESTORE_URL</li> <li>SUMMARYSTORE_URL</li> <li>jsonLD_proxy</li> </ul>"},{"location":"developers/services-infrastructure/setting_up_user_interface/tenant/#issues","title":"issues:","text":"<p>traefik admin</p>"},{"location":"developers/ingest-orchestration/","title":"Dagster","text":""},{"location":"developers/ingest-orchestration/#about","title":"About","text":"<p>The following is a description of the steps and requirements for building and deploying the docker based workflow implemented in  dagster.</p>"},{"location":"developers/ingest-orchestration/#overview","title":"Overview","text":"<p>The image following provides a broad overview of the elements that  are loaded in to the Docker orchestration environment.  This is a very  basic view and doesn't present any scaling or fail over elements.  </p> <p>The key elements are:</p> <ul> <li>sources to configuration and then the creation of the archive files that are loaded and used to load into the Gleaner and Nabu tools</li> <li>The Dagster set which loads three containers to support workflow operations</li> <li>The Gleaner Architecture images which loads three or more containers to support </li> <li>s3 object storage</li> <li>graph database (triplestore)</li> <li>headless chrome for page rendering to support dynamically inserted JSON-LD</li> <li>any other support packages like text, semantic or spatial indexes</li> <li>The GleanerIO tools which loads two containers (Gleaner and Nabu) that are run  and removed by the Dagster workflow</li> </ul> <p></p>"},{"location":"developers/ingest-orchestration/#template-files","title":"Template files","text":"<p>The template files define the Dagster Ops, Jobs and Schedules.  From these and a GleanerIO config file a set of Python scripts for Dagster are created in the output directory. </p> <p>These only need to be changed or used to regenerate if you wish to alter the  execution graph (ie, the ops, jobs and schedules) or change the config file. In the later case only a regeneration needs to be done.</p> <p>There are then Docker build scripts to build out new containers.  </p> <p>See:  template</p>"},{"location":"developers/ingest-orchestration/#steps-to-build-and-deploy","title":"Steps to build and deploy","text":"<p>1) Move to the implnets directory 1) Place your gleanerconfig.yaml (use that exact name) in confgis/NETWORK/gleanerconfig.yaml    1) Note:  When doing your docker build, you will use this NETWORK name as a value in the command such as    <pre><code>podman build  --tag=\"docker.io/fils/dagster_nsdf:$(VERSION)\"  --build-arg implnet=nsdf --file=./build/Dockerfile </code></pre> 1) Make any needed edits to the templates in directory templates/v1/ or make your own template set in that directory</p> <p>The command to build using the pygen.py program follows.  This is done from the standpoint of running in from the  implenet directory.</p> <pre><code> python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./generatedCode/implnet-nsdf/output  -td ./templates/v1   -d 7</code></pre> <p>1) This will generate the code to build a dagster instance from the combination of the templates and gelanerconfig.yaml. 2) </p>"},{"location":"developers/ingest-orchestration/#archive-files","title":"Archive files","text":"<p>The archive files need to be compress tar files with the Gleaner or Nabu configs and other required files like the schema context. </p> <p>The archive files are defined in:</p> <p><pre><code>        ARCHIVE_FILE = os.environ.get('GLEANERIO_GLEANER_ARCHIVE_OBJECT')\n        ARCHIVE_PATH = os.environ.get('GLEANERIO_GLEANER_ARCHIVE_PATH')\n\n        ARCHIVE_FILE = os.environ.get('GLEANERIO_NABU_ARCHIVE_OBJECT')\n        ARCHIVE_PATH = os.environ.get('GLEANERIO_NABU_ARCHIVE_PATH')</code></pre> The required config file names are expressed in the CMD line:</p> <pre><code>        CMD = [\"--cfg\", \"/gleaner/gleanerconfig.yaml\", \"--source\", source]\n\n        CMD = [\"--cfg\", \"/nabu/nabuconfig.yaml\", \"prefix\", \"summoned/\" + source]</code></pre> <p>So:  gleanerconfig.yaml  and nabuconfig.yaml are the required configuration names.</p> <p>NOTE: At present only yaml is supported, JSON support is a simple addition  once the system is tested and working OK with the yaml files. </p> <p>The contents will look something like </p> <pre><code>\u276f tar -ztf GleanerCfg.tgz\n./gleanerconfig.yaml\n./jsonldcontext.json\n\u276f tar -ztf NabuCfg.tgz\n./nabuconfig.yaml\n./jsonldcontext.json</code></pre> <p>These files need to be in the bucket prefix defined by: </p> <pre><code>GLEANERIO_GLEANER_ARCHIVE_OBJECT=scheduler/configs/GleanerCfg.tgz\nGLEANERIO_NABU_ARCHIVE_OBJECT=scheduler/configs/NabuCfg.tgz</code></pre>"},{"location":"developers/ingest-orchestration/#environment-files","title":"Environment files","text":"<pre><code>export PORTAINER_URL=http://example.org:9000/api/endpoints/2/docker/\nexport PORTAINER_KEY=KEY\n\nexport GLEANERIO_GLEANER_IMAGE=fils/gleaner:v3.0.11-development-df\nexport GLEANERIO_GLEANER_ARCHIVE_OBJECT=scheduler/configs/testGleanerCfg.tgz\nexport GLEANERIO_GLEANER_ARCHIVE_PATH=/gleaner/\n\nexport GLEANERIO_NABU_IMAGE=fils/nabu:2.0.4-developement\nexport GLEANERIO_NABU_ARCHIVE_OBJECT=scheduler/configs/testNabuCfg.tgz\nexport GLEANERIO_NABU_ARCHIVE_PATH=/nabu/\n\nexport GLEANERIO_LOG_PREFIX=scheduler/logs/\n\nexport GLEANER_MINIO_URL=192.168.202.114\nexport GLEANER_MINIO_PORT=49153\nexport GLEANER_MINIO_SSL=False\nexport GLEANER_MINIO_SECRET=SECRET\nexport GLEANER_MINIO_KEY=KEY\nexport GLEANER_MINIO_BUCKET=gleaner.test\n</code></pre>"},{"location":"developers/ingest-orchestration/#implementation-networks","title":"Implementation Networks","text":"<p>This (https://github.com/sharmasagar25/dagster-docker-example)  is an example on how to structure a [Dagster] project in order to organize the jobs, repositories, schedules, and ops. The example also contains examples on unit-tests and a docker-compose deployment file that utilizes a Postgresql database for the run, event_log and schedule storage.</p> <p>This example should in no way be considered suitable for production and is merely my own example of a possible file structure. I personally felt that it was difficult to put the Dagster concepts to use since the projects own examples had widely different structure and was difficult to overview as a beginner.</p> <p>The example is based on the official [tutorial].</p>"},{"location":"developers/ingest-orchestration/#folders","title":"Folders","text":"<ul> <li>build:  build directives for the docker containers</li> <li>configs</li> <li>src</li> <li>tooling</li> </ul>"},{"location":"developers/ingest-orchestration/#requirements","title":"Requirements","text":"<p>At this point it is expected that you have a valid Gleaner config file named gleanerconfig.yaml located in some path within the configs directory.</p>"},{"location":"developers/ingest-orchestration/#building-the-dagster-code-from-templates","title":"Building the dagster code from templates","text":"<p>The python program pygen will read a gleaner configuration file and a set of  template and build the Dagster code from there.  </p> <pre><code>python pygen.py -cf ./configs/nsdf/gleanerconfig.yaml -od ./src/implnet-nsdf/output  -td ./src/implnet-nsdf/templates  -d 7</code></pre>"},{"location":"developers/ingest-orchestration/#running","title":"Running","text":"<p>There is an example on how to run a single pipeline in <code>src/main.py</code>. First install the dependencies in an isolated Python environment.</p> <pre><code>pip install -r requirements</code></pre> <p>The code built above can be run locally, though your templates may be set up  to reference services and other resources not present on your dev machine.  For  complex examples like these, it can be problematic.  </p> <p>If you are looking for some simple examples of Dagster, check out the directory examples for some smaller self-contained workflows.  There are good for testing things like sensors and other approaches. </p> <p>If you wish to still try the generated code cd into the output directory you specified in the pygen command.</p> <p>Then use:</p> <pre><code>dagit -h ghost.lan -w workspace.yaml</code></pre>"},{"location":"developers/ingest-orchestration/#building","title":"Building","text":"<pre><code> podman build  -t  docker.io/fils/dagster:0.0.24  .</code></pre> <pre><code> podman push docker.io/fils/dagster:0.0.24</code></pre>"},{"location":"developers/ingest-orchestration/#appendix","title":"Appendix","text":""},{"location":"developers/ingest-orchestration/#setup","title":"Setup","text":""},{"location":"developers/ingest-orchestration/#docker-api-sequence","title":"Docker API sequence","text":""},{"location":"developers/ingest-orchestration/#appendix_1","title":"Appendix","text":""},{"location":"developers/ingest-orchestration/#portainer-api-setup","title":"Portainer API setup","text":"<p>You will need to setup Portainer to allow for an API call.  To do this look  at the documentation for Accessing the Portainer API</p>"},{"location":"developers/ingest-orchestration/#notes","title":"Notes","text":"<p>Single file testing run</p> <pre><code> dagit -h ghost.lan -f test1.py</code></pre> <ul> <li>Don't forget to set the DAGSTER_HOME dir like in </li> </ul> <pre><code> export DAGSTER_HOME=/home/fils/src/Projects/gleaner.io/scheduler/python/dagster</code></pre> <pre><code>dagster-daemon run</code></pre> <p>Run from directory where workspace.yaml is. <pre><code>dagit --host 192.168.202.159</code></pre></p>"},{"location":"developers/ingest-orchestration/#cron-notes","title":"Cron Notes","text":"<p>A useful on-line tool:  https://crontab.cronhub.io/</p> <pre><code>0 3 * * *   is at 3 AM each day\n\n0 3,5 * * * at 3 and 5 am each day\n\n0 3 * * 0  at 3 am on Sunday\n\n0 3 5 * *  At 03:00 AM, on day 5 of the month\n\n0 3 5,19 * * At 03:00 AM, on day 5 and 19 of the month\n\n0 3 1/4 * * At 03:00 AM, every 4 days</code></pre>"},{"location":"developers/ingest-orchestration/#indexing-approaches","title":"Indexing Approaches","text":"<p>The following approaches</p> <ul> <li>Divide up the sources by sitemap and sitegraph</li> <li>Also divide by production and queue sources</li> </ul> <p>The above will result in at most 4 initial sets.</p> <p>We can then use the docker approach</p> <pre><code>./gleanerDocker.sh -cfg /gleaner/wd/rundir/oih_queue.yaml  --source cioosatlantic</code></pre> <p>to run indexes on specific sources in these configuration files.  </p>"},{"location":"developers/ingest-orchestration/#references","title":"References","text":"<ul> <li>Simple Dagster example</li> </ul>"},{"location":"developers/ingest-orchestration/add_containers/","title":"Containers for Dagster Scheduler","text":""},{"location":"developers/ingest-orchestration/add_containers/#things-to-work-on","title":"Things to work on","text":"<ul> <li>secrets</li> <li>modify compose to use a project variable </li> <li>network modify to use passed evn for network names</li> </ul> <p>Future: * can we use a volume. Use git to pull?</p>"},{"location":"developers/ingest-orchestration/add_containers/#build-docker","title":"Build Docker","text":"<p>Needs to be automated with a workflow</p> <p>something about archive file</p>"},{"location":"developers/ingest-orchestration/add_containers/#add-stack","title":"add Stack.","text":"<p>dagster/deployment/compose.yaml make usre that it is the correct version.   image: docker.io/fils/dagster_eco:0.0.44</p> <p>NOTE: we can use a env ${project} varibale like we do for geocodes_.._named.yaml</p>"},{"location":"developers/ingest-orchestration/alternatives/","title":"Alternative Approaches","text":""},{"location":"developers/ingest-orchestration/alternatives/#abouts","title":"Abouts","text":"<p>These are just some scratch notes on some of the approaches for  job workflows.  This is a more historical document than anything. </p>"},{"location":"developers/ingest-orchestration/alternatives/#options","title":"Options","text":"<ul> <li>Dagster<ul> <li>Dagster</li> <li>Build ETL pipelines</li> <li>Create new project</li> </ul> </li> <li>Red Engine<ul> <li>Red Engine</li> <li>HN Posting on Red Engine</li> <li>Red Engine read the docs</li> <li>Rocketry</li> </ul> </li> <li>Dramtiq<ul> <li>Dramatic</li> </ul> </li> <li>Temporalio<ul> <li>Temporalio</li> </ul> </li> <li>Luigi<ul> <li>Luigi</li> </ul> </li> <li>AppScheduler<ul> <li>AppScheduler</li> </ul> </li> <li>Go Cadance</li> <li>Airflow</li> <li>Background jobs in linux</li> <li>exec-command</li> </ul>"},{"location":"developers/ingest-orchestration/alternatives/#other-dagster-references","title":"Other Dagster References","text":"<ul> <li>pybokeh/dagster-sklearn</li> <li>Gave me the inspiration for the primary folder structure. Although that     example is more advanced and utilizes sklearn.</li> <li>dagster-io/dagster examples</li> <li>Dagster's own examples.</li> <li>xyzy-web/dagster-exchangerates</li> <li>An example that includes Kubernetes Deployment.</li> <li>sephib/dagster-graph-project</li> <li>sspaeti-com/practical-data-engineering</li> </ul>"},{"location":"developers/ingest-orchestration/eco_deploy/","title":"ECO Scheduler Notes","text":"<p>Note</p> <p>these will need to become the gleanerio scheduler documentation.  for now these are rough. Images and graphics need to be loaded</p> <pre><code>flowchart TB\nPostgres_Container-- defined by --&gt; compose_project\nDagit_UI_Container-- defined by --&gt; compose_project\nDagster_Container  -- defined by --&gt; compose_project\nHeadless_Container -- defined by --&gt; compose_project\nconfigs_volume_Container -- defined by --&gt; compose_project\ncompose_project -- deployed to --&gt; docker_portainer\n\nGleaner_container -- image manual add --&gt; docker_portainer\nNabu_container -- image manual add --&gt; docker_portainer\n\nGleaner_container -- deployed by --&gt; Dagster_Container\nNabu_container -- deployed by --&gt; Dagster_Container\n\nGleaner_container--  deployed to --&gt; docker_portainer\nNabu_container--  deployed to --&gt; docker_portainer\n\nDagit_UI_Container -- Created by --&gt; Github_action\nDagster_Cotnainer -- Created by --&gt; Github_action\n\nNabuConfig.tgz -- Archive to --&gt; Nabu_container\nGleanerConfig.tfz -- Archive to --&gt; Gleaner_container\n\nNabuConfig.tgz -- Stored in s3 --&gt; s3\nGleanerConfig.tfz -- Stored in s3 --&gt; s3\n\nconfigs_volume_Container -- populates volume --&gt; dagster-project\ndagster-project -- has --&gt; gleanerConfig.yaml\ndagster-project -- has --&gt; nabuConfig.yaml\n</code></pre>"},{"location":"developers/ingest-orchestration/eco_deploy/#deploy","title":"Deploy","text":""},{"location":"developers/ingest-orchestration/eco_deploy/#deploy-dagster-in-portainer","title":"Deploy Dagster in Portainer","text":"<p>You will need to deploy dagster contiainers to portainer, for a docker swarm</p> <ol> <li>Pull scheduler repo</li> <li>cd dagster/implnets/deployment</li> <li>create a copy of envFile.env and edit env variables</li> <li>pull images for nabu and gleaner    <code>GLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest</code> and     <code>GLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest</code></li> <li>as noted as noted in (Compose, Environment and Docker API Assets), deploy the configuration to s3. </li> <li>create network and volumes needed <code>dagster_setup_docker.sh</code></li> <li>create a stack, from compose_project.yaml, with the env variables you set</li> </ol>"},{"location":"developers/ingest-orchestration/eco_deploy/#deploy-local","title":"Deploy Local","text":"<p>this runs a local compose</p> <ol> <li>cd dagster/implnets</li> <li><code>make eco-clean</code>and <code>make eco-generate</code></li> <li>cd dagster/implnets/deployment </li> <li>run dagster_setup_docker.sh </li> <li>run dagster_localrun.sh</li> </ol>"},{"location":"developers/ingest-orchestration/eco_deploy/#implementation-network-builder","title":"Implementation network builder","text":"<p>The work for building the dagster containers for a given implementation network starts in  the directory <code>scheduler/dagster/implnets</code>.  This has been automated for CI For local development, At this time most of this can be driven by the Makefile</p>"},{"location":"developers/ingest-orchestration/eco_deploy/#automated-ci-build","title":"AUTOMATED CI Build","text":"<p>The containers are now built by a github action, and stored in the nsf earhtcube dockerhub</p> <ol> <li>Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where    NETWORK is your implmentation network like eco, iow, etc. </li> <li>if on a branch that you want to build, be sure the branch is in to github action</li> <li>increment VERSION, and commit. </li> <li>watch actions</li> </ol>"},{"location":"developers/ingest-orchestration/eco_deploy/#local-development","title":"LOCAL DEVELOPMENT","text":"<ol> <li>Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where NETWORK is your implmentation network like eco, iow, etc. </li> <li>Check the VERSION file and make sure it has a value you want in it to be tagged to the containers.</li> <li><code>make eco-clean</code>  will remove any existing generated code from the ./generatedCode/implnet-NETWORK directory</li> <li><code>make eco-generate</code> will build the code new.  Set the -d N in the makefile to a value N that is the number    of days you want the runs to cycle over.  So 30 would mean they run once every 30 days.  If you want some providers    to index at different rates you currently need to go in and edit the associated provider schedules file editing the    line <code>@schedule(cron_schedule=\"0 12 * * 6\", job=implnet_job_amgeo, execution_timezone=\"US/Central\")</code> with a     cron value you want.</li> <li><code>make eco-build</code> builds the Docker images following the build file ./build/Docker file.  Note this uses the     command line argument <code>--build-arg implnet=eco</code> to set the implementation NETWORK so that the correct build code     from generatedCode/NETWORK is copied over</li> </ol>"},{"location":"developers/ingest-orchestration/eco_deploy/#compose-environment-and-docker-api-assets","title":"Compose, Environment and Docker API Assets","text":"<ol> <li>You will need the (or need to make) the portainer access token      from your https://portainer.geocodes-aws-dev.earthcube.org/#!/account</li> <li>You will need a valid Gleaner configuration file named gleanerconfig.yaml and a nabu config named nabuconfig.yaml </li> <li>You will need the schema.org context files places in a directory assets  get each of the http and https versions</li> <li><code>wget https://schema.org/version/latest/schemaorg-current-https.jsonld</code></li> <li><code>wget https://schema.org/version/latest/schemaorg-current-http.jsonld</code></li> <li>Generate the archive files for Gleaner and Nabu.  Note the path to the context files should map with what is in the configuration files</li> <li><code>tar -zcf ./archives/NabuCfg.tgz ./nabuconfig.yaml ./assets</code></li> <li><code>tar -zcf ./archives/GleanerCfg.tgz ./gleanerconfig.yaml ./assets</code></li> <li>The archives .tgz files named NabuCfg.tgz and GleanerCfg.tgz need to be copied to the schedule prefix    in your bucket used for Gleaner</li> <li><code>mc cp GleanerCfg.tgz NabuCfg.tgz  gleaner/scheduler/configs</code></li> <li>Make sure GLEANERIO_NABU_ARCHIVE_OBJECT and GLEANERIO_GLEANER_ARCHIVE_OBJECT reflect this location in the .env file</li> <li>Next you will need to build the scheduler containers for your implementation network. Push these containers    to your container registry of choice and make sure the values are set in the .env file and that    the containers are available to Portainer or will get pulled on use.   These are the image files in the     compose file and also the images notes in the environment variables GLEANERIO_GLEANER_IMAGE and GLEANERIO_NABU_IMAGE    in the .env file.</li> </ol> <p>At this point you are ready to move to your Docker or Portainer environment and deploy the  compose and environment files.  </p>"},{"location":"developers/ingest-orchestration/eco_deploy/#notes","title":"Notes","text":"<ol> <li>I do not have the API call to ensure/check/pull and image used by the API, so these images need to be     pulled down manually at this time.  These are the images noted by the .env files at     <code>GLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest</code> and     <code>GLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest</code></li> </ol>"},{"location":"developers/ingest-orchestration/quick/","title":"Notes","text":""},{"location":"developers/ingest-orchestration/quick/#implementation-network-builder","title":"Implementation network builder","text":"<p>The work for building the dagster containers for a given implementation network starts in  the directory <code>scheduler/dagster/implnets</code>.  At this time most of this can be driven by the Makefile.</p> <p>1) Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where    NETWORK is your implmentation network like eco, iow, etc. 2) Check the VERSION file and make sure it has a value you want in it to be tagged to the containers. 3) <code>make eco-clean</code>  will remove any existing generated code from the ./generatedCode/implnet-NETWORK directory 4) <code>make eco-generate</code> will build the code new.  Set the -d N in the makefile to a value N that is the number    of days you want the runs to cycle over.  So 30 would mean they run once every 30 days.  If you want some providers    to index at different rates you currently need to go in and edit the associated provider schedules file editing the    line <code>@schedule(cron_schedule=\"0 12 * * 6\", job=implnet_job_amgeo, execution_timezone=\"US/Central\")</code> with a     cron value you want. 5) <code>make eco-build</code> builds the Docker images following the build file ./build/Docker file.  Note this uses the     command line argument <code>--build-arg implnet=eco</code> to set the implementation NETWORK so that the correct build code     from generatedCode/NETWORK is copied over 6) <code>make eco-push</code> push to your container registry of choice, here docker.io</p>"},{"location":"developers/ingest-orchestration/quick/#compose-environment-and-docker-api-assets","title":"Compose, Environment and Docker API Assets","text":"<p>1) You will need the (or need to make) the portainer access token      from your https://portainer.geocodes-aws-dev.earthcube.org/#!/account 2) You will need a valid Gleaner configuration file named gleanerconfig.yaml and a nabu config named nabuconfig.yaml 3) You will need the schema.org context files places in a directory assets  get each of the http and https versions    1) <code>wget https://schema.org/version/latest/schemaorg-current-https.jsonld</code>    2) <code>wget https://schema.org/version/latest/schemaorg-current-http.jsonld</code> 4) Generate the archive files for Gleaner and Nabu.  Note the path to the context     files should map with what is in the configuration files    1) <code>tar -zcf ./archives/NabuCfg.tgz ./nabuconfig.yaml ./assets</code>    2) <code>tar -zcf ./archives/GleanerCfg.tgz ./gleanerconfig.yaml ./assets</code> 5) The archives .tgz files named NabuCfg.tgz and GleanerCfg.tgz need to be copied to the schedule prefix    in your bucket used for Gleaner    1) <code>mc cp GleanerCfg.tgz NabuCfg.tgz  gleaner/scheduler/configs</code>    2) Make sure GLEANERIO_NABU_ARCHIVE_OBJECT and GLEANERIO_GLEANER_ARCHIVE_OBJECT reflect this location in the .env file 6) Next you will need to build the scheduler containers for your implementation network. Push these containers    to your container registry of choice and make sure the values are set in the .env file and that    the containers are available to Portainer or will get pulled on use.   These are the image files in the     compose file and also the images notes in the environment variables GLEANERIO_GLEANER_IMAGE and GLEANERIO_NABU_IMAGE    in the .env file.</p> <p>At this point you are ready to move to your Docker or Portainer environment and deploy the  compose and environment files.  </p>"},{"location":"developers/ingest-orchestration/quick/#notes_1","title":"Notes","text":"<p>1) I do not have the API call to ensure/check/pull and image used by the API, so these images need to be     pulled down manually at this time.  These are the images noted by the .env files at     <code>GLEANERIO_GLEANER_IMAGE=fils/gleaner:v3.0.11-development-df</code> and     <code>GLEANERIO_NABU_IMAGE=fils/nabu:2.0.8-development</code></p>"},{"location":"developers/ingest-orchestration/refactor/","title":"refactoring","text":""},{"location":"developers/ingest-orchestration/refactor/#rework-code-to-generate-less","title":"rework code to generate less.","text":"<ul> <li>just generate the graph, and some configuration loading</li> <li>pass the ops the configuration</li> </ul>"},{"location":"developers/ingest-orchestration/refactor/#can-we-use-s3-manager-to-store-some-assets","title":"can we use s3 manager to store some assets?","text":"<ul> <li>reports seems like the ideal use case for these.</li> </ul>"},{"location":"developers/ingest-orchestration/refactor/#handle-multiple-workflows","title":"handle multiple workflows","text":"<ul> <li>need to add ability to deploy some other workflows</li> </ul>"},{"location":"developers/ingest-orchestration/refactor/#handle-multiple-organizations","title":"Handle Multiple Organizations","text":"<ul> <li>Deploy to /usr/app/src/orgs/{org}?</li> <li>this will require deploying configurations to dockers that are mounted, i think</li> <li>changed to gleaner/nabu/glcon. Separate server configuration from source information</li> <li>deploy configs that are mounted, in docker compose pass env variables with those paths.</li> <li>might teach gleaner to read secrets from files, then those would be passed.</li> </ul>"},{"location":"developers/ingest-orchestration/refactor/#possible-workflows","title":"possible workflows","text":"<ul> <li> <p>timeseries after final graph</p> <ul> <li>generate a csv of the load reports size of (sitemap, summoned, summon failure, milled, loaded to graph, datasets)</li> </ul> </li> <li> <p>weekly summary</p> <ul> <li>what is the size of the graph this week.</li> </ul> </li> <li>post s3 check, as an approval check. <ul> <li>do these not contain JSONLD</li> <li>store as asset, or maybe have file we publish as 'approved/expected non-summoned</li> </ul> </li> <li>sitemap check<ul> <li>just run a sitemap head to see that url work, and exist, weekly.</li> <li>publish as paritioned data in s3 ;)</li> </ul> </li> <li>shacl... should we shacl releases.<ul> <li>if so, then maybe teach dagster to watch the graph/latest for changes.</li> </ul> </li> </ul>"},{"location":"developers/ingest-orchestration/assets/configs/","title":"Configs","text":""},{"location":"developers/ingest-orchestration/assets/configs/#about","title":"About","text":"<p>This directory holds some of the general configuration files that might be useful for running Gleaner and Nabu.  This section is broken down by the various communities (implementation networks).</p> <p>These are provided as examples.</p>"},{"location":"developers/ingest-orchestration/assets/configs/#tar-archive","title":"TAR archive","text":"<p>The tar archive must be compressed and must be named to align  with the archive ENV variable</p> <pre><code>GLEANERIO_GLEANER_ARCHIVE_OBJECT=scheduler/configs/GleanerCfg.tgz\nGLEANERIO_NABU_ARCHIVE_OBJECT=scheduler/configs/NabuCfg.tgz</code></pre> <pre><code> tar -zcf GleanerCfg.tgz ./gleanerconfig.yaml ./jsonldcontext.json\n ```\n\n```bash\n tar -zcf NabuCfg.tgz ./nabuconfig.yaml ./jsonldcontext.json ./assets</code></pre>"},{"location":"developers/ingest-orchestration/assets/configs/#environment-variables","title":"Environment variables","text":"<p>The command</p> <pre><code>source file.env</code></pre> <p>should set your values.  Be sure not to have spaces in  your vars.</p>"},{"location":"developers/ingest-orchestration/assets/configs/#docker-compose","title":"Docker Compose","text":"<p>Example compose file for use with Dagster is included here. It also is edited to read and express the shell variables  into the containers.</p>"},{"location":"developers/ingest-orchestration/images/appendix/","title":"Appendix","text":""},{"location":"developers/ingest-orchestration/images/appendix/#the-asset-file-flow-in-more-detail","title":"The asset file flow in more detail:","text":"<ul> <li>Creation of template files for the various operations, jobs and schedules</li> <li>Creation of the archive files that hold the configuration for the jobs run </li> <li>Environment file for the values needed by the operations</li> </ul>"},{"location":"indexing/tools/gleaner-docs/","title":"Gleaner Documentation","text":"<p>gleaner.io</p>"},{"location":"indexing/tools/gleaner-docs/#gleaner","title":"Gleaner","text":"<p>Gleaner is a tool for extracting JSON-LD from web pages. You provide Gleaner a list of sites to index and it will access and retrieve pages based on the sitemap.xml of the domain(s). Gleaner can then check for well formed and valid structure in documents and process the JSON-LD data graphs into a form usable to drive a search interface. It is part of the bigger picture. </p>"},{"location":"indexing/tools/gleaner-docs/#open-foundation","title":"Open Foundation","text":"<p>Communities of practice can leverage open schema (schema.org) along with web architecture approaches to build domain search portals. Enhance and extend with community vocabularies to address specific domain needs. This foundation is also leveraged by Google Data Set Search and is complementary to that service. Web architecture as foundation allows a community to provide a more detailed community experiences, while still leveraging the global reach of commercial search indexes. </p>"},{"location":"indexing/tools/gleaner-docs/#big-picture","title":"Big Picture","text":"<p>Gleaner is part of the larger GleanerIO approach. GleanerIO includes approaches for leveraging spatial, semantic, full text or other index approaches. Additionally there is guidance on running Gleaner as part of a routinely updated index of resources and a reference interface for searching the resulting graph. GleanerIO provides a full stack approach to go from indexing to a basic user interface searching a generated Knowledge Graph, an example index. The whole GleanerIO stack can be run on a laptop (it uses Docker Compose files) or deployed to the cloud. Cloud environments used include AWS, Google Cloud, and OpenStack.</p> <p>GleanerIO is also designed to play well with others. As long as packages work well in a web architecture framework, they likely can be integrated into the GleanerIO approach. The GleanerIO approach is modular and even Gleaner itself could be swapped out for other implementations.</p> <p>Indeed, GleanerIO advocates principles over project. GleanerIO is really just a set of principles for which reference implementations (projects) have been developed or external projects have been used. These have evolved and been implemented to address communities like Ocean InfoHub, Internet of Water, GeoCODES and more. The results and approaches of these communities are openly maintained at the GleanerIO GitHub Organization pages. They provide guidance on how yet other communities could leverage this approach to address their functional needs. </p>"},{"location":"indexing/tools/gleaner-docs/#history","title":"History","text":"<p>Communities of practice can leverage open schema (schema.org) along with web architecture approaches to build domain search portals. Enhance and extend with community vocabularies to address specific domain needs. This foundation is also leveraged by Google Data Set Search and is complementary to that service. Web architecture as foundation allows a community to provide a more detailed community experiences, while still leveraging the global reach of commercial search indexes. </p>"},{"location":"indexing/tools/gleaner-docs/cli/","title":"CLI","text":""},{"location":"indexing/tools/gleaner-docs/cli/#quickstart","title":"QuickStart","text":"<p>Before using Gleaner, not that Gleaner does have one prerequisit in the form of an accessible S3 compliant  object store.  This can be AWS S3, Google Cloud Storage or others.  Also, there is the open source  and free Minio object store which is used in many of the examples in GleanerIO. </p> <p>Once you have the object store running and ready you are ready to run Gleaner.  Pull down the release that matches your ssysem from version 3.0.4. Below is an example of pulling this down for a Linux system on ADM64 architecture.  </p> <pre><code>wget https://github.com/gleanerio/gleaner/releases/download/v3.0.4-dev/gleaner-v3.0.4-dev-linux-amd64.tar.gz</code></pre> <p>You will need a configuration file and an example such file can be found in the resources directory.  See also  the config file in the Gleaner Config page.</p> <p>You can set the values in this configuration file.  However, you can leave the Minio value empty and pass then via environment variables.  This sort of approach can work better in some orchestration environments or just  be a safer approach to managing these keys.  </p> <pre><code>export MINIO_ADDRESS=localhost\nexport MINIO_PORT=9000\nexport MINIO_USE_SSL=false\nexport MINIO_ACCESS_KEY=KEYVALUE\nexport MINIO_SECRET_KEY=SECRETVALUE\nexport MINIO_BUCKET=mybucket</code></pre> <p>With those set and your configuration file in palce you can run Gleaner with </p> <pre><code>  ./gleaner -cfg gleanerconfig.yaml --source nameFromCfgFile -rude</code></pre> <pre><code>EarthCube Gleaner\nUsage of /tmp/go-build589932266/b001/exe/main:\n  -cfg string\n        Configuration file (can be YAML, JSON) Do NOT provide the extension in the command line. -cfg file not -cfg file.yml (default \"config\")\n  -log string\n        The log level to output (trace | debug | info | warn | error | fatal) (default \"warn\")\n  -mode string\n        Set the mode (full | diff) to index all or just diffs (default \"full\")\n  -rude\n        Ignore any robots.txt crawl delays or allow / disallow statements\n  -setup\n        Run Gleaner configuration check and exit\n  -source string\n        Override config file source(s) to specify an index target</code></pre>"},{"location":"indexing/tools/gleaner-docs/config/","title":"Config File","text":""},{"location":"indexing/tools/gleaner-docs/config/#source","title":"Source","text":"<p>Sources can be defined as two type. A sitemap, which is a traditional sitemap that  points to resources or a sitemap index that points to a set of sitemaps.</p> <p>The other is a sitegraph, which is a pre-computed graph for a site.  </p> <p>Examples of their formats respectively are:</p>"},{"location":"indexing/tools/gleaner-docs/config/#sitemap","title":"sitemap","text":"<pre><code>  sourcetype: sitemap\n  name: unidata\n  logo: \"\"\n  url: https://www.unidata.ucar.edu/sitemap.xml\n  headless: false\n  pid: https://www.re3data.org/repository/r3d100010355\n  propername: UNIDATA\n  domain: http://www.unidata.ucar.edu/\n  active: false\n  credentialsfile: \"\"\n  other: {}\n  headlesswait: 0\n  delay: 0</code></pre>"},{"location":"indexing/tools/gleaner-docs/config/#sitegraph","title":"sitegraph","text":"<pre><code>- sourcetype: sitegraph\n  name: aquadocs\n  logo: \"\"\n  url: https://oih.aquadocs.org/aquadocs.json\n  headless: false\n  pid: http://hdl.handle.net/1834/41372\n  propername: AquaDocs\n  domain: https://aquadocs.org\n  active: false\n  credentialsfile: \"\"\n  other: {}\n  headlesswait: 0\n  delay: 0</code></pre>"},{"location":"indexing/tools/gleaner-docs/config/#example-config-file","title":"Example config file","text":"<p>A complete configuration file follows.  You can download the file here.  </p> <pre><code>context:\n  cache: true\n  strict: true\ncontextmaps:\n  - prefix: \"https://schema.org/\"\n    file: \"./schemaorg-current-https.jsonld\"  # wget http://schema.org/docs/jsonldcontext.jsonld\n  - prefix: \"http://schema.org/\"\n    file: \"./schemaorg-current-https.jsonld\"  # wget http://schema.org/docs/jsonldcontext.jsonld\ngleaner:\n  mill: true\n  runid: runX\n  summon: true\nsummoner:\n    after: \"\"\n    delay:  # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1)\n    headless: http://localhost:9222\n    mode: full\n    threads: 5\nmillers:\n  graph: true\nminio:\n  address:\n  port:\n  ssl:\n  accesskey:\n  secretkey:\n  bucket:\nsources:\n- sourcetype: sitemap\n  name: unidata\n  logo: \"\"\n  url: https://www.unidata.ucar.edu/sitemap.xml\n  headless: false\n  pid: https://www.re3data.org/repository/r3d100010355\n  propername: UNIDATA\n  domain: http://www.unidata.ucar.edu/\n  active: false\n  credentialsfile: \"\"\n  other: {}\n  headlesswait: 0\n  delay: 0\n- sourcetype: sitegraph\n  name: aquadocs\n  logo: \"\"\n  url: https://oih.aquadocs.org/aquadocs.json\n  headless: false\n  pid: http://hdl.handle.net/1834/41372\n  propername: AquaDocs\n  domain: https://aquadocs.org\n  active: false\n  credentialsfile: \"\"\n  other: {}\n  headlesswait: 0\n  delay: 0\n</code></pre>"},{"location":"indexing/tools/gleaner-docs/dockercli/","title":"Gleaner CLI Docker Image","text":""},{"location":"indexing/tools/gleaner-docs/dockercli/#about","title":"About","text":"<p>This approach for quick starting with Gleaner. Is based on  It is a script that exposes a containerized version of Gleaner as a CLI interface.</p> <p>You can use the -init flag to pull down all the support files you need including the Docker Compose file for setting up the object store, a triplestore and  the support for headless indexing.  </p>"},{"location":"indexing/tools/gleaner-docs/dockercli/#prerequisites","title":"Prerequisites","text":"<p>You need Docker installed.  Later, to work with the results and load them into a triplestore, you will also need an S3 compatible client.  We will use the Minio client, mc, for this.  </p>"},{"location":"indexing/tools/gleaner-docs/dockercli/#steps","title":"Steps","text":"<p>Download the script gleanerDocker.sh from https://github.com/gleanerio/gleaner/tree/master/docs/cliDocker You may need to make it run-able with </p> <pre><code>curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/docs/cliDocker/gleanerDocker.sh\n\nchmod 755 gleanerDocker.sh</code></pre> <p>Next you can run the script with the -init flag to pull down all the support files you need.</p> <pre><code>./gleanerDocker.sh -init</code></pre> <p>This will also download the needed docker image and the support files.  Your directory should look like this now:</p> <pre><code>fils@ubuntu:~/clidocker# ls -lt\ntotal 1356\n-rw-r--r-- 1 fils fils    1281 Aug 15 14:07 gleaner-IS.yml\n-rw-r--r-- 1 fils fils     290 Aug 15 14:07 setenvIS.sh\n-rw-r--r-- 1 fils fils    1266 Aug 15 14:07 demo.yaml\n-rw-r--r-- 1 fils fils 1371350 Aug 15 14:07 schemaorg-current-https.jsonld\n-rwxr-xr-x 1 fils fils    1852 Aug 15 14:06 gleanerDocker.sh</code></pre> <p>Let's see if we can setup our support infrastructure for Gleaner.  The  file gleaner-IS.yml is a docker compose file that will set up the object store, and a triplestore.</p> <p>To do this we need to set up a few environment variables.  To do this we can  leverage the setenvIS.sh script.  This script will set up the environment we need. Note you can also use a .env file or other approaches.  You can references  the Environment variables in Compose documentation.  </p> <pre><code>root@ubuntu:~/clidocker# source setenvIS.sh \nroot@ubuntu:~/clidocker# docker-compose -f gleaner-IS.yml up -d\nCreating network \"clidocker_traefik_default\" with the default driver\nCreating clidocker_triplestore_1 ... done\nCreating clidocker_s3system_1    ... done\nCreating clidocker_headless_1    ... done</code></pre> <p>Note:  In a fresh run all the images will be pulled down.  This may take a while.</p> <p>In the end, you should be able to see these images running:</p> <pre><code>root@ubuntu:~/clidocker# docker ps\nCONTAINER ID        IMAGE                            COMMAND                  CREATED              STATUS              PORTS                                              NAMES\na26f7c945479        nawer/blazegraph                 \"docker-entrypoint.s\u2026\"   About a minute ago   Up About a minute   0.0.0.0:9999-&gt;9999/tcp                             clidocker_triplestore_1\nf3a4197c42be        minio/minio:latest               \"/usr/bin/docker-ent\u2026\"   About a minute ago   Up About a minute   0.0.0.0:9000-&gt;9000/tcp, 0.0.0.0:54321-&gt;54321/tcp   clidocker_s3system_1\n062f029462b1        chromedp/headless-shell:latest   \"/headless-shell/hea\u2026\"   About a minute ago   Up About a minute   0.0.0.0:9222-&gt;9222/tcp  </code></pre> <p>At this point we should be able to do a run.  During the init process a  working config file was downloaded.   </p> <p>Note:  This config file will change...  it's pointing to an OIH partner  and I will not do that for the release.  I have a demo site I will use.  </p> <p>Next we need to setup our object for Gleaner.  Gleaner itself can do this  task so we will use </p> <pre><code>root@ubuntu:~/clidocker# ./gleanerDocker.sh -setup -cfg demo\nmain.go:35: EarthCube Gleaner\nmain.go:110: Setting up buckets\ncheck.go:58: Gleaner Bucket gleaner not found, generating\nmain.go:117: Buckets generated.  Object store should be ready for runs</code></pre> <p>Note:  Here is where we go off the rails.  The config file uses 0.0.0.0 as the  location and this is not working.   You need to edit the config file with the  \"real\" IP of the host machine.  In may case is this 192.168.122.77.  This is  obviously still a local network IP but it does work.  I am still investigating  this issue.</p> <p>We can now do a run with the example template file.  </p> <p>Note:  Best to delete the \"sitegraph\" node, I will do that soon.  It should  work, but is currently slow and gives little feedback</p> <p>If everything goes well, you should see something like the following:</p> <pre><code>root@ubuntu:~/clidocker# ./gleanerDocker.sh -cfg demo\nmain.go:35: EarthCube Gleaner\nmain.go:122: Validating access to object store\ncheck.go:39: Validated access to object store: gleaner.\norg.go:156: Building organization graph (nq)\norg.go:163: {samplesearth  https://samples.earth/sitemap.xml false https://www.re3data.org/repository/samplesearth Samples Earth (DEMO Site) https://samples.earth}\nmain.go:154: Sitegraph(s) processed\nsummoner.go:16: Summoner start time: 2021-08-15 14:34:08.907152656 +0000 UTC m=+0.067250623 \nresources.go:74: samplesearth : 202\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 20 it/s)        \nsummoner.go:34: Summoner end time: 2021-08-15 14:34:20.36804137 +0000 UTC m=+11.528139340 \nsummoner.go:35: Summoner run time: 0.191015 \nwebfeed.go:37: 1758\nmillers.go:26: Miller start time: 2021-08-15 14:34:20.368063453 +0000 UTC m=+11.528161421 \nmillers.go:40: Adding bucket to milling list: summoned/samplesearth\nmillers.go:51: Adding bucket to prov building list: prov/samplesearth\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 236 it/s)        \ngraphng.go:77: Assembling result graph for prefix: summoned/samplesearth to: milled/samplesearth\ngraphng.go:78: Result graph will be at: results/runX/samplesearth_graph.nq\npipecopy.go:16: Start pipe reader / writer sequence\ngraphng.go:84: Pipe copy for graph done\nmillers.go:80: Miller end time: 2021-08-15 14:34:21.84702814 +0000 UTC m=+13.007126109 \nmillers.go:81: Miller run time: 0.024649 \n</code></pre>"},{"location":"indexing/tools/gleaner-docs/dockercli/#working-with-results","title":"Working with results","text":"<p>If all has gone well, at this point you have downloaded the JSON-LD documents into Minio or  some other object store.Next we will install a client that we can use to work with these objects.</p> <p>Note, there is a web interface exposed on the port mapped in the Docker compose file. In the case of these demo that is 9000.  You can access it at http://localhost:9000/ with the credentials set in the environment variable file.  </p> <p>However, to work with these objects it would be better to use a command line tool, like mc. The Minio Client, can be installed following their Minio Client Quickstate Guide. Be sure to place it somewhere where it can be seen from the command line, ie, make sure it is in your PATH variable. </p> <p>If you are on Linux this might look something like:</p> <pre><code>wget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\n./mc --help</code></pre> <p>There is also a Minio Client Docker image  that you can use as well but it will be more difficult to use with the following scripts due to container isolation. </p> <p>To man an entry in the mc config use:</p> <pre><code>mc alias set oih  http://localhost:9000 worldsbestaccesskey worldsbestsecretkey</code></pre> <p>We should now be able to list our object store.  We have set it up using the alias oih.</p> <pre><code>user@ubuntu:~/clidocker# mc ls oih\n[2021-08-15 14:31:20 UTC]     0B gleaner/\nuser@ubuntu:~/clidocker# mc ls oih/gleaner\n[2021-08-19 13:36:04 UTC]     0B milled/\n[2021-08-19 13:36:04 UTC]     0B orgs/\n[2021-08-19 13:36:04 UTC]     0B prov/\n[2021-08-19 13:36:04 UTC]     0B results/\n[2021-08-19 13:36:04 UTC]     0B summoned/</code></pre> <p>You can explore mc and see how to copy and work with the object store.  </p>"},{"location":"indexing/tools/gleaner-docs/dockercli/#loading-to-the-triplestore","title":"Loading to the triplestore","text":"<p>As part of our Docker compose file we also spun up a triplestore.  Let's use that now.  </p> <p>Now Download the minio2blaze.sh script.</p> <pre><code>curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/scripts/minio2blaze.sh\nchmod 755 minio2blaze.sh </code></pre> <p>The content we need to load into the triplestore needs to be in RDF for Blazegraph.  We also need to tell the triplestore how we have encoded that RDF.   If look in the object store at</p> <pre><code>mc ls oih/gleaner/milled\n[2021-08-19 13:26:52 UTC]     0B samplesearth/</code></pre> <p>We should see a bucket that is holding the RDF data converted from the JSON-LD.  Let's use this in our test.   We can pass this path to the minio2blaze.sh script.  This script will go looking  for the mc command we installed above, so be sure it is in a PATH location that script can see.  </p> <pre><code>./minio2blaze.sh oih/gleaner/milled/samplesearth\n...   lots of results removed </code></pre> <p>If all has gone well, we should have RDF in the triplestore.  We started our triplestore as part of the docker-compose.yml file.  You can visit the triplestore at http://localhost:9999/blazegraph/#splash</p> <p>Note, you may have to try other addresses other than localhost if your networking is a bit different with Docker.  For me, I had to use a real local IP address for my network, you might also try 0.0.0.0.</p> <p>Hopefully you will see something like the following.</p> <p></p> <p>We loaded into the default kb namespace, so we should be good there.  We can see that is listed  as the active namespace at the Current Namespace: kb report.  </p> <p>Let's try a simple SPARQL query.  Click on the Query tab to get to the query user interfaced. We can use something like:</p> <pre><code>select * \nwhere \n{ \n  ?s ?p ?o\n}\nLIMIT 10</code></pre> <p>A very simple SPARQL to give us the first 10 results from the triplestore.  If all has gone well,  we should see something like:</p> <p></p> <p>You can explore more about SPARQL and the wide range of queries you can do with it at the W3C SPARQL 1.1 Query Language reference. </p>"},{"location":"indexing/tools/gleaner-docs/dockercli/#conclusion","title":"Conclusion","text":"<p>We have attempted here to give a quick introduction to the use of Gleaner in a Docker  environment.  This is a very simple example, but it should give you an idea of the approach used.  This approach can then be combined with other approaches documented to establish a  more production oriented implementation.  Most of this documentation will be located at the Gleaner.io GitHub repository and Gleaner repository.</p> <p>Note: The plan is to merge the Gleaner.io GitHub repository into the first. </p>"},{"location":"indexing/tools/gleaner-docs/faircontext/","title":"Gleaner in the Context of Communities Leveraging FAIR Principle","text":""},{"location":"indexing/tools/gleaner-docs/faircontext/#the-community-context","title":"The Community Context","text":"<p>This document provides some context to where GleanerIO can be leveraged by communities who are implementing FAIR principles.   These could be communities of practice or groups of domain related facilities. </p> <p>To provide further context a set of personas can be defined that help set the stage and provide scoping for the various parts of a community and their relations.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#personas","title":"Personas","text":""},{"location":"indexing/tools/gleaner-docs/faircontext/#about","title":"About","text":"<p>To provide better context we can define three personas to better express the roles within a community.  </p> <p></p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#persona-publisher","title":"Persona: Publisher","text":"<p>The Publisher is engaged authoring the JSON-LD documents and publishing them  to the web.  This persona is focused on describing and presenting structured data on the web to aid in the discovery and use the resources they manage.   Details on this persona can be found in the Publisher section. Additionally, this persona would be leveraging this encoding described in the JSON-LD Foundation section and the  profiles described in the Thematic Patterns. </p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#persona-indexer","title":"Persona: Indexer","text":"<p>The Indexer or Aggregator is a person or organization who is indexing resources on the  web using the structured data on the web patterns described in this documentation. Their goal is to efficiently and efficiently index the resources exposed by the Publisher  persona and generate usable indexes.  Further, they would work to exposed these indexes in  a manner that is usable by the User persona. Details on the approach used by OIH and potential alternatives can be found in the  Aggregator section.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#persona-user","title":"Persona: User","text":"<p>The user is the individual or community who wished to leverage the indexes generated as a result of the publishing and aggregation activities. The user may be using the  developed knowledge graph or some web interface built on top of the knowledge graph or  other index.  They may also use query languages like SPARQL or other APIs or even  directly work with the underlying data warehouse of collected data graphs.  </p> <p>User tools may be web sites or scientific notebooks.  Some examples of these  user experiences are described in the User section.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#fair-implementation-network","title":"FAIR Implementation Network","text":"<p>We can think of the above personnas and how they might be represented in a FAIR  implementation network.  The diagram that follow represents some of these relations.</p> <p></p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#gleaner","title":"Gleaner","text":"<p>Gleaner is a tool for extracting JSON-LD from web pages. You provide Gleaner a list of sites to index and it will access  and retrieve pages based on the sitemap.xml of the domain(s). Gleaner can then check for well formed and valid structure  in documents. The product of Gleaner runs can then be used to form Knowledge Graphs, Full-Text Indexes, Semantic Indexes, Spatial Indexes or other products to drive discovery and use.</p> <p>GleanerIO is the set of reference tools that have been developed to support organizations to harvest and leverage  structure data on the web.  The tools can be found at: https://github.com/gleanerio and include:</p> <ul> <li>Gleaner:  a structured data harvesting tool</li> <li>Nabu: an ETL/ELT tool for loading JSON-LD data graphs into various data systems such as triple-stores, text index, spatial indexes, etc.</li> <li>Scheduler: an automated workflow system built on Dagster</li> <li>Example Jupyter notebooks</li> <li>Example web UIs</li> <li>Documentation </li> </ul> <p></p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#principles-over-project","title":"Principles over Project","text":"<p>The approach to using structured data on the web can be viewed in the context of Principles over Project.  The view that the import aspects are the basic principles and not the implementation or code used to address the goal.  Nothing in Gleaner or in an activity  implementation using Gleaner is critical.   All the components of the approach can be replaced with other approaches, including Gleaner itself which can be replaced with other software packages.  </p> Principles Project Structured data on the web Gleaner harvested data graphs Web architecture https Semantics / Context schema.org (Science on Schema) DCAT, GepSPARQL, PROV Open formats JSON-LD"},{"location":"indexing/tools/gleaner-docs/faircontext/#example-activity-diagram-for-one-user-internet-of-water","title":"Example activity diagram for one user (Internet of Water)","text":""},{"location":"indexing/tools/gleaner-docs/faircontext/#fair-principles","title":"FAIR Principles","text":"<p>Recall the FAIR principles which are noted at many locations.  For reference you can visit the Go-FAIR FAIR Principles page. </p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#findable","title":"Findable","text":"Principles Project Resolvable URIs that resolve to metadata records Web architecture based indexing  Proper @id use in JSON-LD Use PIDs and Controlled Vocabularies Harvested data graphs (JSON-LD) to form a KG.  Keywords and other elements leverage PIDs and resolvable terms Validation here though indexing and inspection Data graphs can be framed and used for spatial, text or semantic indexes"},{"location":"indexing/tools/gleaner-docs/faircontext/#accessible","title":"Accessible","text":"Principles Project Distribution URLs Implemented with schema:distribution or related Access control details Implemented with schema:conditionOfAccess or related Validation We can implement validation of these elements with SHACL.  This allows us to build reports and continuous integration style approaches"},{"location":"indexing/tools/gleaner-docs/faircontext/#interoperable","title":"Interoperable","text":"Principles Project Open formats JSON-LD Open vocabularies schema.org, DCAT, GeoSPARQL, PROV Define associated data models Connect established data models and or associated tooling (GeoCODES resources registry)"},{"location":"indexing/tools/gleaner-docs/faircontext/#reusable","title":"Reusable","text":"Principles Project License schema:license or related (again, here we can leverage SHACL validation) Community standards Ocean InfoHub, POLDER, CCADI, GeoCODEs, Internet of Water"},{"location":"indexing/tools/gleaner-docs/faircontext/#users","title":"Users","text":"<p>The users of GleanerIO can really be seen as examples of things like Go FAIR Implementation Networks or related. Examples of such groups using this tooling follow. </p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#users-of-gleaner","title":"Users of Gleaner","text":"<p>The following are some communities using or exploring the use of Gleaner.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#_1","title":"Communities and FAIR","text":"<p>https://geocodes.earthcube.org/</p> <p>GeoCODES is an NSF Earthcube program effort to better enable cross-domain discovery of and access to geoscience data and research tools. GeoCODES is made up of three components respectively.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#ocean-infohub","title":"Ocean InfoHub","text":"<p>https://oceaninfohub.org/</p> <p>The Ocean InfoHub (OIH) Project aims to improve access to global oceans information, data and knowledge products for management and sustainable development.The OIH will link and anchor a network of regional and thematic nodes that will improve online access to and synthesis of existing global, regional and national data, information and knowledge resources, including existing clearinghouse mechanisms. The project will not be establishing a new database, but will be supporting discovery and interoperability of existing information systems.The OIH Project is a three-year project funded by the Government of Flanders, Kingdom of Belgium, and implemented by the IODE Project Office of the IOC/UNESCO.</p> <ul> <li>OIH Book</li> <li>Example Validation</li> <li>Validation repo</li> </ul>"},{"location":"indexing/tools/gleaner-docs/faircontext/#polder-polar-data-discovery-enhancement-research","title":"Polder: Polar Data Discovery Enhancement Research","text":"<p>https://polder.info/</p> <p>Federated metadata search for the polar regions will dramatically simplify data discovery for polar scientists. Instead of searching dozens of metadata catalogues separately, a user can come to a single search page.</p> <p>This is a rapidly moving field and POLDER is working to find the best path forward for our community. POLDER is a collaboration between the Southern Ocean Observing System, Arctic Data Committee, and Standing Committee on Antarctic Data Management.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#canadian-consortium-for-arctic-data-interoperability","title":"Canadian Consortium for Arctic Data Interoperability","text":"<p>https://ccadi.ca/</p> <p>The Canadian Consortium for Arctic Data Interoperability (CCADI) is an initiative to develop an integrated Canadian arctic data management system that will facilitate information discovery, establish sharing standards, enable interoperability among existing data infrastructures, and that will be co-designed with, and accessible to, a broad user base. Key to the CCADI vision are: standards and mechanisms for metadata, data and semantic interoperability; a distributed data exchange platform; streamlined data services with common entry, access, search, match, analysis, visualization and output tools; an intellectual property and sensitive data service; and data stewardship capacity.</p>"},{"location":"indexing/tools/gleaner-docs/faircontext/#internet-of-water","title":"Internet of Water","text":"<p>Geoconnex</p> <p>Geoconnex rests on widespread adoption of metadata best practices, automatically harvesting metadata and indexing data to real-world hydrologic features (e.g. lakes, reservoirs, wells, streams, water distribution systems, monitoring locations). The resulting water-specific search index will be browsable from a common water metadata catalog for the IoW network in both a human and machine-readable format.</p>"},{"location":"indexing/tools/gleaner-docs/gleaner/","title":"Gleaner","text":"<p>Documentation will be coming here soon.  For now visit the  repository docs at: https://github.com/gleanerio/gleaner/tree/master/docs</p>"},{"location":"indexing/tools/gleaner-docs/nabu/","title":"Nabu","text":"<p>Documentation will be coming here soon.  For now visit the  repository docs at: https://github.com/gleanerio/nabu/tree/master/docs</p>"},{"location":"indexing/tools/gleaner-docs/quickstart/","title":"Gleaner Quick Start","text":""},{"location":"indexing/tools/gleaner-docs/quickstart/#about","title":"About","text":"<p>This document provides a quick approach to the setup and use of gleaner.</p> <p></p> <p>The basic flow of Gleaner begins with a data source sitemap.  That sitemap is  read by Gleaner. Gleaner will leverage an object store, S3 compliant, and, if  needed, a headless chrome instance.  </p> <p>This headless chrome is only needed if the site  inserts the JSON-LD into the HTML by means of Javascript.  If the JSON-LD is in the  HTML itself via <code>&lt;script&gt;</code>  tag, this is not needed.</p> <p>Once Gleaner runs the JSON-LD is present in the object store.  From there it can  loaded by scripts or other ETL programs, like Nabu, into a triplestore or other  data systems.</p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#1-pre-requisites","title":"1: Pre-requisites","text":"<p>Before running Gleaner there are a few requirements for backend architecture that  are required.  </p> <p>First is an AWS S3 compatible object store as a place to store and  access the objects.  The other, is a means to render pages when needed. This last point is only required if you are working with sources that place the JSON-LD  into the page with Javascript.  </p> <p>Finally, many of these systems are easiest to use when leveraging OCI compliant  container system like Podman or Docker.</p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#object-store","title":"Object Store","text":"<p>Gleaner uses any AWS S3 compatible object store.  These means you can use  cloud based object stores like Amazon AWS S3, Google Cloud Storage.  Using  commercial cloud is not required as there are open source and cross platform  solutions.  Two of those will be discoursed here but there are others  not discussed such as Ceph, </p> <p>Set up base config with</p> <pre><code>export MINIO_ADDRESS=192.168.202.114\nexport MINIO_PORT=49153\nexport MINIO_USE_SSL=false\nexport MINIO_ACCESS_KEY=bestkeyever\nexport MINIO_SECRET_KEY=bestsecretever\nexport MINIO_BUCKET=gleaner.test</code></pre>"},{"location":"indexing/tools/gleaner-docs/quickstart/#s3-ninja","title":"S3 ninja","text":"<p>For a fast setup for exploration or simple testing the S3 ninja  package is fast and simple to use.  From the documentation a quick start looks like:</p> <ul> <li>A readily packaged docker image is available at scireum/s3-ninja</li> <li>Run like <code>docker run -p 9444:9000 scireum/s3-ninja:latest</code></li> <li>Navigate to http://localhost:9444/ui</li> <li>Run S3 API-Calls against http://localhost:9444/ (e.g. http://localhost:9444/test-bucket/test-object)</li> <li>Provide an volume for /home/sirius/data to persist data accross restarts. S3Ninja runs as user id 2000 inside the container. If you link a existing directory  into your container, change the user/group id to 2000.</li> </ul> <p>Note that in the above approach the data will not be persisted between instance of  S3 ninja. So you may wish to add a local volume (directory) mount to the command.  </p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#minio","title":"Minio","text":"<p>A more production level approach would be to use Minio.  Minio is an  open source S3 compliant object store.  It is a strong and robust approach.  Minio has extensive documentation for use.  The OCI/Docker approach is provided by Minio at Quickstart for Containers</p> <p><pre><code>mkdir -p ~/minio/data\n\npodman run --privileged  --group-add keep-groups \\\n   -p 9000:9000 \\\n   -p 9090:9090 \\\n   -v ~/minio/data:/data \\\n   -e \"MINIO_ROOT_USER=ROOTNAME\" \\\n   -e \"MINIO_ROOT_PASSWORD=CHANGEME123\" \\\n   quay.io/minio/minio server /data --console-address \":9090\"\n   ```\n\nPoint your browser at:  http://localhost:9090/buckets \nand use the USER and PASSWORD you set above.\n\nYou should see something like:\n\n![step 1](assets/images/miniostep1.png)\n\nNext you will need to create a bucket.\n\n![step 2](assets/images/miniostep2.png)\n\nThe next step is to create access keys for Gleaner to.  Select the _Access Keys_\nmenu item.  Use the create access key button to generate a set of keys.  \n\n![step 3](assets/images/miniostep3.png)\n\nNote these down or download them.  The secret will not be shown again and you will\nneed it.\n\n![step 4](assets/images/miniostep4.png)\n\nAt this point Minio should be set up, running, have a bucket you can use and you have\nthe access values to use for Gleaner.  You will need these in the _Configuration_ section\nbelow. \n\n### Headless Chrome\n\nThis pre-requisite is based on the fact that JSON-LD can be placed into HTML pages\neither statically in the HTML or dynamically placed into the HTML by a javascript\nthat is run by the browser on load.  In this later case, it is required to run the \njavascript to update the HTML.   \n\nTo do this we use a \"headless chrome\" which can be thought of as the engine of Chrome\nrunning without a user interfaces that simply generates the HTML as would be done \nby a user browser.  \n\nThere are many ways to address this so we will simply present one based on \na OCI image that you can run with Docker or Podman, for example.  \n\nThis documentation will use the zenika/alpine-chrome which can also be found \non GitHUb at [https://github.com/Zenika/alpine-chrome](https://github.com/Zenika/alpine-chrome).\nThe prebuilt container can also be found at Docker Hub at\n[zenika/alpine-chrome](https://hub.docker.com/r/zenika/alpine-chrome).  \n\n\n## 2: Starting with Gleaner\n\nWith the required architecture set up and ready we are ready to get started with \nGleaner.\n\n### Download\n\nFirst we need to download the Gleaner release.  This can be found at\nthe URL [https://github.com/gleanerio/gleaner/releases](https://github.com/gleanerio/gleaner/releases).\nFor this documentation we will use the [v3.0.7_badsitemap](https://github.com/gleanerio/gleaner/releases/tag/v3.0.7_badsitemap).\nYou can download the release that matches your platform at this location.\n\nWe will be use the [https://github.com/gleanerio/gleaner/releases/download/v3.0.7_badsitemap/glcon-v3.0.7_badsitemap-linux-amd64.tar.gz](https://github.com/gleanerio/gleaner/releases/download/v3.0.7_badsitemap/glcon-v3.0.7_badsitemap-linux-amd64.tar.gz) \ndownload, but select that one that matches your OS and chip \narchitecture.  \n\n_Download_\n</code></pre> \u276f mkdir testrun \u276f cd testrun \u276f wget https://github.com/gleanerio/gleaner/releases/download/v3.0.7_badsitemap/gleaner-v3.0.7_badsitemap-linux-amd64.tar.gz ... \u276f ls gleaner-v3.0.7_badsitemap-linux-amd64.tar.gz</p> <p><pre><code>\n_Extract_</code></pre> \u276f tar -zxf gleaner-v3.0.7_badsitemap-linux-amd64.tar.gz \u276f ls configs  docs  gleaner  gleaner-v3.0.7_badsitemap-linux-amd64.tar.gz  README.md  schemaorg-current-https.jsonld  scriptsls</p> <p><pre><code>\nThere are several coponents to this download.  These include \nscripts, docs, etc.  \n\nFor this quickstart we will make a ```rundir``` and copy the elements we\nneed into that directory.\n</code></pre> \u276f mkdir rundir \u276f cd rundir \u276f cp ../gleaner . \u276f cp ../schemaorg-current-https.jsonld . \u276f cp ../configs/template/gleaner_base.yaml . <pre><code>\nAt this point we ware ready to generate or edit our \nconfiguration files.  \n\n\n\n### Configure\n\n_Object store secrets_\n\nThere are many ways you can deal with the secret values used\nto access the object store.  You can edit them directly into the \nmain gleaner config if you wish.\n\nAlternatively, Gleaner can read these values from environment \nvariables.  This can be useful when running Gleaner in a container\nenvironment but can also be used at the command line as in this \ndocumentation.\n\nTo use this approach, generate small scripts and set the \nenvironment variables according to your shells documentation.  This\nwill change and the example that follows is valid for bash or \nzsh style shells.  \n</code></pre> export MINIO_ADDRESS=192.168.202.114 export MINIO_PORT=9000 export MINIO_USE_SSL=false export MINIO_ACCESS_KEY=yourkeyhere export MINIO_SECRET_KEY=yoursecrethere export MINIO_BUCKET=gleaner.test <pre><code>\n&gt; Note:  be careful when naming buckets.  The structure must be compliant with \n&gt; S3 bucket naming conventions.  \n\n_Gleaner Config file_\n\n```yaml\ncontext:\n  cache: true\n  strict: true\ncontextmaps:\n  - prefix: \"https://schema.org/\"\n    file: \"./schemaorg-current-https.jsonld\"  # wget http://schema.org/docs/jsonldcontext.jsonld\n  - prefix: \"http://schema.org/\"\n    file: \"./schemaorg-current-https.jsonld\"  # wget http://schema.org/docs/jsonldcontext.jsonld\ngleaner:\n  mill: false\n  runid: runX\n  summon: true\nsummoner:\n    after: \"\"\n    delay:  # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1)\n    headless: http://127.0.0.1:9222  # URL for headless see docs/headless\n    mode: full\n    threads: 5\nmillers:\n  graph: true\nminio:\n  address:\n  port:\n  accessKey:\n  secretKey:\n  ssl:\n  bucket:\nsources:\n  - sourcetype: sitemap\n    name: iris\n    logo: http://ds.iris.edu/static/img/layout/logos/iris_logo_shadow.png\n    url: http://ds.iris.edu/files/sitemap.xml\n    headless: false\n    pid: https://www.re3data.org/repository/r3d100010268\n    propername: IRIS\n    domain: http://iris.edu\n    active: true\n    credentialsfile: \"\"\n    other: {}\n    headlesswait: 0\n    delay: 0\n</code></pre></p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#run","title":"Run","text":"<p>The current set of flags follows:</p> <pre><code>EarthCube Gleaner\nUsage of /tmp/go-build589932266/b001/exe/main:\n  -cfg string\n        Configuration file (can be YAML, JSON) Do NOT provide the extension in the command line. -cfg file not -cfg file.yml (default \"config\")\n  -log string\n        The log level to output (trace | debug | info | warn | error | fatal) (default \"warn\")\n  -mode string\n        Set the mode (full | diff) to index all or just diffs (default \"full\")\n  -rude\n        Ignore any robots.txt crawl delays or allow / disallow statements\n  -setup\n        Run Gleaner configuration check and exit\n  -source string\n        Override config file source(s) to specify an index target</code></pre> <p>With the above configurations set we should be able to run Gleaner with a specified source.  Note the example only has one source so this is not needed.  However when you have multiple source you can specify a source with the <code>-source</code> flag.</p> <p>This can be useful when running via a scheduling system or cron and you want to index certain targets on a given schedule.  </p> <p>If you exclude this <code>--source</code> flag, all the source will be indexed in a run.  </p> <p>So an example run would look like the following.  </p> <pre><code>\u276f ./gleaner -cfg gleanerconfigLocalNAS.yaml --source iris\n\nversion:\nEarthCube Gleaner\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (28/28, 5 it/s)\n-------RUN STATS --------\nStart 2022-11-21 08:32:49.183440428 -0600 CST m=+0.039051392\n---iris----\n   SitemapCount: 28\n   SitemapHttpError: 0\n   SitemapIssues: 0\n   SitemapSummoned: 28\n   SitemapStored: 28\nCalling cleanUp on a successful run\nOn success, will, if flagged, copy the log file to object store and delete it</code></pre>"},{"location":"indexing/tools/gleaner-docs/quickstart/#3-working-with-results","title":"3: Working with results","text":"<p>If all has gone well, at this point you have downloaded the JSON-LD documents into Minio or  some other object store.Next we will install a client that we can use to work with these objects.</p> <p>Note, there is a web interface exposed on the port mapped in the Docker compose file. In the case of these demo that is 9000.  You can access it at http://localhost:9000/ with the credentials set in the environment variable file.  </p> <p>However, to work with these objects it would be better to use a command line tool, like mc. The Minio Client, can be installed following their Minio Client Quickstate Guide. Be sure to place it somewhere where it can be seen from the command line, ie, make sure it is in your PATH variable. </p> <p>If you are on Linux this might look something like:</p> <pre><code>wget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\n./mc --help</code></pre> <p>There is also a Minio Client Docker image  that you can use as well but it will be more difficult to use with the following scripts due to container isolation. </p> <p>To man an entry in the mc config use:</p> <pre><code>mc alias set oih  http://localhost:9000 worldsbestaccesskey worldsbestsecretkey</code></pre> <p>We should now be able to list our object store.  We have set it up using the alias oih.</p> <pre><code>user@ubuntu:~/clidocker# mc ls oih\n[2021-08-15 14:31:20 UTC]     0B gleaner/\nuser@ubuntu:~/clidocker# mc ls oih/gleaner\n[2021-08-19 13:36:04 UTC]     0B milled/\n[2021-08-19 13:36:04 UTC]     0B orgs/\n[2021-08-19 13:36:04 UTC]     0B prov/\n[2021-08-19 13:36:04 UTC]     0B results/\n[2021-08-19 13:36:04 UTC]     0B summoned/</code></pre> <p>You can explore mc and see how to copy and work with the object store.  </p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#loading-to-the-triplestore","title":"Loading to the triplestore","text":"<p>As part of our Docker compose file we also spun up a triplestore.  Let's use that now.  </p> <p>Now Download the minio2blaze.sh script.</p> <pre><code>curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/scripts/minio2blaze.sh\nchmod 755 minio2blaze.sh </code></pre> <p>The content we need to load into the triplestore needs to be in RDF for Blazegraph.  We also need to tell the triplestore how we have encoded that RDF.   If look in the object store at</p> <pre><code>mc ls oih/gleaner/milled\n[2021-08-19 13:26:52 UTC]     0B samplesearth/</code></pre> <p>We should see a bucket that is holding the RDF data converted from the JSON-LD.  Let's use this in our test.   We can pass this path to the minio2blaze.sh script.  This script will go looking  for the mc command we installed above, so be sure it is in a PATH location that script can see.  </p> <pre><code>./minio2blaze.sh oih/gleaner/milled/samplesearth\n...   lots of results removed </code></pre> <p>If all has gone well, we should have RDF in the triplestore.  We started our triplestore as part of the docker-compose.yml file.  You can visit the triplestore at http://localhost:9999/blazegraph/#splash</p> <p>Note, you may have to try other addresses other than localhost if your networking is a bit different with Docker.  For me, I had to use a real local IP address for my network, you might also try 0.0.0.0.</p> <p>Hopefully you will see something like the following.</p> <p></p> <p>We loaded into the default kb namespace, so we should be good there.  We can see that is listed  as the active namespace at the Current Namespace: kb report.  </p> <p>Let's try a simple SPARQL query.  Click on the Query tab to get to the query user interfaced. We can use something like:</p> <pre><code>select * \nwhere \n{ \n  ?s ?p ?o\n}\nLIMIT 10</code></pre> <p>A very simple SPARQL to give us the first 10 results from the triplestore.  If all has gone well,  we should see something like:</p> <p></p> <p>You can explore more about SPARQL and the wide range of queries you can do with it at the W3C SPARQL 1.1 Query Language reference. </p>"},{"location":"indexing/tools/gleaner-docs/quickstart/#conclusion","title":"Conclusion","text":"<p>We have attempted here to give a quick introduction to the use of Gleaner in a Docker  environment.  This is a very simple example, but it should give you an idea of the approach used.  This approach can then be combined with other approaches documented to establish a  more production oriented implementation.  Most of this documentation will be located at the Gleaner.io GitHub repository and Gleaner repository.</p> <p>Note: The plan is to merge the Gleaner.io GitHub repository into the first. </p>"},{"location":"geocodes/docs/data_loading/install_glcon/","title":"Install tool glcon","text":""},{"location":"geocodes/docs/data_loading/install_glcon/#install-glcon","title":"Install glcon","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <ul> <li>create a directory <code>cd ~ ; mkdir indexing</code></li> <li>download and install: We will try to keep this updated, but for the latest release. <code>wget https://github.com/gleanerio/gleaner/releases/download/{{RELASE}}}}</code></li> </ul> <p><code>tar xf glcon-v3.0.8-linux-amd64.tar.gz</code></p> OS download linux intel glcon-{{VERSION}}-linux-amd64.tar.gz linux arm glcon-{{VERSION}}-linux-arm64.tar.gz mac glcon-{{VERSION}}-darwin-arm64.tar.gz windows  intel glcon-{{VERSION}}-windows-amd64.zip downloading glcon <pre><code>    3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    --2022-07-21 23:04:55--  https://github.com/gleanerio/gleaner/releases/download/v3.0.4-dev/glcon-v3.0.4-dev-linux-amd64.tar.gz\n    Resolving github.com (github.com)... 140.82.113.4\n    Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream [following]\n    --2022-07-21 23:04:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/127204495/28707eb9-9cd2-4d4e-8b94-5e27db26a08f?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220721%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20220721T230428Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=668c44362081f0506f138cc52483f54d73fbd48fa906365ac80909b3b5e2b787&amp;X-Amz-SignedHeaders=host&amp;actor_id=0&amp;key_id=0&amp;repo_id=127204495&amp;response-content-disposition=attachment%3B%20filename%3Dglcon-v3.0.4-dev-linux-amd64.tar.gz&amp;response-content-type=application%2Foctet-stream\n    Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n    Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 13826668 (13M) [application/octet-stream]\n    Saving to: \u2018glcon-v3.0.4-dev-linux-amd64.tar.gz\u2019\n\nglcon-v3.0.4-dev-linux- 100%[=============================&gt;]  13.19M  12.6MB/s    in 1.0s</code></pre> <p>See that it is installed <pre><code>ubuntu@geocodes-dev:~/indexing$ tar xf glcon-v3.0.4-dev-linux-amd64.tar.gz\nubuntu@geocodes-dev:~/indexing$ ls\nREADME.md  docs   glcon-v3.0.4-dev-linux-amd64.tar.gz  scripts\nconfigs    glcon  schemaorg-current-https.jsonld</code></pre></p> <ul> <li>test</li> </ul> <code>./glcon --help</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon --help\nINFO[0000] EarthCube Gleaner                            \nThe gleaner.io stack harvests JSON-LD from webpages using sitemaps and other tools\nstore files in S3 (we use minio), uploads triples to be processed by nabu (the next step in the process)\nconfiguration is now focused on a directory (default: configs/local) with will contain the\nprocess to configure and run is:\n* glcon config init --cfgName {default:local}\n  edit files, servers.yaml, sources.csv\n* glcon config generate --cfgName  {default:local}\n* glcon gleaner Setup --cfgName  {default:local}\n* glcon gleaner batch  --cfgName  {default:local}\n* run nabu (better description)\n\nUsage:\n  glcon [command]\n\nAvailable Commands:\n  completion  generate the autocompletion script for the specified shell\n  config      commands to intialize, and generate tools: gleaner and nabu\n  gleaner     command to execute gleaner processes\n  help        Help about any command\n  nabu        command to execute nabu processes\n\nFlags:\n      --access string        Access Key ID (default \"MySecretAccessKey\")\n      --address string       FQDN for server (default \"localhost\")\n      --bucket string        The configuration bucket (default \"gleaner\")\n      --cfg string           compatibility/overload: full path to config file (default location gleaner in configs/local)\n      --cfgName string       config file (default is local so configs/local) (default \"local\")\n      --cfgPath string       base location for config files (default is configs/) (default \"configs\")\n      --gleanerName string   config file (default is local so configs/local) (default \"gleaner\")\n  -h, --help                 help for glcon\n      --nabuName string      config file (default is local so configs/local) (default \"nabu\")\n      --port string          Port for minio server, default 9000 (default \"9000\")\n      --secret string        Secret access key (default \"MySecretSecretKeyforMinio\")\n      --ssl                  Use SSL boolean\n\nUse \"glcon [command] --help\" for more information about a command.</code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/","title":"Loading Data for The Initial Installation","text":"<p>This is step 4 of 5 major steps:</p> <ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol> <p>Step Overview:</p> <ol> <li>create data stores in minioadmin and graph</li> <li>install glcon, if not installed</li> <li>create a configuration file to install a small set of data<ol> <li><code>./glcon config init --cfgName gctest</code></li> <li>edit</li> <li><code>./glcon config generate --cfgName gctest</code></li> </ol> </li> <li>setup and summon data using 'gleaner' <ol> <li><code>./glcon gleaner setup --cfgName gctest</code></li> <li><code>./glcon gleaner batch --cfgName gctest</code></li> </ol> </li> <li>load data to graph using 'nabu' <ol> <li><code>./glcon nabu prefix --cfgName gctest</code></li> <li><code>./glcon nabu prune --cfgName gctest</code></li> </ol> </li> <li>Test data in Graph </li> <li>Example of how to edit the source</li> <li>edit gctest.csv</li> <li>regenerate configs</li> <li>rerun batch</li> <li>Run Summarize task. This is performance related.</li> </ol> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#step-details","title":"Step Details","text":""},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#setup-datastores","title":"Setup Datastores","text":"<p>There are several datastores required to enable data summoning(harvesting), converting to a graph. While the production presently uses the earthcube repository convention, we suggest that  tutorial and communities setting up an instance to use the geocodes repository pattern. Earthcube/Decoder staff should use the A Community pattern when setting up an instance for a community.</p> Repository config s3 Bucket graph namespaces notes GeocodesTest gctest gctest gctest, gctest_summary samples of actual datasets geocodes geocodes geocodes geocodes, geocodes_summary suggested standalone repository earthcube geocodes gleaner earthcube, summary DEFAULT PRODUCTION NAME A COMMUNITY eg {acomm} {acomm} {acomm}, {acomm}_summary A communities tenant repository <p>Initial Setup</p> <p>we will be setting up both the gctest and gecodes repositories.</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#setup-minio-buckets","title":"Setup Minio buckets","text":"<p>Gleaner extracts JSONLD from a web apge, and stores it in an s3 system (Minio) in </p> <p>go to https://minioadmin.{youhost}/</p> <p>create buckets gctest, and geocodes</p> <p>go to settings for the bucket and make  public.</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#setup-graph-stores","title":"Setup Graph stores.","text":"<p>Nabu pulls from the s3 system, converts to RDF quads, and uploads to a graph store.</p> <p>go to https://graph.{your host}</p> <p>namespace tab, create  a mode 'quads' namespace with full text index,  \"gctest\", and \"geocodes\"</p> <p>namespace tab, create mode 'triples' namespace with full text index, \"gctest_summary\", and \"geocodes_summary\"</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#install-indexing-software","title":"Install Indexing Software","text":"<p><code>glcon</code> is a console application that combines the functionality of Gleaner and Nabu into a single application. It also has features to create and manage configurations for gleaner and nabu.</p> <p>Install glcon</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#harvest-and-load-data","title":"Harvest and load data","text":"<p>Goal is to create a configuration file to load gctest data. The sitemap is here:</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#create-a-configuration-and-load-sample-data","title":"Create a configuration and load sample data","text":""},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#create-a-configuration-for-continuous-integration","title":"Create a configuration for Continuous Integration","text":"<code>./glcon config init --cfgName gctest</code> <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon config init --cfgName gctest\n    2022/07/21 23:27:31 EarthCube Gleaner\n    init called\n    make a config template is there isn't already one\n    ubuntu@geocodes-dev:~/indexing$ ls configs/gctest\n    README_Configure_Template.md  localConfig.yaml  sources.csv\n    gleaner_base.yaml             nabu_base.yaml\n    ubuntu@geocodes-dev:~/indexing$ </code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#copy-sources-list-to-configsgctest","title":"Copy sources list to configs/gctest","text":"<p>Note</p> <p>assumes you are in indexing, and have put the geocodes at ~/geocodes aka your home directory</p> <p><code>cp ~/geocodes/deployment/ingestconfig/gctest.csv configs/gctest/</code></p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#edit-files","title":"edit files:","text":"<p>You will need to change the localConfig.yaml</p> <code>nano configs/gctest/localConfig.yaml</code> <pre><code>---\nminio:\n  address: oss.{YOU HOST}\n  port: 433\n  accessKey: worldsbestaccesskey\n  secretKey: worldsbestaccesskey\n  ssl: true\n  bucket: gctest # can be overridden with MINIO_BUCKET\nsparql:\n  endpoint: https://graph.{YOU HOST}/blazegraph/namespace/gctest/sparql\ns3:\n  bucket: gctest # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here.\n  domain: us-east-1 \n#headless field in gleaner.summoner\nheadless: http://127.0.0.1:9222\nsourcesSource:\n  type: csv\n  location: gctest.csv \n# this can be a remote csv\n#  type: csv\n#  location: https://docs.google.com/spreadsheets/d/1G7Wylo9dLlq3tmXe8E8lZDFNKFDuoIEeEZd3epS0ggQ/gviz/tq?tqx=out:csv&amp;sheet=TestDatasetSources</code></pre> <p>regenerate</p> <p>if you edit localConfig.yaml, you need to regenerate the configs using <code>./glcon config generate --cfgName gctest</code></p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#generate-configs","title":"Generate configs","text":"<code>./glcon config generate --cfgName gctest</code> <pre><code>./glcon config generate --cfgName gctest\n2022/07/21 23:37:46 EarthCube Gleaner\ngenerate called\n{SourceType:sitemap Name:geocodes_demo_datasets Logo:https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources URL:https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml Headless:false PID:https://www.earthcube.org/datasets/ ProperName:Geocodes Demo Datasets Domain:0 Active:true CredentialsFile: Other:map[] HeadlessWait:0}\nmake copy of servers.yaml\nRegnerate gleaner\nRegnerate nabu</code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#flightest","title":"flightest","text":"<p>Run setup to see if you can connect to the minio store</p> `./glcon gleaner setup --cfgName gctest <pre><code>   ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner setup --cfgName gctest\n   2022/07/21 23:42:54 EarthCube Gleaner\n   Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\n   Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\n   setup called\n   2022/07/21 23:42:54 Validating access to object store\n   2022/07/21 23:42:54 Connection issue, make sure the minio server is running and accessible. The specified bucket does not exist.\n   ubuntu@geocodes-dev:~/indexing$ </code></pre> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect</li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#load-data","title":"Load Data","text":"<p>Gleaner will harvest jsonld from the URL's listed in the sitemap.</p> <p>Robots.txt</p> <p>OK TO IGNORE. you will need to ignore errors about robot.txt and sitemap.xml not being an index <pre><code>{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T20:45:53-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T20:45:53-06:00\"}    </code></pre></p> <p>Access issues</p> <pre><code>{\u201cfile\u201d:\u201c/github/workspace/internal/organizations/org.go:87\",\u201cfunc\u201d:\u201cgithub.com/gleanerio/gleaner/internal/organizations.BuildGraph\u201d,\u201clevel\u201d:\u201cerror\u201d,\u201cmsg\u201d:\u201corgs/geocodes_demo_datasets.nqThe Access Key Id you provided does not exist in our records.\u201c,\u201dtime\u201d:\u201c2023-01-31T15:27:39-06:00\u201d}</code></pre> <ul> <li>Access Key password could be incorrect </li> <li>address may be incorrect. It is a hostname or TC/IP, and not a URL</li> <li>ssl may need to be true</li> <li>See setup issues</li> </ul> <code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes-dev:~/indexing$ ./glcon gleaner batch --cfgName gctest\nINFO[0000] EarthCube Gleaner                            \nUsing gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner\nUsing nabu config file: /home/ubuntu/indexing/configs/gctest/nabu\nbatch called\n{\"file\":\"/github/workspace/internal/organizations/org.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/organizations.BuildGraph\",\"level\":\"info\",\"msg\":\"Building organization graph.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/pkg/gleaner.go:35\",\"func\":\"github.com/gleanerio/gleaner/pkg.Cli\",\"level\":\"info\",\"msg\":\"Sitegraph(s) processed\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:17\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner start time:2022-07-22 19:16:53.451745139 +0000 UTC m=+0.182100234\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:189\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"info\",\"msg\":\"Getting robots.txt from 0/robots.txt\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/utils.go:23\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsTxt\",\"level\":\"error\",\"msg\":\"error fetching robots.txt at 0/robots.txtGet \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:192\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for 0:Get \\\"0/robots.txt\\\": unsupported protocol scheme \\\"\\\"\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:63\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasetscontinuing without it.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:127\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getSitemapURLList\",\"level\":\"info\",\"msg\":\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/gh-pages/metadata/Dataset/sitemap.xml is not a sitemap index, checking to see if it is a sitemap\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:32\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResRetrieve\",\"level\":\"info\",\"msg\":\"Queuing URLs for geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/acquire.go:74\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getConfig\",\"level\":\"info\",\"msg\":\"Thread count 5 delay 0\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:53Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n12% |\u2588\u2588\u2588\u2588\u2588\u2588                                                 | (2/16, 2 it/s) [0s:7s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n43% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                                | (7/16, 6 it/s) [1s:1s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:54Z\"}\n68% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  | (11/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n75% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | (12/16, 6 it/s) [1s:0s]{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/jsonutils.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.Upload\",\"level\":\"info\",\"msg\":\"context.strict is not set to true; doing json-ld fixups.\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (16/16, 9 it/s)        \n{\"file\":\"/github/workspace/internal/summoner/summoner.go:37\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner end time:2022-07-22 19:16:55.660367672 +0000 UTC m=+2.390721648\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/summoner/summoner.go:38\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner.Summoner\",\"level\":\"info\",\"msg\":\"Summoner run time:0.0368103569\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:27\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller start time2022-07-22 19:16:55.661434567 +0000 UTC m=+2.391819553\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:44\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to milling list:summoned/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:55\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Adding bucket to prov building list:prov/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:55Z\"}\n100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (15/15, 51 it/s)        \n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:82\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Assembling result graph for prefix:summoned/geocodes_demo_datasetsto:milled/geocodes_demo_datasets\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:83\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Result graph will be at:results/runX/geocodes_demo_datasets_graph.nq\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/graph/graphng.go:89\",\"func\":\"github.com/gleanerio/gleaner/internal/millers/graph.GraphNG\",\"level\":\"info\",\"msg\":\"Pipe copy for graph done\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:84\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller end time:2022-07-22 19:16:56.387639969 +0000 UTC m=+3.117994225\",\"time\":\"2022-07-22T19:16:56Z\"}\n{\"file\":\"/github/workspace/internal/millers/millers.go:85\",\"func\":\"github.com/gleanerio/gleaner/internal/millers.Millers\",\"level\":\"info\",\"msg\":\"Miller run time:0.0121029112\",\"time\":\"2022-07-22T19:16:56Z\"}</code></pre> <p>See files in Minio</p> <p>You can open the minioadmin console (https://minioadmin.{your host}/) and look to see that file are uploaded into the bucket, in this case gctest.. summon/gecodes_demo_data</p> <p>(NEED IMAGE HERE)</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#push-to-graph","title":"Push to graph","text":"<p>Nabu will read files from the bucket, and push them to the graph store.</p> <code>./glcon nabu prefix --cfgName gctest</code> <p>```json lines ./glcon nabu prefix --cfgName gctest INFO[0000] EarthCube Gleaner                           Using gleaner config file: /home/ubuntu/indexing/configs/gctest/gleaner Using nabu config file: /home/ubuntu/indexing/configs/gctest/nabu check called 2022/07/22 19:23:16 Load graphs from prefix to triplestore {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:41\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"[milled/geocodes_demo_datasets prov/geocodes_demo_datasets org]\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:61\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.ObjectAssembly\",\"level\":\"info\",\"msg\":\"gleaner:milled/geocodes_demo_datasets object count: 15\\n\",\"time\":\"2022-07-22T19:23:16Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:79\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.PipeLoad\",\"level\":\"info\",\"msg\":\"Loading milled/geocodes_demo_datasets/11316929f925029101493e8a05d043b0ae829559.rdf \\n\",\"time\":\"2022-07-22T19:23:16Z\"} [snip] {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:197\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Status: 200 OK\",\"time\":\"2022-07-22T19:23:21Z\"} {\"file\":\"/go/pkg/mod/github.com/gleanerio/nabu@v0.0.0-20220223141452-a01fa9352430/internal/sparqlapi/pipeload.go:198\",\"func\":\"github.com/gleanerio/nabu/internal/sparqlapi.Insert\",\"level\":\"info\",\"msg\":\"response Headers: map[Access-Control-Allow-Credentials:[true] Access-Control-Allow-Headers:[Authorization,Origin,Content-Type,Accept] Access-Control-Allow-Origin:[*] Content-Length:[449] Content-Type:[text/html;charset=utf-8] Date:[Fri, 22 Jul 2022 19:23:21 GMT] Server:[Jetty(9.4.z-SNAPSHOT)] Vary:[Origin] X-Frame-Options:[SAMEORIGIN]]\",\"time\":\"2022-07-22T19:23:21Z\"} 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (1/1, 15 it/s)</p> <p>```</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#test-in-graph","title":"Test in Graph","text":"<p>One the data is loaded into the graph store <code>https://graph.{your host}/blazegraph/#query</code></p> <ol> <li>go to namespace tab, select gctest, </li> <li>go to query tab, input the </li> </ol> returns all triples <pre><code>select * \nwhere {\n?s ?p ?o\n }\nlimit 1000</code></pre> <p>A more complex query can be ran:</p> what types are in the system <pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n       ?s a ?type .\n       }\n} \nGROUP By ?type\nORDER By DESC(?scount)</code></pre> <p>A more complex query can be ran:</p> Show me just datasets <pre><code>SELECT (count(?g ) as ?count) \nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}</code></pre> <p>More SPARQL Examples</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#example-of-how-to-edit-the-source","title":"Example of how to edit the source","text":"<p>This demonstrates a feature where if you have duplicate identifiers, then you can ensure all data get loaded. It's a bad idea to have the same ID, but it happens.</p> <p>There are two lines in gctest csv.  The second dataset is [actual data] (https://github.com/earthcube/GeoCODES-Metadata/tree/main/metadata/Dataset/actualdata).  There are three files, the two earthchem files have the same @id, 1 2 The identifierType is set to 'filesha' which generates a sha based on the entire file.</p> gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,FALSE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#edit-gctestcsv","title":"edit gctest.csv","text":"<p>Set the second line active to TRUE</p> edited gctest cs <pre><code>hack,SourceType,Active,Name,ProperName,URL,Headless,HeadlessWait,IdentifierType,IdentifierPath,Domain,PID,Logo,validator link,NOTE\n58,sitemap,TRUE,geocodes_demo_datasets,Geocodes Demo Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/allgood/sitemap.xml,FALSE,0,identifiersha,,https://www.earthcube.org/datasets/allgood,https://github.com/earthcube/GeoCODES-Metadata/metadata/OtherResources,,,\n59,sitemap,TRUE,geocodes_actual_datasets,Geocodes Actual Datasets,https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/actualdata/sitemap.xml,FALSE,0,filesha,,https://www.earthcube.org/datasets/actual,https://github.com/earthcube/GeoCODES-Metadata/metadata/,,,</code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#regenerate-configs","title":"regenerate configs","text":"<p><code>./glcon config generate --cfgName gctest</code></p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#rerun-batch","title":"rerun batch","text":"<code>./glcon gleaner batch --cfgName gctest</code> <pre><code>ubuntu@geocodes:~/indexing$ ./glcon gleaner batch --cfgName gctest\nversion:  v3.0.8-fix129\nbatch called\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/allgood:Robots.txt unavailable at https://www.earthcube.org/datasets/allgood/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_demo_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:204\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.getRobotsForDomain\",\"level\":\"error\",\"msg\":\"error getting robots.txt for https://www.earthcube.org/datasets/actual:Robots.txt unavailable at https://www.earthcube.org/datasets/actual/robots.txt\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n{\"file\":\"/github/workspace/internal/summoner/acquire/resources.go:66\",\"func\":\"github.com/gleanerio/gleaner/internal/summoner/acquire.ResourceURLs\",\"level\":\"error\",\"msg\":\"Error getting robots.txt for geocodes_actual_datasets, continuing without it.\",\"time\":\"2023-01-30T21:09:49-06:00\"}\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (3/3, 10 it/s)        \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 25 it/s)        \nRunStats:\n  Start: 2023-01-30 21:09:49.120833598 -0600 CST m=+0.105789938\n  Repositories:\n    - name: geocodes_demo_datasets\n      SitemapCount: 9 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n      SitemapSummoned: 9 \n      SitemapStored: 9 \n    - name: geocodes_actual_datasets\n      SitemapSummoned: 3 \n      SitemapStored: 3 \n      SitemapCount: 3 \n      SitemapHttpError: 0 \n      SitemapIssues: 0 \n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (9/9, 168 it/s)\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (2/2, 123 it/s)</code></pre>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#create-a-materialized-view-of-the-data-using-summarize-to-the-repo_summary-namespace","title":"Create  a materialized view of the data using summarize to the  repo_summary namespace","text":"<p>DOCUMENTATION NEEDED </p> <p>(TBD assigned to Mike Bobak)</p>"},{"location":"geocodes/docs/data_loading/setup_indexing_with_gleanerio/#go-to-step-5","title":"Go to step 5.","text":"<ol> <li>Install base containers on a server</li> <li>Setup services containers</li> <li>Setup Gleaner containers</li> <li>Initial setup of services and loading of data</li> <li>Setup Geocodes UI using datastores defined in Initial Setup</li> </ol>"},{"location":"sos/assets/","title":"Index","text":"<p>A directory for static content such as images used by the guides.</p>"},{"location":"sos/assets/diagrams/","title":"Index","text":"<p>Diagrams of Class and Property recommendations for using schema.org and external vocabularies.</p> <p>Diagrams are created using Lucidchart.</p> <p> Creating Diagrams</p> <ol> <li>When recommending which properties of a schema.org class to use, creating a diagram of the connections between those resources and literal values can be helpful for visualizing the big picture.</li> <li>Diagrams should follow the following convention:   </li> <li>Diagrams are currently being made using Lucidchart, and are being shared to edit here: schema.org diagrams on Lucidchart</li> </ol>"},{"location":"sos/guides/DataRepository/","title":"DataRepository","text":"<p> Home | DataRepository</p>"},{"location":"sos/guides/DataRepository/#describing-a-repository","title":"Describing a Repository","text":"<ul> <li>Basic Fields</li> <li>Identifier</li> <li>Funding Source</li> <li>Services</li> <li>Data Collections</li> <li>Advanced Publishing Techniques</li> <li>How to use external vocabularies</li> </ul>"},{"location":"sos/guides/DataRepository/#describing-a-repository_1","title":"Describing a Repository","text":"<p>In schema.org, we model a repository as both an schema:ResearchProject, a sub-class of an Organization, and a schema:Service. This double-typing gives us the most flexibility in describing the characteristics of the organization providing the service and the services offered by the organization.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\"\n  \n}\n</pre> <p> The other fields you can use to describe the Organziation and the Service are:</p> <p></p> <ul> <li>schema:legalName should be the official name of the  repository,</li> <li>schema:name can be an acronym or the name typcially used for the repository,</li> <li>schema:url should be the url of your repository's homepage,</li> <li>schema:description should be text describing your repository,</li> <li>schema:sameAs can be used to link the repository to other URLs such as Re3Data, Twitter, LinkedIn, etc.,</li> <li>schema:category can be used to describe the discipline, domain, area of study that encompasses the repository's holdings.</li> </ul> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"sameAs\": [\n        \"http://www.re3data.org/repository/r3d1000000xx\",\n        \"https://twitter.com/SDRO\",\n        \"https://www.linkedin.com/company/123456789/\"\n    ],\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ]\n}\n</pre> <p>(See advanced publishing techniques for how to describe categories/disciplines in more detail than just simple text.)</p> <p>If you are using the \"@id\" attribute for your Repository, and the provider of the repository's services is the same Organziation, you can specify the schema:provider  of the schema:Service in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"@id\": \"https://www.sample-data-repository.org\",\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  }\n}\n</pre> <p>However, if your repository has a situation where multiple organizations act as the provider or you want to recognize a different organization as the provider of the repository's service, schema:provider can be used in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": [\n    {\n      \"@type\": \"ResearchProject\",\n      \"name\": \"SDRO Technical Office\",\n      \"description\": \"We provide all the infrastructure for the SDRO\"\n      ...\n    },\n    {\n      \"@type\": \"ResearchProject\",\n      \"name\": \"SDRO Science Support Office\",\n      \"description\": \"We provide all the science support functionality for the SDRO\"\n      ...\n    }\n  ]\n}\n</pre> <p>Adding additional fields of schema:ResearchProject:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  }\n  \"logo\": {\n    \"@type\": \"ImageObject\",\n    \"url\": \"https://www.sample-data-repository.org/images/logo.jpg\"\n  },\n  \"contactPoint\": {\n    \"@type\": \"ContactPoint\",\n    \"name\": \"Support\",\n    \"email\": \"info@bco-dmo.org\",\n    \"url\": \"https://www.sample-data-repository.org/about-us\",\n    \"contactType\": \"customer support\"\n  },\n  \"foundingDate\": \"2006-09-01\",\n  \"address\": {\n    \"@type\": \"PostalAddress\",\n    \"streetAddress\": \"123 Main St.\",\n    \"addressLocality\": \"Anytown\",\n    \"addressRegion\": \"ST\",\n    \"postalCode\": \"12345\",\n    \"addressCountry\": \"USA\"\n  }\n}\n</pre> <p>If this ResearchProject, or Organization, has a parent entity such as a college, university or research center, that information can be provided using the schema:parentOrganization property:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  },\n   \"parentOrganization\": {\n     \"@type\": \"Organization\",\n     \"@id\": \"http://www.someinstitute.edu\",\n     \"legalName\": \"Some Institute\",\n     \"name\": \"SI\",\n     \"url\": \"http://www.someinstitute.edu\",\n     \"address\": {\n       \"@type\": \"PostalAddress\",\n       \"streetAddress\": \"234 Main St.\",\n       \"addressLocality\": \"Anytown\",\n       \"addressRegion\": \"ST\",\n       \"postalCode\": \"12345\",\n       \"addressCountry\": \"USA\"\n     }\n   }\n  }\n}\n</pre> <p>Back to top</p> <p></p>"},{"location":"sos/guides/DataRepository/#describing-a-repositorys-identifier","title":"Describing a Repository's Identifier","text":"<p>Some organizations may have a persistent identifier (DOI) assigned to their organization from authorities like the Registry of Research Data Repositories (re3data.org). The way to describe these organizational identifiers is to use the schema:identifier property in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  },\n  \"identifier\": {\n    \"@type\": \"PropertyValue\",\n    \"name\": \"Re3data DOI: 10.17616/R37P4C\",\n    \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n    \"value\": \"doi:10.17616/R37P4C\",\n    \"url\": \"https://doi.org/10.17616/R37P4C\"\n  }\n}\n</pre> <p>For more information on describing identifiers, see the Dataset - Identifier guide.</p> <p></p>"},{"location":"sos/guides/DataRepository/#describing-a-repositorys-funding-source","title":"Describing a Repository's Funding Source","text":"<p>To describe the funding source of a repository, you use the schema:funder property of schema:Organization:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"category\": [\n    \"Biological Oceanography\",\n    \"Chemical Oceanography\"\n  ],\n  \"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  },\n   \"parentOrganization\": {\n     \"@type\": \"Organization\",\n     \"@id\": \"http://www.someinstitute.edu\",\n     \"legalName\": \"Some Institute\",\n     \"name\": \"SI\",\n     \"url\": \"http://www.someinstitute.edu\",\n     \"address\": {\n       \"@type\": \"PostalAddress\",\n       \"streetAddress\": \"234 Main St.\",\n       \"addressLocality\": \"Anytown\",\n       \"addressRegion\": \"ST\",\n       \"postalCode\": \"12345\",\n       \"addressCountry\": \"USA\"\n     },\n     \"funder\": {\n      \"@type\": \"Organization\",\n      \"@id\": \"https://doi.org/10.13039/100000141\",\n      \"legalName\": \"Division of Ocean Sciences\",\n      \"alternateName\": \"OCE\",\n      \"url\": \"https://www.nsf.gov/div/index.jsp?div=OCE\",\n      \"identifier\": {\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n        \"value\": \"doi:10.13039/100000141\",\n        \"url\": \"https://doi.org/10.13039/100000141\"\n      },\n      \"parentOrganization\": {\n        \"@type\": \"Organization\",\n        \"@id\": \"http://doi.org/10.13039/100000085\",\n        \"legalName\": \"Directorate for Geosciences\",\n        \"alternateName\": \"NSF-GEO\",\n        \"url\": \"http://www.nsf.gov\",\n        \"identifier\": {\n          \"@type\": \"PropertyValue\",\n          \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n          \"value\": \"doi:10.13039/100000085\",\n          \"url\": \"https://doi.org/10.13039/100000085\"\n         },\n        \"parentOrganization\": {\n          \"@type\": \"Organization\",\n          \"@id\": \"http://dx.doi.org/10.13039/100000001\",\n          \"legalName\": \"National Science Foundation\",\n          \"alternateName\": \"NSF\",\n          \"url\": \"http://www.nsf.gov\",\n          \"identifier\": {\n            \"@type\": \"PropertyValue\",\n            \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n            \"value\": \"doi:10.13039/100000001\",\n            \"url\": \"https://doi.org/10.13039/100000001\"\n          }\n        }\n      }\n    }\n  }\n}\n</pre> <p>Back to top</p> <p></p>"},{"location":"sos/guides/DataRepository/#describing-a-repositorys-services","title":"Describing a Repository's Services","text":"<p>For repositories might offer services for accessing data as opposed to directly accessing data files. The schema:Service allows us to describe these services as well as repository searches, data submission services, and syndication services. In this first example, we describe a search service at the repository using schema:ServiceChannel.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  ...\n  \"availableChannel\": [\n    {\n      \"@type\": \"ServiceChannel\",\n      \"serviceUrl\": \"https://www.sample-data-repository.org/search\",\n      \"providesService\": {\n        \"@type\": \"Service\",\n        \"name\": \"SDRO Website Search\",\n        \"description\": \"Search for webpages, datasets, authors, funding awards, instrumentation and measurements\",\n        \"potentialAction\": {\n          \"@type\": \"SearchAction\",\n          \"target\": \"https://www.sample-data-repository.org/search?keywords={query_string}\",\n          \"query-input\": {\n            \"@type\": \"PropertyValueSpecification\",\n            \"valueRequired\": true,\n            \"valueName\": \"query_string\"\n          }\n        }\n      }\n    }\n  ]\n}\n</pre> <p>By specifying the schema:potentialAction, we create a machine-actionable way to execute searches.</p> <p>Back to top</p> <p></p>"},{"location":"sos/guides/DataRepository/#describing-a-repositorys-data-collections","title":"Describing a Repository's Data Collections","text":"<p>If your repository has a concept of a data collection, some grouping of a number of datasets, we can use the schema:DataCatalog to describe these collections using the schema:OfferCatalog. One exampel of a DataCatalog might be to group datasets by a categorization such as 'biological data' or 'chemical data'. Or a catalog could be grouped by instrument, parameter or whatever logical grouping a repository may have.</p> <p></p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  ...\n  \"hasOfferCatalog\": {\n    \"@type\": \"OfferCatalog\",\n    \"name\": \"Sample Data Repository Resource Catalog\",\n    \"itemListElement\": [\n      {\n       \"@type\": \"DataCatalog\",\n        \"@id\": \"https://www.sample-data-repository.org/collection/biological-data\",\n        \"name\": \"Biological Data\",\n        \"audience\": {\n          \"@type\": \"Audience\",\n          \"audienceType\": \"public\",\n          \"name\": \"General Public\"\n        }\n      },\n      {\n        \"@type\": \"DataCatalog\",\n        \"@id\": \"https://www.sample-data-repository.org/collection/geological-data\",\n        \"name\": \"Geological Data\",\n        \"audience\": {\n          \"@type\": \"Audience\",\n          \"audienceType\": \"public\",\n          \"name\": \"General Public\"\n        }\n      }\n    ]\n  }\n}\n</pre> <p>Back to top</p> <p></p>"},{"location":"sos/guides/DataRepository/#advanced-publishing-techniques","title":"Advanced Publishing Techniques","text":""},{"location":"sos/guides/DataRepository/#how-to-publish-resources-for-the-categoriesdisciplines-at-repository-services","title":"How to publish resources for the categories/disciplines at repository services.","text":""},{"location":"sos/guides/DataRepository/#how-to-use-external-vocabularies","title":"&amp; How to use external vocabularies","text":"<p>The SWEET ontology defines a number of science disciplines and a repository could reference those, or another vocabuary's resources, by adding the vocabular to the <code>@context</code> attribute of the JSON-LD markup.</p> <pre>\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"sweet-rel\": \"http://sweetontology.net/rela/\",\n      \"sweet-kd\": \"http://sweetontology.net/humanKnowledgeDomain/\"\n    }\n  ],\n  \"@type\": [\"Service\", \"ResearchProject\"],\n  \"legalName\": \"Sample Data Repository Office\",\n  \"name\": \"SDRO\",\n  \"url\": \"https://www.sample-data-repository.org\",\n  \"description\": \"The Sample Data Repository Service provides access to data from an imaginary domain accessible from this website.\",\n  \"sweet-rel:hasRealm\": [\n    { \"@id\": \"sweet-kd:Biogeochemistry\" },\n    { \"@id\": \"sweet-kd:Oceanography\" }\n  ]\n  \n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/","title":"Dataset","text":"<p> Home | Dataset</p>"},{"location":"sos/guides/Dataset/#describing-a-dataset","title":"Describing a Dataset","text":"<ul> <li>Describing a Dataset<ul> <li>Common Properties</li> <li>Keywords</li> <li>Identifier<ul> <li>How to reference Short DOIs</li> </ul> </li> <li>Variables</li> <li>Collections of datasets using schema.org DataCatalog</li> <li>Metadata</li> <li>Distributions<ul> <li>Accessing Data through a Service Endpoint</li> </ul> </li> <li>Dates</li> <li>Temporal Coverage<ul> <li>Geologic Time</li> </ul> </li> <li>Spatial Coverage<ul> <li>Use GeoCoordinates for Point locations</li> <li>Use GeoShape for all other location types</li> <li>Handling multiple locations</li> <li>Spatial Reference Systems</li> </ul> </li> <li>Roles of People</li> <li>Publisher and Provider</li> <li>Funding</li> <li>License</li> <li>Checksum</li> <li>Provenance Relationships</li> </ul> </li> </ul>"},{"location":"sos/guides/Dataset/#common-properties","title":"Common Properties","text":"<p>Google has drafted a guide to help publishers. The guide describes the only required fields as - name and description. * name - A descriptive name of a dataset (e.g., \u201cSnow depth in Northern Hemisphere\u201d) * description - A short summary describing a dataset.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \"\n}\n</pre> <p>The Google guide also recommends following fields:</p> <ul> <li>url - Location of a page describing the dataset.</li> <li>sameAs - Other URLs that can be used to access the dataset page. A link to a page that provides more information about the same dataset, usually in a different repository.</li> <li>version - The version number or identifier for this dataset (text or numeric).</li> <li>isAccessibleForFree - Boolean (true|false) specifying if the dataset is accessible for free.</li> <li>keywords - Keywords summarizing the dataset.</li> <li>identifier - An identifier for the dataset, such as a DOI. (text,URL, or PropertyValue).</li> <li>variableMeasured - What does the dataset measure? (e.g., temperature, pressure)</li> </ul> <p></p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"sameAs\": \"https://search.dataone.org/#view/https://www.sample-data-repository.org/dataset/472032\",\n  \"version\": \"2013-11-21\",\n  \"isAccessibleForFree\": true,\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"],\n  \"license\": [ \"http://spdx.org/licenses/CC0-1.0\", \"https://creativecommons.org/publicdomain/zero/1.0\"]\n  \n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#keywords","title":"Keywords","text":"<p>Adding the schema:keywords field can be done in three ways - a text description, a URL, or by using schema:DefinedTerm. We recommend using <code>schema:DefinedTerm</code> if a keyword comes from a controlled vocabulary.</p> <p></p>"},{"location":"sos/guides/Dataset/#keywords-as-text","title":"Keywords as Text","text":"<p>For a dataset with the keywords of: <code>ocean acidification</code>, <code>Dissolved Organic Carbon</code>, <code>bacterioplankton respiration</code>, <code>pCO2</code>, <code>carbon dioxide</code>, <code>oceans</code>, you can express these:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"]\n}\n</pre>"},{"location":"sos/guides/Dataset/#keywords-as-definedterm","title":"Keywords as DefinedTerm","text":"<p>If you have information about a controlled vocabulary from which keywords come from,  use <code>schema:DefinedTerm</code> to descibe that kewyword. The relevant properties of a <code>schema:DefinedTerm</code> are:</p> <ul> <li>name - The name of the keyword. (Required)</li> <li>inDefinedTermSet - The controlled vocabulary responsible for this keyword. (Required)</li> <li>url - The canonical URL for the keyword. (Optional)</li> <li>termCode - A representative code for this keyword in the controlled vocabulary (Optional)</li> </ul> <p>As an example, we demonstrate these fields using the <code>oceans</code> keyword from the NASA GCMD Keyword vocabulary, <code>ice core studies</code> from  SnowTerm, and <code>Baked Clay</code> from EarthRef controlled vocabulary.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Dataset shell for example DefinedTerm keywords\",\n  \"keywords\": [\n    {\n      \"@type\": \"DefinedTerm\",\n      \"name\": \"OCEANS\",\n      \"inDefinedTermSet\": \"https://gcmd.earthdata.nasa.gov/kms/concepts/concept_scheme/sciencekeywords\",\n      \"url\": \"https://gcmd.earthdata.nasa.gov/kms/concept/91697b7d-8f2b-4954-850e-61d5f61c867d\",\n      \"termCode\": \"91697b7d-8f2b-4954-850e-61d5f61c867d\"\n    },\n    {\n      \"@type\": \"DefinedTerm\",\n      \"name\": \"ice core studies\",\n      \"inDefinedTermSet\": \"https://vocabularyserver.com/cnr/ml/snowterm/en/\",\n      \"url\": \"https://vocabularyserver.com/cnr/ml/snowterm/en/index.php?tema=29330\",\n      \"identifier\": {\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/ark\",\n        \"value\": \"ark:/99152/t3v4yo3eeqepj0\",\n        \"url\": \"https://vocabularyserver.com/cnr/ml/snowterm/en/?ark=ark:/99152/t3v4yo3eeqepj0\"\n      }\n    },\n    {\n      \"@type\": \"DefinedTerm\",\n      \"name\": \"Baked Clay\",\n      \"inDefinedTermSet\": \"https://www2.earthref.org/vocabularies/controlled\"\n    }\n  ]\n}\n</pre>"},{"location":"sos/guides/Dataset/#identifier","title":"Identifier","text":"<p>Adding the schema:identifier field can be done in three ways - a text description, a URL, or by using the schema:PropertyValue field.</p> <p></p> <p>We highly recommend using schema:PropertyValue.</p> <p>Q: Why are simple text or URLs not good enough? A: Identifiers have multiple properties that are useful when trying to find them across the web.</p> <p>Most identifiers have these properties:</p> <ul> <li>a value,</li> <li>a domain or scheme (in which the value is guaranteed to be unique),</li> <li>(optionally) a resolvable URL (where the thing being identified can be found),</li> <li>(optionally) a domain prefix (a token string of characters succeeded by a colon ':' that represents the domain or scheme).</li> </ul> <p>For example, the Digital Object Identifier (DOI) for a dataset may be: doi:10.5066/F7VX0DMQ. To break it down into its properties, we arrive at:</p> <ul> <li>value: <code>10.5066/F7VX0DMQ</code></li> <li>scheme: <code>Digital Object Identifier (DOI)</code></li> <li>url: <code>https://doi.org/10.5066/F7VX0DMQ</code></li> <li>prefix: <code>doi</code></li> </ul> <p>Q: Can't we just say the scheme is a 'DOI'? A: Yes, but there's a better way - a URI or URL. Because the we are publishing schema.org to express the explicit values of our content, we want to explicitly identify and classify our content such that harvesters can determine when our content appears elsewhere on the web. By detecting these shared pieces of content, we form the Web of Data.</p> <p>Because the scheme <code>Digital Object Identifier (DOI)</code> is described using unstructured text, we need a better way to explicitly state this value. Fortunately, identifiers.org has registered URIs for almost 700 different identifier schemes which can be browsed at: https://registry.identifiers.org/registry.</p> <p>We can specify the scheme as being a DOI with this identifiers.org Registry URI:</p> <p>https://registry.identifiers.org/registry/doi</p> <p>Looking at the available fields from schema:PropertyValue, we can map our identifier fields as follows:</p> <ul> <li><code>schema:value</code> as the identifier value <code>10.5066/F7VX0DMQ</code></li> <li><code>schema:propertyID</code> is the registry.identifiers.org URI for the identifier scheme <code>https://registry.identifiers.org/registry/doi</code>,</li> <li><code>schema:url</code> is the resolvable url for that identifier <code>https://doi.org/10.5066/F7VX0DMQ</code>.</li> </ul> <p>Q: Where should the prefix go? A: There is no ideal property for the prefix. But, we may include it as part of the <code>schema:value</code>.</p> <p>Q: Why include <code>doi:</code> as part of the value? Doesn't the URL <code>https://doi.org/10.5066/F7VX0DMQ</code> acheive the same result? A: While the actual value of the DOI is <code>10.5066/F7VX0DMQ</code>, we felt that this representation helps schema.org publishers specify an identifier value that is familiar to the research community. For example, in most citation styles such as APA, the DOI 10.5066/F7VX0DMQ is cited as <code>doi:10.5066/F7VX0DMQ</code>. Also, there can be many proper URLs for a specific identifier:</p> <ul> <li>http://doi.org/10.5066/F7VX0DMQ</li> <li>https://doi.org/10.5066/F7VX0DMQ</li> <li>http://dx.doi.org/10.5066/F7VX0DMQ</li> <li>https://dx.doi.org/10.5066/F7VX0DMQ</li> <li>https://www.sciencebase.gov/catalog/item/56b3e649e4b0cc79997fb5ec</li> </ul> <p>For these reasons, we recommend that any identifier having a known prefix value should be included in the value succeeded by a colon to form ':', or for this DOI: <code>doi:10.5066/F7VX0DMQ</code>. <p>Q: How do I know if an Identifier has a known prefix? A: Each Identifier in the identifiers.org Registry that has a known prefix will be specified on the identifers.org registry page under the section called 'Identifier Schemes' at the field labeled 'Prefix'.</p> <p>An example of using schema:PropertyValue to describe an Identifier:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"sameAs\": \"https://search.dataone.org/#view/https://www.sample-data-repository.org/dataset/472032\",\n  \"version\": \"2013-11-21\",\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"],\n  \"identifier\":\n      {\n        \"@id\": \"https://doi.org/10.5066/F7VX0DMQ\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n        \"value\": \"doi:10.5066/F7VX0DMQ\",\n        \"url\": \"https://doi.org/10.5066/F7VX0DMQ\"\n      }\n}\n</pre> <p>Optionally, the <code>schema:name</code> field can be used to give this specific identifier a label such as \"DOI: 10.5066/F7VX0DMQ\" or \"DOI 10.5066/F7VX0DMQ\", but <code>schema:name</code> should never be used to simply say \"DOI\".</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"identifier\":\n      {\n        \"@id\": \"https://doi.org/10.5066/F7VX0DMQ\",\n        \"@type\": \"PropertyValue\",\n        \"name\": \"DOI: 10.5066/F7VX0DMQ\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n        \"value\": \"doi:10.5066/F7VX0DMQ\",\n        \"url\": \"https://doi.org/10.5066/F7VX0DMQ\"\n      }\n}\n</pre> <p>For more examples of using <code>schema:PropertyValue</code> for identifiers other than DOIs:</p> <ul> <li>ARK: https://registry.identifiers.org/registry/ark</li> <li>PubMed: https://registry.identifiers.org/registry/pubmed</li> <li>PaleoDB: https://registry.identifiers.org/registry/paleodb</li> <li>Protein Data Bank: https://registry.identifiers.org/registry/pdb</li> </ul> <pre>\n\"identifier\": [\n    {\n        \"@id\": \"https://n2t.net/ark:13030/c7833mx7t\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/ark\",\n        \"name\": \"ARK: 13030/c7833mx7t\",\n        \"value\": \"ark:13030/c7833mx7t\",\n        \"url\": \"https://n2t.net/ark:13030/c7833mx7t\"\n    },\n    {\n        \"@id\": \"http://www.ncbi.nlm.nih.gov/pubmed/16333295\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/pubmed\",\n        \"name\": \"Pubmed ID #16333295\",\n        \"value\": \"pubmed:16333295\",\n        \"url\": \"http://www.ncbi.nlm.nih.gov/pubmed/16333295\"\n    },\n    {\n        \"@id\": \"https://identifiers.org/paleodb:83088\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/paleodb\",\n        \"name\": \"Paleo Database ID #83088\",\n        \"value\": \"paleodb:83088\",\n        \"url\": \"https://identifiers.org/paleodb:83088\"\n    },\n    {\n        \"@id\": \"https://identifiers.org/pdb:2gc4\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/pdb\",\n        \"name\": \"Protein Data Bank 2gc4\",\n        \"value\": \"pdb:2gc4\",\n        \"url\": \"https://identifiers.org/pdb:2gc4\"\n    }\n]\n</pre> <p>While we strongly recommend using a schema:PropertyValue, in it's most basic form, the <code>schema:identifier</code> as text can be published as:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"sameAs\": \"https://search.dataone.org/#view/https://www.sample-data-repository.org/dataset/472032\",\n  \"version\": \"2013-11-21\",\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"],\n  \"identifier\": \"urn:sdro:dataset:472032\"\n}\n</pre> <p>Or as a URL:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"identifier\": \"http://id.sampledatarepository.org/dataset/472032/version/1\"\n}\n</pre> <p>However, if the identifier is a persistent identifier such as a DOI, ARK, or accession number, then the best way to represent these identifiers is by using a schema:PropertyValue. The PropertyValue allows for more information about the identifier to be represented such as the identifier type or scheme, the identifier's value, it's URL and more. Because of this flexibility, we recommend using PropertyValue for all identifier types.</p> <p>schema:Dataset also defines a field for the schema:citation as either text or a schema:CreativeWork. To provide citation text:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"sameAs\": \"https://search.dataone.org/#view/https://www.sample-data-repository.org/dataset/472032\",\n  \"version\": \"2013-11-21\",\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"],\n  \"identifier\": {\n    \"@id\": \"https://doi.org/10.5066/F7VX0DMQ\",\n    \"@type\": \"PropertyValue\",\n    \"name\": \"DOI: 10.5066/F7VX0DMQ\",\n    \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n    \"value\": \"doi:10.5066/F7VX0DMQ\",\n    \"url\": \"https://doi.org/10.5066/F7VX0DMQ\"\n  },\n  \"citation\": \"J.Smith 'How I created an awesome dataset\u2019, Journal of Data Science, 1966\"\n}\n</pre> <p>NOTE: If you have a DOI, the citation text can be automatically generated for you by querying a DOI URL with the Accept Header of 'text/x-bibliography'.</p>"},{"location":"sos/guides/Dataset/#how-to-reference-short-dois","title":"How to reference Short DOIs","text":"<p>Short DOI is a redirect service offered by the International DOI Foundation that provides a shorter version of an orginial DOI. For example, the original DOI <code>doi:10.5066/F7VX0DMQ</code> has a short DOI of <code>doi.org/csgf</code>. Short DOIs are resolvable using standard DOI URLS such as <code>http://doi.org/fg5v</code>. These short DOIs are treated identically to the original DOI. If you are using the short DOI service, we recommend publishing a short DOI URL using the <code>schema:sameAs</code> property of the <code>schema:Dataset</code>:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"description\": \"This dataset includes results of laboratory experiments which measured dissolved organic carbon (DOC) usage by natural bacteria in seawater at different pCO2 levels. Included in this dataset are; bacterial abundance, total organic carbon (TOC), what DOC was added to the experiment, target pCO2 level. \",\n  \"url\": \"https://www.sample-data-repository.org/dataset/472032\",\n  \"sameAs\": [\n    \"https://search.dataone.org/#view/https://www.sample-data-repository.org/dataset/472032\",\n    \"http://doi.org/fg5v\"\n  ],\n  \"version\": \"2013-11-21\",\n  \"keywords\": [\"ocean acidification\", \"Dissolved Organic Carbon\", \"bacterioplankton respiration\", \"pCO2\", \"carbon dioxide\", \"oceans\"],\n  \"identifier\":\n      {\n        \"@id\": \"https://doi.org/10.5066/F7VX0DMQ\",\n        \"@type\": \"PropertyValue\",\n        \"propertyID\": \"https://registry.identifiers.org/registry/doi\",\n        \"value\": \"doi:10.5066/F7VX0DMQ\",\n        \"url\": \"https://doi.org/10.5066/F7VX0DMQ\"\n      }\n}\n</pre> <p><code>schema:sameAs</code> is used here for the following reasons:</p> <ol> <li>It doesn't add too many more statements that might increase the page weight (which may impact major search engine crawlers stopping the crawl of schema.org markup).</li> <li>Crawlers that follow the URL for the short DOI can retrieve structured metadata for the DOI itself:</li> </ol> <p><code>curl --location --request GET \"http://doi.org/fg5v\" --header \"Accept: application/ld+json\"</code></p> <p>Back to top </p>"},{"location":"sos/guides/Dataset/#variables","title":"Variables","text":"<p>A Dataset is a collection of data entities, each of which contains structured and unstructured values for a set of properties about that entity. For example, an hypothetical Dataset might contain three data files: 1) a data table in CSV format containing columns of data that both classify and measure the properties of a set of lakes in a region; 2) an image file containing rasterized geospatial data values for each location for properties like water temperature at multiple depths; and, 3) a text file containing responses to a survey assessing perspectives on water rights, with values for questions containing both natural language responses and responses on a Likert scale. In each of these examples, we are recording the value of attributes (aka properties) about an entity of interest (lake). In schema.org, details about these attributes can be recorded using <code>schema:variableMeasured</code>. So, while schema.org uses the term \"variable\" and the term \"measured\", it is usually conceptualized as a listing of any of the properties or attributes of an entity that are recorded, and not strictly a measured variable. Thus, we recommend using <code>schema:variableMeasured</code> to represent any recordable property of an entity that is found in the dataset. While this includes quantitatively \"measured\" observations (e.g., rainfaill in mm), it also includes classification values that are asserted or qualitatively assigned (e.g., \"moderate velocity\"), contextual attributes such as spatial locations, times, or sampling information associated with a value, and textual values such as narrative text.</p> <p>Information about the variables/attributes in a dataset can enhance discovery and support evaluation of the data. This can be done using the schema:variableMeasured field. Schema.org allows the value of variableMeasured to be a simple text string, but it is strongly recommended to use the schema:PropertyValue type to describe the variable in more detail.</p> <p></p> <p>This recommendation outlines several tiers of variable description. Tier 1 is the simplest, with other tiers adding recommendations for additional content (Tier 2 and 3).   See Experimental Recommendations, for proposed recommendations to document variables with with non-numeric or enumerated (controlled vocabulary) values, variables whose values are structured objects (e.g. json objects, arrays, gridded data), or are references to external value representations.</p>"},{"location":"sos/guides/Dataset/#tier-1-simple-list-of-variable-names","title":"Tier 1. Simple list of variable names","text":"<p>The simplest approach is to provide a <code>schema:name</code> and a textual description of the variable. The <code>schema:name</code> should match the label associated with the variable in the dataset serialization (e.g. the column name in a CSV file). If the variable name in the dataset does not clearly convey the variable concept, a more human-intelligible name can be provide using <code>schema:alternateName</code>. The field <code>schema:description</code> is used to provide a definition of the variable/property/attribute that allows others to correctly understand and interpret the values.</p> <p>Example:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities ...\",\n  ...\n  \"variableMeasured\": [\n    {\n      \"@type\": \"PropertyValue\",\n      \"name\": \"latdd\",\n      \"alternateName\":\"latitude, decimal degrees\",\n      \"description\": \"Latitude where water samples were collected ...\",\n    },\n    ...\n  ]\n}\n</pre>"},{"location":"sos/guides/Dataset/#tier-2-names-of-variables-with-formal-property-types","title":"Tier 2: Names of variables with formal property types","text":"<p>In Tier 2, we recommend using a <code>schema:PropertyValue</code> object to provide a schema:propertyID that better defines the semantics of the variable than plain text can. This <code>schema:propertyID</code> should be a URI that resolves to a web page providing a human-friendly description of the variable and, ideally, this identifier should also be resolvable to obtain an RDF representation using a documented vocabulary for machine consumption, for example a sosa:Observation or DDI represented variable. Describing the variables with machine understandable vocabularies is necessary if you want your data to be interoperable with other data, i.e., to be more FAIR.  The property can be identified at any level of specificity, depending on what the data provider can determine about the interpretation of the variable. For example, one might use a propertyID for the property 'temperature', or use a more specific property like 'water temperature', 'sea surface water temperature', or 'sea surface water temperature measured with protocol X, daily average, Kelvins, xsd:decimal'. If there are choices, the most specific property identifier should be used.</p> <p>Example:</p> <pre>\n{\n  \"@context\": {\n    \"@vocab\": \"https://schema.org/\"\n  },\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities ...\",\n  ...\n  \"variableMeasured\": [\n    {\n      \"@type\": \"PropertyValue\",\n      \"name\": \"latdd\",\n      \"alternateName\":\"latitude, decimal degrees\",\n      \"propertyID\":\"http://purl.obolibrary.org/obo/NCIT_C68642\",\n      \"description\": \"Latitude where water samples were collected ...\",\n    },\n    ...\n  ]\n}\n</pre>"},{"location":"sos/guides/Dataset/#tier-3-numeric-values","title":"Tier 3: Numeric values","text":"<p>For variables with numeric measured values, other properties of schema:PropertyValue can add additional useful information:</p> <ul> <li>schema:unitText. A string that identifies a unit of measurement that applies to all values for this variable.</li> <li>schema:unitCode. Value is expected to be TEXT or URL. We recommend providing an HTTP URI that identifies a unit of measure from a vocabulary accessible on the web.  The QUDT unit vocabulary provides an extensive set of registered units of measure that can be used to populate the schema:unitCode property to specify the units of measure used to report data values when that is appropriate.</li> <li>schema:minValue. If the value for the variable is numeric, this is the minimum value that occurs in the dataset. Not useful for other value types.</li> <li>schema:maxValue. If the value for the variable is numeric, this is the maximum value that occurs in the dataset. Not useful for other value types.</li> <li>schema:measurementTechnique. A text description of the measurement method used to determine values for this variable. If standard measurement protocols are defined and registered, these can be identified via http URI's.</li> <li>schema:url Any schema:Thing can have a URL property, but because the value is simply a url the relationship of the linked resource can not be expressed.  Usage is optional. The recommendation is that <code>schema:url</code> should link to a web page that would be useful for a person to interpret the variable, but is not intended to be machine-actionable.</li> </ul> <p>Example:</p> <pre>\n{\n  \"@context\": {\n    \"@vocab\": \"https://schema.org/\"\n  },\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n\n  \"variableMeasured\": [\n    {\n      \"@type\": \"PropertyValue\",\n      \"name\": \"latitude\",\n      \"propertyID\":\"http://purl.obolibrary.org/obo/NCIT_C68642\",\n      \"url\": \"https://www.sample-data-repository.org/dataset-parameter/665787\",\n      \"description\": \"Latitude where water samples were collected; north is positive. Latitude is a geographic coordinate which refers to the angle from a point on the Earth's surface to the equatorial plane\",\n      \"unitText\": \"decimal degrees\",\n      \"unitCode\":\"http://qudt.org/vocab/unit/DEG\",\n      \"minValue\": \"45.0\",\n      \"maxValue\": \"15.0\"\n    },\n    ...\n  ]\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#collections-of-datasets-using-schemaorg-datacatalog","title":"Collections of datasets using schema.org DataCatalog","text":"<p>For some repositories, data collections containing multiple data sets are used to help contextualize or organize the data. In schema.org, you define these collections using schema:DataCatalog.</p> <p></p> <p>The best way to use these DataCatalogs is to define these catalogs as an \"offering\" of your repository and including the offering <code>@id</code> in the dataset JSON-LD. For example, the repository JSON-LD defines a schema:DataCatalog with the</p> <p><code>\"@id\": \"https://www.sample-data-repository.org/collection/biological-data\"</code>.</p> <p>In the dataset JSON-LD, we reuse that <code>@id</code> to say a dataset belongs in that DataCatalog:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"includedInDataCatalog\": {\n    \"@id\": \"https://www.sample-data-repository.org/collection/biological-data\",\n    \"@type\": \"DataCatalog\"\n  }\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#metadata","title":"Metadata","text":"<p>While this schema.org record represents metadata about a Dataset, many providers will also have other metadata records that may be more complete or that conform to other metadata formats and vocabularies that might be useful. For example, repositories often contain detailed records in ISO TC 211 formats, EML, and other formats. Aggregators and other consumers can make use of this additional metadata if they are linked in a standardized way to the schema.org record.  We recommend that the location of the alternative forms of the metadata be provided using the schema:subjectOf and schema:about properties:</p> <p>Link metadata documents to a schema:Dataset by using schema:subjectOf.     - Or if a schema.org snippet describes the metadata as the main resource, then link to the Dataset it describes using schema:about.</p> <p>These two approaches are equivalent, and which is used depends on the subject of the schema.org record.</p> <p></p> <p>Once the linkage has been made, further details about the metadata can be provided. We recommend using schema:encodingFormat to indicate the metadata format/vocabulary to which the metadata record conforms.  If it conforms to multiple formats, or to a specific and general format types, multiple types can be listed. We use the schema:DataDownload class for Metadata files so that we can use the schema:MediaObject properties for describing bytesize, encoding, etc.</p> <p>It can be useful to aggregators and other consumers to indicate when the metadata record was last modified using <code>schema:dateModified</code>, which can be used to optimize harvesting schedules for search indices and other applications.</p> <p>An example of a metadata reference to an instance of EML-formatted structured metadata, embedded within a <code>schema:Dataset</code> record follows:</p> <pre>\n  {\n    \"@context\": \"https://schema.org/\",\n    \"@type\": \"Dataset\",\n    \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n    \"distribution\": {\n      \"@type\": \"DataDownload\",\n      ...\n    },\n    \"subjectOf\": {\n      \"@type\": \"DataDownload\",\n      \"name\": \"EML metadata for dataset\",\n      \"description\": \"EML metadata describing the dataset\",\n      \"encodingFormat\": [\"application/xml\", \"https://eml.ecoinformatics.org/eml-2.2.0\"],\n      \"contentURL\":\"https://example.com/metadata/eml-metadata.xml\",\n      \"dateModified\":\"2019-06-12T14:44:15Z\"\n    }\n  }\n</pre> <p>Alternatively, if the schema.org record is meant to describe the metadata record, one could use the inverse property <code>schema:about</code> to indicate the linkage back to the Dataset that it describes.  This should be a rare situation, as typically the schema.org record will describe the Dataset itself.</p> <p>Note that the <code>encodingFormat</code> property contains an array of formats to describe multiple formats to which the document conforms (in this example, the document is both conformant with XML and the EML metadata dialect).</p> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#distributions","title":"Distributions","text":"<p>While the schema:url property of the Dataset should point to a landing page, the way to describe how to download the data is through the schema:distribution property. The \"distribution\" property describes where to get the data and in what format by using the schema:DataDownload type. If your dataset is not accessible through a direct download URL, but rather through a service URL that may need input parameters jump to the next section Accessing Data through a Service Endpoint.</p> <p></p> <p>For data available in multiple formats, there will be multiple values of the schema:DataDownload:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"distribution\": {\n    \"@type\": \"DataDownload\",\n    \"contentUrl\": \"https://www.sample-data-repository.org/dataset/472032.tsv\",\n    \"encodingFormat\": \"text/tab-separated-values\"\n  }\n}\n</pre>"},{"location":"sos/guides/Dataset/#accessing-data-through-a-service-endpoint","title":"Accessing Data through a Service Endpoint","text":"<p>If access to the data requires some input parameters before a download can occur, we can use the schema:potentialAction in this way:</p> <p></p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"potentialAction\": {\n    \"@type\": \"SearchAction\",\n    \"target\": {\n        \"@type\": \"EntryPoint\",\n        \"contentType\": [\"application/x-netcdf\", \"text/tab-separated-values\"],\n        \"urlTemplate\": \"https://www.sample-data-repository.org/dataset/1234/download?format={format}&amp;startDateTime={start}&amp;endDateTime={end}&amp;bounds={bbox}\",\n        \"description\": \"Download dataset 1234 based on the requested format, start/end dates and bounding box\",\n        \"httpMethod\": [\"GET\", \"POST\"]\n    },\n    \"query-input\": [\n      {\n        \"@type\": \"PropertyValueSpecification\",\n        \"valueName\": \"format\",\n        \"description\": \"The desired format requested either 'application/x-netcdf' or 'text/tab-separated-values'\",\n        \"valueRequired\": true,\n        \"defaultValue\": \"application/x-netcdf\",\n        \"valuePattern\": \"(application\\/x-netcdf|text\\/tab-separated-values)\"\n      },\n      {\n        \"@type\": \"PropertyValueSpecification\",\n        \"valueName\": \"start\",\n        \"description\": \"A UTC ISO DateTime\",\n        \"valueRequired\": false,\n        \"valuePattern\": \"(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])T(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(.[0-9]+)?(Z)?\"\n      },\n      {\n        \"@type\": \"PropertyValueSpecification\",\n        \"valueName\": \"end\",\n        \"description\": \"A UTC ISO DateTime\",\n        \"valueRequired\": false,\n        \"valuePattern\": \"(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[1-9])-(3[01]|0[1-9]|[12][0-9])T(2[0-3]|[01][0-9]):([0-5][0-9]):([0-5][0-9])(.[0-9]+)?(Z)?\"\n      },\n      {\n        \"@type\": \"PropertyValueSpecification\",\n        \"valueName\": \"bbox\",\n        \"description\": \"Two points in decimal degrees that create a bounding box fomatted at 'lon,lat' of the lower-left corner and 'lon,lat' of the upper-right\",\n        \"valueRequired\": false,\n        \"valuePattern\": \"(-?[0-9]+(.[0-9]+)?),[ ]*(-?[0-9]+(.[0-9]+)?)[ ]*(-?[0-9]+(.[0-9]+)?),[ ]*(-?[0-9]+(.[0-9]+)?)\"\n      }\n    ]\n  }\n}\n</pre> <p>Here, we use the schema:SearchAction type becuase it lets you define the query parameters and HTTP methods so that machines can build user interfaces to collect those query parmaeters and actuate a request to provide the user what they are looking for.</p> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#dates","title":"Dates","text":"<p>Scientific datasets typically have multiple associated date or time periods.  Time periods can be specified for 1) the time at which an entity or phenonomon occurred or was measured, and 2) the time periods when a dataset containing that information was created, changed, published, etc. <code>temporalCoverage</code> describes the age of the sample and the other dates describe the data created from observations or process. For example, if one took a sample 200 ft down in an ice core, <code>temporalCoverage</code> would describe the period when that layer of ice was deposited in geologic time, while other date properties (dateCreated, dateModified, datePublished, and expired) would describe the dataset that was created by measuring and analyzing that ice core sample. The temporalCoverage might also be a range of ages. For example if the dataset was from a study of the whole ice core it could have a range of ages from 300 to 6000 years before present (BP).</p> <p>Schema.org offers various date properties that can be used to encode this information. We recommend use of the following fields for Dates:</p> <ul> <li> <p><code>schema:temporalCoverage</code> :: use to specify the time period(s) that the content applies to, i.e. the time the entity or phenomenon described in the dataset occurred. See details at temporalCoverage. <code>temporalCoverage</code> is usually prior to the date of data publication for observational data, and can be afterwards for models, simulations, and forecasts.</p> </li> <li> <p><code>schema:dateCreated</code> ::  use to specify the date the dataset was initially generated (e.g., when a sensor recorded a value, when a model was run, or when data processing was completed). This is typically fixed when the first dataset version is created.</p> </li> <li><code>schema:dateModified</code> :: use to specify the date the dataset was most recently updated or changed.</li> <li><code>schema:datePublished</code> :: use to specify the date when a dataset was made available to the public through a publication process.</li> <li><code>schema:expires</code> :: use to specify the date when the dataset expires and is no longer useful or available. If <code>datePublished</code> is when the dataset is made available, then 'expires' brackets the time the dataset is valid or recommended for use.</li> </ul> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#temporal-coverage","title":"Temporal Coverage","text":"<p>Temporal coverage is defined as \"the time period during which data was collected or observations were made; or a time period that an activity or collection is linked to intellectually or thematically (for example, 1997 to 1998; the 18th century)\" (ARDC RIF-CS). For documentation of Earth Science, Paleobiology or Paleontology datasets, we are interested in the second case-- the time period that data are linked to thematically.</p> <p>Temporal coverage is a difficult concept to cover across all the possible scenarios. Schema.org uses ISO 8601 time interval format to describe time intervals and time points, but doesn't provide capabilities for geologic time scales or dynamically generated data up to present time. We ask for your feedback on any temporal coverages you may have that don't currently fit into schema.org. You can follow similar issues on the schema.org GitHub issue queue. We hope that our examples of the use of OWL Time and temporal reference system (TRS) elements will help you describe the dates or ages of the entities in a dataset. We have also included examples that describe dataset's time uncertainties.    </p> <p></p> <p>To represent a single date and time:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"temporalCoverage\": \"2018-01-22T14:51:12+00:00\"\n}\n</pre> <p>Or a single date:</p> <pre>\n{\n  ...\n  \"temporalCoverage\": \"2018-01-22\"\n}\n</pre> <p>Or a date range:</p> <pre>\n{\n  ...\n  \"temporalCoverage\": \"2012-09-20/2016-01-22\"\n}\n</pre> <p>Or an open-ended date range (thanks to @lewismc for this example from NASA PO.DAAC) :</p> <pre>\n{\n  ...\n  \"temporalCoverage\": \"2012-09-20/..\"\n}\n</pre> <p>Schema.org also lets you provide date ranges and other temporal coverages through the DateTime data type and URL. For more granular temporal coverages go here: https://schema.org/DateTime.</p>"},{"location":"sos/guides/Dataset/#geologic-time","title":"Geologic Time","text":"<p>Dates or ages used for describing geological, archeological, and paleontological samples range from the very simple to highly complex. A lava rock age could be simply described as 1.23 million years. Other ages are more descriptive. Some other examples are: a zircon crystal with an age of 456.4 +/- 1.4  billion years (Ga) at a standard error of 2-sigma, a core with rocks from the Triassic to the Jurassic, a carbon date of a bone with non-symmetrical uncertainties of 3242 (+160 -40) B.C. We make use of the OWL time (Cox and Little) descriptive tags (elements), the Queensland Department of Natural Resources, Mines and Energy Temporal Reference Systems (TRS), and geoschemas' properties to describe ages and age ranges in detail. These methods could also be used to describe the temporal coverage for other disciplines as well. </p> <p>There are two main types of geologic time: Proper Intervals and Instants. They are diagrammed below and used in the examples that follow.</p> <p>Proper Interval</p> <p></p> <p>Instant</p> <p></p> <p>These examples can be found in one JSON-LD file at temporalCoverage.jsonld</p> <ol> <li>The dataset's temporalCoverage is described using ProperInterval, hasBeginning, and hasEnd elements from OWL Time. The human readable description can be found in the description field: \"Eruptive activity at Mt. St. Helens, Washington, March 1980 - January 1981\". </li> </ol> <p>Example:</p> <pre>\n{    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n    },\n\n    \"@type\": \"Dataset\",\n        \"description\": \"Eruptive activity at Mt. St. Helens, Washington, March 1980 - January 1981\",\n        \"temporalCoverage\": [\n            {\n                \"@type\": \"time:ProperInterval\",\n                \"time:hasBeginning\": {\n                     \"@type\": \"time:Instant\",\n                     \"time:inXSDDateTimeStamp\": \"1980-03-27T19:36:00Z\"\n                 },\n                 \"time:hasEnd\": {\n                    \"@type\": \"time:Instant\",\n                    \"time:inXSDDateTimeStamp\": \"1981-01-03T00:00:00Z\"\n                 }\n            }]\n}\n</pre> <ol> <li>The dataset's temporalCoverage is described using Instant, inTimePosition, hasTRS, and numericPosition elements for a single geological date/age without uncertainties from OWL Time.  Use a decimal value with appropriate timescale temporal reference system (TRS) and date/age unit abbreviation. The human readable description can be found in the description field: \"Eruption of Bishop Tuff, about 760,000 years ago\". </li> </ol> <p>Example:</p> <pre>\n{    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"gsqtime\": \"https://vocabs.gsq.digital/object?uri=http://linked.data.gov.au/def/trs\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"xsd\": \"https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Eruption of Bishop Tuff, about 760,000 years ago\",\n    \"temporalCoverage\": [\n        {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                \"@type\": \"time:TimePosition\",\n                \"time:hasTRS\": {\n                    \"@id\": \"gsqtime:MillionsOfYearsAgo\"\n                },\n                \"time:numericPosition\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 0.76\n                },\n                \"gstime:geologicTimeUnitAbbreviation\": {\n                    \"@type\": \"xsd:string\",\n                    \"value\": \"Ma\"\n                }\n            }\n        }\n    ]\n}\n</pre> <ol> <li>The dataset's temporalCoverage is described using the Instant, inTimePosition, TimePosition, numericPosition from OWL Time with a geological date/age with uncertainties. Use a decimal value with appropriate timescale temporal reference system(TRS), date/age unit abbreviation, the uncertainty value and specify at what sigma. The human readable description can be found in the description field: \"Very old zircons from the Jack Hills formation Australia 4.404 +- 0.008 Ga (2-sigma)\". </li> </ol> <p>Example:</p> <pre>\n{    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"gsqtime\": \"https://vocabs.gsq.digital/object?uri=http://linked.data.gov.au/def/trs\",\n        \"gstime\": \"http://schema.geoschemas.org/contexts/temporal#\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"xsd\": \"https://www.w3.org/TR/200 (from [OWL Time](http://www.w3.org/2006/time))4/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Very old zircons from the Jack Hills formation Australia 4.404 +- 0.008 Ga (2-sigma)\",\n    \"temporalCoverage\": [\n        {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                \"@type\": \"time:TimePosition\",\n                \"time:hasTRS\": {\n                    \"@id\": \"gsqtime:BillionsOfYearsAgo\"\n                },\n                \"time:numericPosition\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 4.404\n                },\n                \"gstime:geologicTimeUnitAbbreviation\": {\n                    \"@type\": \"xsd:string\",\n                    \"value\": \"Ma\"\n                },\n                \"gstime:uncertainty\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 0.008\n                },\n                \"gstime:uncertaintySigma\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 2.0\n                }\n            }\n        }\n    ]\n}\n</pre> <ol> <li>The dataset's temporalCoverage is described using the ProperInterval, hasBeginning, hasEnd, Instant, inTimePosition, TimePosition, and hasTRS elements from OWL Time with a geological date/age range with uncertainties. Use a decimal value with appropriate timescale temporal reference system(TRS), date/age unit abbreviation, uncertainty value and at what sigma. The human readable description can be found in the description field: \"Isotopic ages determined at the bottom and top of a stratigraphic section in the Columbia River Basalts\". </li> </ol> <p>Example:</p> <pre>\n{\n    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"gsqtime\": \"https://vocabs.gsq.digital/object?uri=http://linked.data.gov.au/def/trs\",\n        \"gstime\": \"http://schema.geoschemas.org/contexts/temporal#\",\n        \"rdfs\": \"https://www.w3.org/2001/sw/RDFCore/Schema/200212/\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"xsd\": \"https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Isotopic ages determined at the bottom and top of a stratigraphic section in the Columbia River Basalts\",\n    \"temporalCoverage\": [\n        {\n            \"@type\": \"time:ProperInterval\",\n            \"time:hasBeginning\": {\n                \"@type\": \"time:Instant\",\n                 \"time:inTimePosition\": {\n                    \"@type\": \"time:TimePosition\",\n                    \"rdfs:comment\": \"beginning is older bound of age envelop\",\n                    \"time:hasTRS\": {\n                        \"@id\": \"gsqtime:MillionsOfYearsAgo\"\n                    },\n                    \"time:numericPosition\": {\n                        \"@type\": \"xsd:decimal\",\n                        \"value\": 17.1\n                    },\n                    \"gstime:geologicTimeUnitAbbreviation\": {\n                        \"@type\": \"xsd:string\",\n                        \"value\": \"Ma\"\n                    }\n                },\n                \"gstime:uncertainty\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 0.15\n                },\n                \"gstime:uncertaintySigma\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 1.0\n                }\n            },\n            \"time:hasEnd\": {\n                \"@type\": \"time:Instant\",\n                \"time:inTimePosition\": {\n                    \"@type\": \"time:TimePosition\",\n                    \"rdfs:comment\": \"ending is younger bound of age envelop\",\n                    \"time:hasTRS\": {\n                        \"@id\": \"gsqtime:MillionsOfYearsAgo\"\n                    },\n                    \"time:numericPosition\": {\n                        \"@type\": \"xsd:decimal\",\n                        \"value\": 15.7\n                    },\n                    \"gstime:geologicTimeUnitAbbreviation\": {\n                        \"@type\": \"xsd:string\",\n                        \"value\": \"Ma\"\n                    }\n                },\n                \"gstime:uncertainty\": {\n                    \"@type\": \"xsd:decimal\",\n                    \"value\": 0.14\n                },\n                \"gstime:uncertaintySigma\": {\n                    \"@type\": \"xsd:decimal\",\n                     \"value\": 2.0\n                }\n            }\n        }\n    ]\n}\n</pre> <ol> <li>The dataset's temporalCoverage is described using the Instant, inTimePosition, TimePosition, and hasTRS elements from OWL Time with a archeological date/age range with uncertainties. Use a decimal value with appropriate timescale temporal reference system(TRS), date/age unit abbreviation, the older and younger uncertainty values and at what sigma. The human readable description can be found in the description field: \"Age of a piece of charcoal found in a burnt hut at an archeological site in Kenya carbon dated at BP Calibrated of 2640 +130 -80 (one-sigma) using the INTCAL20 carbon dating curve.\"</li> </ol> <p>Example:</p> <pre>\n{\n    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"gsqtime\": \"https://vocabs.gsq.digital/object?uri=http://linked.data.gov.au/def/trs\",\n        \"gstime\": \"http://schema.geoschemas.org/contexts/temporal#\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"xsd\": \"https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Age of a piece of charcoal found in a burnt hut at an archeological site in Kenya carbon dated at BP Calibrated of 2640 +130 -80 (one-sigma) using the INTCAL20 carbon dating curve.\",\n    \"temporalCoverage\": [\n        {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                 \"@type\": \"time:TimePosition\",\n                 \"time:hasTRS\": {\n                     \"@id\": \"gsqtime:BeforePresentCalibrated\"\n                 },\n                 \"time:numericPosition\": {\n                     \"@type\": \"xsd:decimal\",\n                     \"value\": 2460.0\n                 },\n                 \"gstime:geologicTimeUnitAbbreviation\": {\n                     \"@type\": \"xsd:string\",\n                     \"value\": \"BP-CAL\"\n                 },\n                 \"gstime:uncertaintyOlder\": {\n                     \"@type\": \"xsd:decimal\",\n                     \"value\": 130.0\n                 },\n                 \"gstime:uncertaintyYounger\": {\n                     \"@type\": \"xsd:decimal\",\n                     \"value\": 80.0\n                 },\n                 \"gstime:uncertaintySigma\": {\n                     \"@type\": \"xsd:decimal\",\n                     \"value\": 1.0\n                }\n            }\n        }\n    ]\n}\n</pre> <ol> <li>The dataset's temporalCoverage is described using the Instant, TimePosition, inTimePosition, NominalPosition, Interval, hasBeginning, hasEnd, and hasTRS elements from OWL Time. With temporal coverage that is a named time interval from a geologic time scale, provide numeric positions of the beginning and end for interoperability. Providing the numeric values is only critical, but still recommended, if the TRS for the nominalPosition is not the International Chronostratigraphic Chart. In this example the temporalCoverage is described in two ways: by the named interval Bartonian and by defining a time interval using numerical beginning and end values.</li> </ol> <p>Example:</p> <pre>\n{\n    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"gsqtime\": \"https://vocabs.gsq.digital/object?uri=http://linked.data.gov.au/def/trs\",\n        \"gstime\": \"http://schema.geoschemas.org/contexts/temporal#\",\n        \"icsc\": \"http://resource.geosciml.org/clashttps://vocabs.ardc.edu.au/repository/api/lda/csiro/international-chronostratigraphic-chart/geologic-time-scale-2020/resource?uri=http://resource.geosciml.org/classifier/ics/ischart/Boundariessifier/ics/ischart/\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"ts\": \"http://resource.geosciml.org/vocabulary/timescale/\",\n        \"xsd\": \"https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Temporal position expressed with a named time ordinal era from [International Chronostratigraphic Chart](https://stratigraphy.org/chart):\",\n    \"temporalCoverage\": [\n        {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                \"@type\": \"time:TimePosition\",\n                \"time:hasTRS\": {\n                    \"@id\": \"ts:gts2020\"\n                },\n                \"time:nominalPosition\": {\n                    \"@type\": \"xsd:anyURI\",\n                    \"value\": \"icsc:Bartonian\"\n                }\n            }\n        },\n        {\n            \"@type\": \"time:ProperInterval\",\n            \"time:hasBeginning\": {\n                \"@type\": \"time:Instant\",\n                \"time:inTimePosition\": {\n                    \"@type\": \"time:TimePosition\",\n                    \"time:hasTRS\": {\n                        \"@id\": \"gsqtime:MillionsOfYearAgo\"\n                    },\n                    \"time:numericPosition\": {\n                        \"@type\": \"xsd:decimal\",\n                        \"value\": 41.2\n                    },\n                    \"gstime:geologicTimeUnitAbbreviation\": {\n                        \"@type\": \"xsd:string\",\n                        \"value\": \"Ma\"\n                    }\n                }\n            },\n            \"time:hasEnd\": {\n                \"@type\": \"time:Instant\",\n                \"time:inTimePosition\": {\n                    \"@type\": \"time:TimePosition\",\n                    \"time:hasTRS\": {\n                        \"@id\": \"gsqtime:MillionsOfYearAgo\"\n                    },\n                    \"time:numericPosition\": {\n                        \"@type\": \"xsd:decimal\",\n                        \"value\": 37.71\n                    },\n                    \"gstime:geologicTimeUnitAbbreviation\": {\n                        \"@type\": \"xsd:string\",\n                        \"value\": \"Ma\"\n                    }\n                }\n            }\n        }\n    ]\n}\n</pre> <ol> <li>Temporal intervals with nominal temporal position that have identifiers. When possible, use temporal intervals defined by the International Chronostratigraphic Chart, access via ARDC vocabulary service, or via GeoSciML vocabularies landing page. If temporal intervals with identifies from other schemes are available, they can be included in a separate time:ProperInterval or time:Instant element.  If intervals are not from the ICS chart it is recommended to provide an interval with beginning and end numeric positions for better interoperability.</li> </ol> <p>Example:</p> <pre>\n{\n    \"@context\": {\n        \"@vocab\": \"http://schema.org/\",\n        \"icsc\": \"http://resource.geosciml.org/clashttps://vocabs.ardc.edu.au/repository/api/lda/csiro/international-chronostratigraphic-chart/geologic-time-scale-2020/resource?uri=http://resource.geosciml.org/classifier/ics/ischart/Boundariessifier/ics/ischart/\",\n        \"time\": \"http://www.w3.org/2006/time#\",\n        \"ts\": \"http://resource.geosciml.org/vocabulary/timescale/\",\n        \"xsd\": \"https://www.w3.org/TR/2004/REC-xmlschema-2-20041028/datatypes.html\"\n    },\n    \"@type\": \"Dataset\",\n    \"description\": \"Temporal position expressed with an interval bounded by named time ordinal eras from [International Chronostratigraphic Chart](https://stratigraphy.org/chart). NumericPositions not included, expect clients can lookup bounds for ISC nominal positions:\",\n    \"temporalCoverage\": [{\n        \"@type\": \"time:ProperInterval\",\n        \"time:hasBeginning\": {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                \"@type\": \"time:TimePosition\",\n                \"time:hasTRS\": {\"@id\": \"ts:gts2020\"},\n                \"time:nominalPosition\": { \"@value\": \"icsc:Triassic\", \"@type\": \"xsd:anyURI\" }\n            }\n        },\n        \"time:hasEnd\": {\n            \"@type\": \"time:Instant\",\n            \"time:inTimePosition\": {\n                \"@type\": \"time:TimePosition\",\n                \"time:hasTRS\": {\"@id\": \"ts:gts2020\"},\n                \"time:nominalPosition\": { \"@value\": \"icsc:Jurassic\", \"@type\": \"xsd:anyURI\" }\n            }\n        }\n    }]\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#spatial-coverage","title":"Spatial Coverage","text":"<p>Used to document the location on Earth that is the focus of the  dataset content, using  schema:Place. Recommended practice is to use the schema:geo property with either a schema:GeoCoordinates object to specify a point location, or a schema:GeoShape object to specify a line or area coverage extent. Coordinates describing these extents are expressed as latitude longitude tuples (in that order) using decimal degrees.</p> <p>Schema.org documentation does not specify a convention for the coordinate reference system, our recommended practice is to use WGS84 for at least one spatial coverage description if applicable. Spatial coverage location using other coordinate systems can be included, see recommendation for specifying coordinate reference systems, below.  </p>"},{"location":"sos/guides/Dataset/#use-geocoordinates-for-point-locations","title":"Use GeoCoordinates for Point locations","text":"<p>Please indicate a point location by using a schema:GeoCoordinates object with schema:latitude and schema:longitude properties.</p> <p>Not Recommended The schema:Place definition allows the latitude and longitude of a point location to be specified directly; although this is more succinct, it makes parsing the metadata more complex and should be avoided.</p> <p>Point locations are recommended for data that is associated with specific sample locations, particularly if these are widely spaced such that an enclosing bounding box would be a misleading representation of the spatial location. Be aware that some client applications might only index or display bounding box extents or a single point location.</p> <p> A schema:Dataset that is about a point location would documented in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton ....\",\n  ...\n  \"spatialCoverage\": {\n    \"@type\": \"Place\",\n    \"geo\": {\n      \"@type\": \"GeoCoordinates\",\n      \"latitude\": 39.3280\n      \"longitude\": 120.1633\n    }\n  }\n}\n</pre>"},{"location":"sos/guides/Dataset/#use-geoshape-for-all-other-location-types","title":"Use GeoShape for all other location types","text":"<p>A schema:GeoShape can describe spatial coverage as a line (e.g. a ship track), a bounding box, a polygon, or a circle. The geometry is described with a set of latitude/longitude pairs. The spatial definitions were added to schema.org early in its development based on the GeoRSS specification. The documentation for schema:GeoShape states \"Either whitespace or commas can be used to separate latitude and longitude; whitespace should be used when writing a list of several such points.\" At least for bounding boxes (see the discussion below), it appears that the Google Dataset Search parsing of the coordinate strings depends on whether a comma or space is used to delimit the coordinates in an individual tuple.  </p> <p>Be aware that some client applications might only index or display bounding box extents.</p> <ul> <li>line - a series of two or more points.</li> <li>polygon - a series of four or more points where the first and final points are identical.</li> <li>box - A rectangular (in lat-long space) extent specified by two points, the first in the lower left (southwest) corner and the second in the upper right (northeast) corner.</li> <li>circle - A circular region of a specified radius centered at a specified latitude and longitude, represented as a coordinate pair followed by a radius in meters. Not recommended for use.</li> </ul> <p>Examples:</p>"},{"location":"sos/guides/Dataset/#linear-spatial-location","title":"Linear spatial location","text":"<p>Useful for data that were collected along a traverse, ship track, flight line or other linear sampling feature.</p> <pre>\n  \"spatialCoverage\": {\n    \"@type\": \"Place\",\n    \"geo\": {\n      \"@type\": \"GeoShape\",\n      \"line\": \"39.3280 120.1633 40.445 123.7878\"\n    }\n  }\n}\n</pre>"},{"location":"sos/guides/Dataset/#polygon-spatial-location","title":"Polygon spatial location","text":"<p>A polygon provides the most precise approach to delineating the spatial extent of the focus area for a dataset, but polygon spatial locations might not be recognized (indexed, displayed) by some client applications.</p> <pre>\n  \"polygon\": \"39.3280 120.1633 40.445 123.7878 41 121 39.77 122.42 39.3280 120.1633\"\n</pre>"},{"location":"sos/guides/Dataset/#bounding-boxes","title":"Bounding Boxes","text":"<p>A GeoShape box defines an area on the surface of the earth defined by point locations of the southwest corner and northeast corner of the rectangle in latitude-longitude coordinates. Point locations are tuples of {latitude  east-longitude} (y x). The schema.org GeoShape documentation states \"Either whitespace or commas can be used to separate latitude and longitude; whitespace should be used when writing a list of several such points.\" Since the box is a list of points, a space should be used to separate the latitude and longitude values. The two corner coordinate points are separated by a space. 'East longitude' means positive longitude values are east of the prime (Greenwich) meridian.  A box where 'lower-left' (southwest) corner is 39.3280/120.1633 and 'upper-right' (northeast) corner is 40.445/123.7878 would be encoded thus:</p> <pre>\n  \"box\": \"39.3280 120.1633 40.445 123.7878\"\n</pre> <p>NOTE-- see discussion in GitHub issue 101 on what works with Google Dataset search to display spatial locatation in their search results.</p> <p>East longitude values can be reported 0 &lt;= X &lt;= 360 or -180 &lt;= X &lt;= 180. Some applications will fail under one or the other of these conventions. Recommendation is to use -180 &lt;= X &lt;= 180, consistent with the WKT specification.  Following this recommendation, bounding boxes that cross the antimeridian at \u00b1180\u00b0 longitude, the West longitude value will be numerically greater than the East longitude value. For example, to describe Fiji the box might be</p> <pre>\n  \"box\": \"-19 176 -15 -178\"\n</pre> <p>NOTES: Some spatial data processors will not correctly interpret the bounding coordinates across the antimeridian even if they follow the recommended southwest, northeast corner convention, resulting in boxes that span the circumference of the Earth, excluding the actual area of interest. For applications operating with data in the vicinity of longitude 180, testing is strongly recommended to determine if it works for bounding boxes crossing the antimeridian (+/- 180); an alternative is to define two bounding boxes, one on each side of 180.</p> <p>For bounding boxes that include the north or south pole, schema:box will not work. Recommended practice is to use a schema:polygon to describe spatial location extents that include the poles.  </p>"},{"location":"sos/guides/Dataset/#handling-multiple-locations","title":"Handling multiple locations","text":"<p>If you have multiple geometries, you can publish those by making the schema:geo field an array of GeoShape or GeoCoordinates like so:</p> <pre>\n{\n  ...\n  \"spatialCoverage\": {\n    \"@type\": \"Place\",\n    \"geo\": [\n      {\n        \"@type\": \"GeoCoordinates\",\n        \"latitude\": -17.65,\n        \"longitude\": 50\n      },\n      {\n        \"@type\": \"GeoCoordinates\",\n        \"latitude\": -19,\n        \"longitude\": 51\n      },\n      ...\n    ]\n  }\n  ...\n}\n</pre> <p>Be aware that some client application might not index or display multiple geometries.</p>"},{"location":"sos/guides/Dataset/#spatial-reference-systems","title":"Spatial Reference Systems","text":"<p>A Spatial Reference System (SRS) or Coordinate Reference System (CRS) is the method for defining the frame of reference for geospatial location representation. Schema.org currently has no defined property for specifying a Spatial Reference System; the assumption is that coordinates are WGS84 decimal degrees.</p> <p>In the mean time, to represent an SRS in schema.org, we recommend using the schema:additionalProperty property to specify an object of type schema:PropertyValue, with a schema:propertyID of http://dbpedia.org/resource/Spatial_reference_system to identify the property as a spatial reference system, and the schema:PropertyValue/schema:value is a URI (IRI) that identifies a specific SRS. Some commonly used values are:</p> Spatial Reference System IRI WGS84 http://www.w3.org/2003/01/geo/wgs84_pos#lat_long CRS84 http://www.opengis.net/def/crs/OGC/1.3/CRS84 EPSG:26911 https://spatialreference.org/ref/epsg/nad83-utm-zone-11n/ EPSG:3413 https://spatialreference.org/ref/epsg/wgs-84-nsidc-sea-ice-polar-stereographic-north/ <p>NOTE: Beware of coordinate order differences. WGS84 in the table above specifies latitude, longitude coordinate order, whereas CRS84 specifies longitude, latitude order (like GeoJSON). WGS84 is the assumed typical value for coordinates, so in general the SRS does not need to be specified.</p> <p>A spatial reference system can be added in this way:</p> <pre>\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n        \"dbpedia\": \"http://dbpedia.org/resource/\"\n    }\n  ],\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"spatialCoverage\": {\n    \"@type\": \"Place\",\n    \"geo\": {\n      \"@type\": \"GeoShape\",\n      \"line\": \"39.3280 120.1633 40.445 123.7878\"\n    },\n    \"additionalProperty\": {\n      \"@type\": \"PropertyValue\",\n      \"propertyID\":\"http://dbpedia.org/resource/Spatial_reference_system\",\n      \"value\": \"http://www.w3.org/2003/01/geo/wgs84_pos#lat_long\"\n    }\n  }\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#roles-of-people","title":"Roles of People","text":"<p>People can be linked to datasets using three fields: author, creator, and contributor. Since  schema:contributor is defined as a secondary author, and schema:Creator is defined as being synonymous with the schema:author field, we recommend using the more expressive fields creator and contributor, but using any of these fields is acceptable.</p> <p>NOTE: JSON-LD doesn't preserve the order of its collection values, so if you need to preserve the order of people's names (e.g., for a citation) you can do so by applying the <code>@list</code> JSON-LD keyword (for more information about this see Getting Started - JSON-LD Lists). </p> <p>Given the following <code>creator</code> JSON-LD block,:</p> <pre><code>{\n  ...\n  \"creator:[\n    {\n        \"@type\": \"Person\",\n        \"name\": \"Creator #1\"\n    },\n    {\n        \"@type\": \"Person\",\n        \"name\": \"Creator #2\"\n    }\n  ]\n}</code></pre> <p>The order of these creators can be preserved by the using the <code>@list</code> JSON-LD keyword:</p> <pre><code>{\n  ...\n  \"creator:{\n    \"@list\": [\n      {\n          \"@type\": \"Person\",\n          \"name\": \"Creator #1\"\n      },\n      {\n          \"@type\": \"Person\",\n          \"name\": \"Creator #2\"\n      }\n    ]\n  }\n}</code></pre> <p>Because there are more things that can be said about how and when a person contributed to a Dataset, we use the schema:Role. You'll notice that the schema.org documentation does not state that the Role type is an expected data type of author, creator and contributor, but that is addressed in this blog post introducing Role into schema.org. Thanks to Stephen Richard for this contribution</p> <p></p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"creator\": [\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472036\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Principal Investigator\",\n      \"creator\": {\n        \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n        \"@type\": \"Person\",\n        \"name\": \"Dr Uta Passow\",\n        \"givenName\": \"Uta\",\n        \"familyName\": \"Passow\",\n        \"url\": \"https://www.sample-data-repository.org/person/51317\"\n      }\n    },\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472038\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Co-Principal Investigator\",\n      \"url\": \"https://www.sample-data-repository.org/person-role/472038\",\n      \"creator\": {\n        \"@id\": \"https://www.sample-data-repository.org/person/50663\",\n        \"@type\": \"Person\",\n        \"identifier\": {\n          \"@id\": \"https://orcid.org/0000-0003-3432-2297\",\n          \"@type\": \"PropertyValue\",\n          \"propertyID\": \"https://registry.identifiers.org/registry/orcid\",\n          \"url\": \"https://orcid.org/0000-0003-3432-2297\",\n          \"value\": \"orcid:0000-0003-3432-2297\"\n        },\n        \"name\": \"Dr Mark Brzezinski\",\n        \"url\": \"https://www.sample-data-repository.org/person/50663\"\n      }\n    }\n}\n</pre> <p>NOTE that the Role inherits the property <code>creator</code> and <code>contributor</code> from the Dataset when pointing to the schema:Person.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  ...\n  \"creator\": [\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472036\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Principal Investigator\",\n      \"url\": \"https://www.sample-data-repository.org/person-role/472036\",\n      \"creator\": {\n        \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n        \"@type\": \"Person\",\n        \"name\": \"Dr Uta Passow\",\n        \"givenName\": \"Uta\",\n        \"familyName\": \"Passow\",\n        \"url\": \"https://www.sample-data-repository.org/person/51317\"\n      }\n    }\n}\n</pre> <p>If a single Person plays multiple roles on a Dataset, each role should be explicitly defined in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n  \"creator\": [\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472036\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Principal Investigator\",\n      \"url\": \"https://www.sample-data-repository.org/person-role/472036\",\n      \"creator\": {\n        \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n        \"@type\": \"Person\",\n        \"name\": \"Dr Uta Passow\",\n        \"givenName\": \"Uta\",\n        \"familyName\": \"Passow\",\n        \"url\": \"https://www.sample-data-repository.org/person/51317\"\n      }\n    },\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472037\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Contact\",\n      \"url\": \"https://www.sample-data-repository.org/person-role/472037\",\n      \"creator\": { \"@id\": \"https://www.sample-data-repository.org/person/51317\" }\n    },\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person-role/472038\",\n      \"@type\": \"Role\",\n      \"roleName\": \"Co-Principal Investigator\",\n      \"url\": \"https://www.sample-data-repository.org/person-role/472038\",\n      \"creator\": {\n        \"@id\": \"https://www.sample-data-repository.org/person/50663\",\n        \"@type\": \"Person\",\n        \"identifier\": {\n          \"@id\": \"https://orcid.org/0000-0003-3432-2297\",\n          \"@type\": \"PropertyValue\",\n          \"propertyID\": \"https://registry.identifiers.org/registry/orcid\",\n          \"url\": \"https://orcid.org/0000-0003-3432-2297\",\n          \"value\": \"orcid:0000-0003-3432-2297\"\n        },\n        \"name\": \"Dr Mark Brzezinski\",\n        \"url\": \"https://www.sample-data-repository.org/person/50663\"\n      }\n    }\n}\n</pre> <p>Notice that since Uta Passow has already been defined in the document with <code>\"@id\": \"https://www.sample-data-repository.org/person/51317\"</code> for her role as Principal Investigator, the <code>@id</code> can be used for her role as Contact by defining the Role's creator as <code>\"creator\": { \"@id\": \"https://www.sample-data-repository.org/person/51317\" }</code>.</p> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#publisher-and-provider","title":"Publisher and Provider","text":"<p>If your repository is the publisher and/or provider of the dataset then you don't have to describe your repository as a schema:Organization if your repository markup includes the <code>@id</code>. For example, if you published repository markup such as:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Service\", \"Organization\"],\n  ...\n  \"@id\": \"https://www.sample-data-repository.org\"\n  ...\n}\n</pre> <p>then you can reuse that <code>@id</code> here. Harvesters such as Google and Project418 will make the appropriate linkages and your dataset publisher/provider can be published in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n\"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  },\n  \"publisher\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  }\n}\n</pre> <p>Otherwise, you can define the organization inline in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  ...\n\"provider\": {\n    \"@id\": \"https://www.sample-data-repository.org\",\n    \"@type\": \"Organization\",\n    \"legalName\": \"Sample Data Repository Office\",\n    \"name\": \"SDRO\",\n    \"sameAs\": \"http://www.re3data.org/repository/r3dxxxxxxxxx\",\n    \"url\": \"https://www.sample-data-repository.org\"\n  },\n  \"publisher\": {\n    \"@id\": \"https://www.sample-data-repository.org\"\n  }\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#funding","title":"Funding","text":"<p>Data providers should include funding information in their Dataset descriptions to enable discovery and cross-linking. The information that would be useful includes the title, identifier, and url of the grant or award, along with structured information about the funding organization, including its name and identifier. Organizational identifiers are best represented using either a general purpose institutional identifier such as a ROR, GRID, or ISNI identifier, or a more specific Funder ID from the Crossref Funder Registry. The ROR for the National Science Foundation (https://ror.org/021nxhr62), for example, provides linkages to related identifiers as well. The Funder ID has the advantage that it includes both agency funders like the National Science Foundation (http://dx.doi.org/10.13039/100000001), but also provides identifiers for individual funding programs within those agencies, such as the NSF GEO Directorate (https://api.crossref.org/funders/100000085). When possible, providing both a ROR and Funder ID is helpful. Here's an example of identifiers for the National Science Foundation:</p> <p></p> <p>Linking a Dataset to the grants and awards that fund it can be acheived by adding a schema:MonetaryGrant through the <code>schema:funding</code> property. </p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"@id\": \"https://doi.org/10.18739/A22V2CB44\",\n  \"name\": \"Stable water isotope data from Arctic Alaska snow pits in 2019\",\n\n  \"funding\": [\n    {\n      \"@id\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1604105\",\n      \"@type\": \"MonetaryGrant\",\n      \"identifier\": \"1604105\",\n      \"name\": \"Collaborative Research: Nutritional Landscapes of Arctic Caribou: Observations, Experiments, and Models Provide Process-Level Understanding of Forage Traits and Trajectories\",\n      \"url\": \"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1604105\",\n      \"funder\": {\n        \"@id\": \"http://dx.doi.org/10.13039/100000001\",\n        \"@type\": \"Organization\",\n        \"name\": \"National Science Foundation\",\n        \"identifier\": [\n          \"http://dx.doi.org/10.13039/100000001\",\n          \"https://ror.org/021nxhr62\"\n        ]\n      }\n    },\n    {\n      \"@type\": \"MonetaryGrant\",\n      \"@id\": \"https://akareport.aka.fi/ibi_apps/WFServlet?IBIF_ex=x_hakkuvaus2&amp;HAKNRO1=316349&amp;UILANG=en&amp;TULOSTE=HTML\",\n      \"identifier\": \"316349\",\n      \"name\": \"Where does water go when snow melts? New spatio-temporal resolution in stable water isotopes measurements to inform cold climate hydrological modelling\",        \n      \"url\": \"https://akareport.aka.fi/ibi_apps/WFServlet?IBIF_ex=x_hakkuvaus2&amp;HAKNRO1=316349&amp;UILANG=en&amp;TULOSTE=HTML\",\n      \"funder\": {\n        \"@id\": \"http://dx.doi.org/10.13039/501100002341\",\n        \"@type\": \"Organization\",\n        \"name\": \"Academy of Finland\",\n        \"identifier\": [\n          \"http://dx.doi.org/10.13039/501100002341\",\n          \"https://ror.org/05k73zm37\"\n        ]\n      }\n    }\n  ]\n\n}\n</pre> <p>We recommend providing as much structured information about the grants that fund a Dataset as possible so that aggregators and harvesters can crosslink to the Funding agencies and grants that provided resources for the Dataset.</p> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#license","title":"License","text":"<p>Link a Dataset to its license to document legal constraints by adding a schema:license property. The guide recommends providing a URL that unambiguously identifies a specific version of the license used, but for many licenses it is hard to determine what that URL should be. Thus, we recommend that the license URL be drawn from the SPDX license list, which provides a curated list of licenses and their properties that is well maintained. For each SPDX entry, SPDX provides a canonical URL for the license (e.g., <code>http://spdx.org/licenses/CC0-1.0</code>), a unique <code>licenseId</code> (e.g., <code>CC0-1.0</code>), and other metadata about the license. Here's an example using the SPDX license URI for the Creative Commons CC-0 license:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@id\": \"http://www.sample-data-repository.org/dataset/123\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"license\": \"http://spdx.org/licenses/CC0-1.0\"\n  ...\n}\n</pre> <p>SPDX URIs for each license can be found by finding the appropriate license in the SPDX license list, and then remove the final <code>.html</code> extension from the filename.  For example, in the table one can find the license page for Apache at the URI <code>https://spdx.org/licenses/Apache-2.0.html</code>, which can be converted into the associated linked data URI by removing the <code>.html</code>, leaving us with <code>https://spdx.org/licenses/Apache-2.0</code>. Alternatively, one can find the license file in the structured data listings and copy the URL from the associated file. For example, the URL for the Apache-2.0 license is listed in the file at https://github.com/spdx/license-list-data/blob/master/rdfturtle/Apache-2.0.turtle.</p> <p>While many licenses are ambiguous about the license URI for the license, the Creative Commons licenses and a few others are exceptions in that they provide extremely consistent URIs for each license, and these are in widespread use.  So, while we recommend using the SPDX URI, we recognize that some sites may want to use the CC license URIs directly, which is helpful in recognizing the license.  In this case, we recommend that the SPDX URI still be used as described above, and the other URI also be provided as well in a list. Here's an example using the traditional Creative Commons URI along with the SPDX URI.</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@id\": \"http://www.sample-data-repository.org/dataset/123\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"license\": [ \"http://spdx.org/licenses/CC0-1.0\", \"https://creativecommons.org/publicdomain/zero/1.0\"]\n  ...\n}\n</pre> <p>The following table contains the SPDX URIs for some of the most common licenses.  Others can be looked up at the SPDX site as described above.</p> License SPDX URI Apache-2.0 https://spdx.org/licenses/Apache-2.0 BSD-3-Clause https://spdx.org/licenses/BSD-3-Clause CC-BY-3.0 https://spdx.org/licenses/CC-BY-3.0 CC-BY-4.0 https://spdx.org/licenses/CC-BY-4.0 CC-BY-SA-4.0 https://spdx.org/licenses/CC-BY-SA-4.0 CC0-1.0 https://spdx.org/licenses/CC0-1.0 GPL-3.0-only https://spdx.org/licenses/GPL-3.0-only GPL-3.0-or-later https://spdx.org/licenses/GPL-3.0-or-later MIT https://spdx.org/licenses/MIT MIT-0 https://spdx.org/licenses/MIT-0 <p>Back to top</p>"},{"location":"sos/guides/Dataset/#checksum","title":"Checksum","text":"<p>A <code>schema:Dataset</code> can be composed of multiple digital objects which are listed in the <code>schema:distribution</code> list. For each <code>schema:DataDownload</code>, it can be useful to provide an cryptographic checksum value (like SHA 256 or MD5) that can be used to characterize the contents of the object. Aggregators and distributors can use these values to verify that they have retrieved exactly the same content as the original provider made available, and that replica copies of an object are identical to the original, among other uses. Because schema.org does not contain a class for representing checksum values, by convention we recommend using the <code>spdx:checksum</code> property, which points at an <code>spdx:Checksum</code> instance that provides both the value of the checksum and the algorithm that was used to calculate the checksum.</p> <p>Here's an example that provides two different checksum values for a single digital object within a <code>schema:DataDownload</code> description. Note that providers will need to define the <code>spdx</code> prefix in their <code>@context</code> block in order to use the prefix as shown in the example.</p> <pre>\n{\n    \"@context\": [\n      \"https://schema.org/\",\n      {\n        \"spdx\": \"http://spdx.org/rdf/terms#\"\n      }\n    ],\n    \"@type\": \"Dataset\",\n    \"@id\": \"https://dataone.org/datasets/doi%3A10.18739%2FA2NK36607\",\n    \"sameAs\": \"https://doi.org/10.18739/A2NK36607\",\n    \"name\": \"Conductivity-Temperature-Depth (CTD) data along DBO5 (Distributed Biological Observatory - Barrow Canyon), from the 2009 Circulation, Cross-shelf Exchange, Sea Ice, and Marine Mammal Habitat on the Alaskan Beaufort Sea Shelf cruise on USCGC Healy (HLY0904)\",\n    \"distribution\": {\n        \"@type\": \"DataDownload\",\n        \"@id\": \"https://dataone.org/datasets/urn%3Euuid%3E2646d817-9897-4875-9429-9c196be5c2ae\",\n        \"identifier\": \"urn:uuid:2646d817-9897-4875-9429-9c196be5c2ae\",\n        \"spdx:checksum\": [\n            {\n                \"@type\": \"spdx:Checksum\",\n                \"spdx:checksumValue\": \"39ae639d33cea4a287198bbcdca5e6856e6607a7c91dc4c54348031be2ad4c51\",\n                \"spdx:algorithm\": {\n                    \"@id\": \"spdx:checksumAlgorithm_sha256\"\n                }\n            },\n            {\n                \"@type\": \"spdx:Checksum\",\n                \"spdx:checksumValue\": \"65d3616852dbf7b1a6d4b53b00626032\",\n                \"spdx:algorithm\": {\n                    \"@id\": \"spdx:checksumAlgorithm_md5\"\n                }\n            }\n        ]\n    }\n}\n</pre> <p>The algorithm property is chosen from the controlled SPDX vocabulary of checksum types, making it easy for processors to recalculate checksum values to verify them. Common algorithms that many providers would use include <code>spdx:checksumAlgorithm_sha256</code> and <code>spdx:checksumAlgorithm_md5</code>. Note specifically that the <code>spdx:checksumAlgorithm_sha256</code> value is inside of an <code>@id</code> property so that the SPDX namespace from the context definition is used to define the algorithm URI.</p> <p>Back to top</p>"},{"location":"sos/guides/Dataset/#provenance-relationships","title":"Provenance Relationships","text":"<p>High level relationships that link datasets based on their processing workflows and versioning relationships are critical for data consumers and search engines to link different versions of a schema:Dataset, to clarify when a dataset is derived from one or more source Datasets, and to specify linkages to the software and activities that created these derived datasets for reproducibility. Collectively, this is provenance information.</p> <p>The PROV-O recommendation provides the widely-adopted vocabulary for representing this type of provenance information, and should be used within Dataset descriptions, as most of the necessary provenance properties are currently missing from schema.org. The main exception is <code>schema:isBasedOn</code>, which provides a predicate for indicating that a Dataset was derived from one or more source Datasets. Producers and consumers should interpret <code>schema:isBasedOn</code> to be an equivalent property to <code>prov:wasDerivedFrom</code> (in the <code>owl:equivalentProperty</code> sense). Either is acceptable for representing derivation relationships, but there is utility in expressing the relationship with both predicates for consumers that might only be looking for one or the other. When other <code>PROV</code> predicates are used, it is preferred to use <code>prov:wasDerivedFrom</code> for consistency.</p> <p>We recommend providing provenance information about data processing workflows, data derivation relationships, and versioning information using PROV-O and schema.org predicates, and describe the structures to do this in the following subsections. Aggregators and search systems should use these properties to cluster and cross-link versions of Datasets, and to provide bi-directional linkages to source and derived data products.</p>"},{"location":"sos/guides/Dataset/#indicating-an-earlier-version-provwasrevisionof","title":"Indicating an earlier version: <code>prov:wasRevisionOf</code>","text":"<p>Link a Dataset to a prior version that it replaces by adding a <code>prov:wasRevisionOf</code> property. This indicates that the current <code>schema:Dataset</code> replaces or obsoletes the source Dataset indicated.  The value of the <code>prov:wasRevisionOf</code> should be the canonical IRI for the identifier for the original dataset, preferably to a persistently resolvable IRI such as as a DOI, but other persistent identifiers for the dataset can be used.</p> <pre>\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"prov\": \"http://www.w3.org/ns/prov#\"\n    }\n  ],\n  \"@id\": \"https://doi.org/10.xxxx/Dataset-2.v2\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"prov:wasRevisionOf\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-2.v1\" }\n}\n</pre>"},{"location":"sos/guides/Dataset/#indicating-a-source-dataset-schemaisbasedon-and-provwasderivedfrom","title":"Indicating a source dataset: <code>schema:isBasedOn</code> and <code>prov:wasDerivedFrom</code>","text":"<p>A derived Dataset is one in which the values in the data are somehow related or created from the values in one or more source datasets. For example, raw voltage values from a sensor might be recorded in a raw data file, which is then processed through calibration functions to produce a derived dataset with values in scientific units. Other examples of derived data include data that has been error corrected, gap-filled, or integrated with other sources.</p> <p>To indicate that a Dataset has been derived from a source Dataset, use the <code>prov:wasDerivedFrom</code> property. This indicates that the current <code>schema:Dataset</code> was created in whole or in part from content in the source Dataset, and therefore does not represent an independent set of measurements.  The value of the <code>prov:wasDerivedFrom</code> should be the canonical IRI for the identifer for the source dataset, preferably to a persistently resolvable IRI such as as a DOI, but other persistent identifiers for the dataset can be used. In addition, if a persistent identifier for a digital object within a Dataset is available, the <code>prov:wasDerivedFrom</code> may also be used to indicate that that digital object was derived from that particular source object, rather than the overall Dataset. This allows one to be more specific about the exact relationship between the source and derived data objects.</p> <p>In addition to <code>prov:wasDerivedFrom</code>, schema.org provides the <code>schema:isBasedOn</code> property, which should be considered to be an equivalent property to <code>prov:wasDerivedFrom</code>. For compatibility with schema.org, we recommend that producers use <code>schema:isBasedOn</code> in addition to or instead of <code>prov:wasDerivedFrom</code> to indicate derivation relationships.</p> <p></p> <pre>\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"prov\": \"http://www.w3.org/ns/prov#\"\n    }\n  ],\n  \"@id\": \"https://doi.org/10.xxxx/Dataset-2\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"prov:wasDerivedFrom\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-1\" },\n  \"isBasedOn\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-1\" }\n}\n</pre>"},{"location":"sos/guides/Dataset/#indicating-a-software-workflow-or-processing-activity-provused-and-provwasgeneratedby","title":"Indicating a software workflow or processing activity: <code>prov:used</code> and <code>prov:wasGeneratedBy</code>","text":"<p>Frequently data are processed to create derived Datasets or other products using software programs that use some source data, transform it in various ways, and create the derived products. Understanding these software workflows promotes understanding of the products, and facilitates reproducibility. Describing a software workflow is really just a mechanism to provide more detail about how derived products were created when software was executed. The ProvONE vocabulary extends PROV to define a specific concept for an execution event (<code>provone:Execution</code>) during which a software program (<code>provone:Program</code>) is executed. During this execution, the software can use source data (<code>prov:used</code>) and generate outputs (<code>prov:wasGeneratedBy</code>), which then can be inferred to have been derived from the source data.</p> <p></p> <p>Any portion of the software workflow can be described to increase information about derived datasets. For example, use <code>prov:used</code> to link an execution to one or more source datasets, and use <code>prov:wasGeneratedBy</code> to link an execution to one or more derived products. When information about the execution event itself is known, use <code>provone:Execution</code> to describe that event, and link it to the source and derived products, as well as the program. The program is often a software script that is itself dereferenceable, and may be part of the archived Dataset itself if it has an accessible IRI.</p> <pre>\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"prov\": \"http://www.w3.org/ns/prov#\",\n      \"provone\": \"http://purl.dataone.org/provone/2015/01/15/ontology#\"\n    }\n  ],\n  \"@id\": \"https://doi.org/10.xxxx/Dataset-2\",\n  \"@type\": \"Dataset\",\n  \"name\": \"Removal of organic carbon by natural bacterioplankton communities as a function of pCO2 from laboratory experiments between 2012 and 2016\",\n  \"prov:wasDerivedFrom\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-1\" },\n  \"schema:isBasedOn\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-1\" },\n  \"prov:wasGeneratedBy\":\n      {\n        \"@id\": \"https://example.org/executions/execution-42\",\n        \"@type\": \"provone:Execution\",\n        \"prov:hadPlan\": \"https://somerepository.org/datasets/10.xxxx/Dataset-2.v2/process-script.R\",\n        \"prov:used\": { \"@id\": \"https://doi.org/10.xxxx/Dataset-1\" }\n      }\n}\n</pre> <p>Back to top</p>"},{"location":"sos/guides/GETTING-STARTED/","title":"Getting Started","text":"<p> Home | Getting Started</p>"},{"location":"sos/guides/GETTING-STARTED/#getting-started","title":"Getting Started","text":"<p>If you are new to publishing schema.org, here are some general tips to getting started.</p> <ul> <li>Goal</li> <li>Approach</li> <li>Prerequisites</li> <li>Introduction</li> <li>Using schema.org</li> <li>Modifying web pages to include schema.org as JSON-LD</li> <li>Specifying the context</li> <li>Provide a Sitemap</li> <li>Data Types<ul> <li>Text</li> <li>Number</li> <li>URL</li> <li>Boolean</li> <li>Date</li> <li>DateTime</li> <li>Time</li> <li>HTML</li> </ul> </li> <li>Resource Types</li> <li>Resource Modification Time</li> <li><code>schema.org/dateModified</code></li> <li>HTTP <code>Last-Modified</code></li> <li>Sitemap <code>&lt;lastmod&gt;</code></li> <li>JSON-LD Graph Techniques</li> <li>Ordered Lists</li> </ul>"},{"location":"sos/guides/GETTING-STARTED/#goals","title":"Goals","text":"<p>To provide a place for the scientific data community to work out how best to implement schema.org and other external vocabularies on web pages by publishing guidance documents. Pull requests and Github Issues are welcome!</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#approach","title":"Approach","text":"<ol> <li>To be pragmatic with our use of schema.org and external vocabulary adoption.</li> <li>To consider schema.org classes and properties first before considering external vocabularies.</li> <li>Use JSON-LD in our guidance documents for simplicity and terseness as compared to Microdata and RDFa. For more, see Why JSON-LD? from the Conventions document.</li> <li>Presently, the Google Rich Results Tool enforces use of schema.org classes and properties by displaying an error whenever external vocabularies are used. schema.org proposes linking to external vocabularies usuing the schema:additionalType property. While this property is defined as a sub property of rdf:type, it's data type is a literal. However, using the Schema.org Validator allows for the use of external vocabularies. We encourage the use of JSON-LD <code>'@type'</code> for typing classes to external vocabularies. For more, see Typing to External Vocabularies from the Conventions document.</li> <li>See Governance for how we will govern the project.</li> <li>See Conventions for guidance on creating/editing guidance documents.</li> </ol>"},{"location":"sos/guides/GETTING-STARTED/#prerequisites","title":"Prerequisites","text":"<ol> <li>We assume a general understanding of JSON.</li> <li>We assume a basic knowledge about JSON-LD.</li> </ol> <p>JSON-LD is valid JSON, so standard developer tools that support JSON can be used. For some specific JSON-LD and schema.org help though, there are some other resources.</p> <p>##### JSON-LD resources  https://json-ld.org   Generating the JSON-LD is best done via libraries like those you can find at https://json-ld.org.   There are libraries for; Javascript, Python, PHP, Ruby, Java, C# and Go.  While JSON-LD is just   JSON and can be generated many ways, these libraries   can generate valid JSON-LD spec output.   </p> <p>##### JSON-LD playground https://json-ld.org/playground/   The playground is hosted at the very useful JSON-LD web site site. You   can explore examples of JSON-LD and view how they convert to RDF, flatten, etc.   Note, that JSON-LD   is not associated with schema.org.  It can be used for much more and so most examples here don't   use schema.org and this site will NOT look to see if you are using schema.org types and properties   correctly.  Only that your JSON-LD is well formed.  </p> <ol> <li>We assume that you've heard about schema.org and have already decided that it's useful to you.</li> <li>We assume that you have a general understanding of what may describe a scientific dataset.</li> </ol> <p>Let's go!</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#introduction","title":"Introduction","text":"<p>There is an emerging practice to leverage structured metadata to aid in the discovery of web based resources.  Much of this work is taking place in the context (no pun intended) of schema.org.  This approach has extended to the resource type Dataset. This page will present approaches, tools and references that will aid in the understanding and development of schema.org in JSON-LD and its connection to external vocabularies.  For a more thorough presentation on this visit the Google AI Blog entry of January 24 2017 at https://ai.googleblog.com/2017/01/facilitating-discovery-of-public.html .</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#using-schemaorg","title":"Using schema.org","text":""},{"location":"sos/guides/GETTING-STARTED/#modifying-web-pages-to-include-schemaorg-as-json-ld","title":"Modifying web pages to include schema.org as JSON-LD","text":"<p>JSON-LD should be incorporated into the landing page html inside the <code>&lt;head&gt;&lt;/head&gt;</code> as a <code>&lt;script&gt;</code> element with a type of <code>application/ld+json</code>.  </p> <p><pre><code>&lt;html&gt;\n  &lt;head&gt;\n    ...\n    &lt;script id=\"schemaorg\" type=\"application/ld+json\"&gt;\n    {\n      \"@context\": \"https://schema.org/\",\n       \"@id\": \"http://opencoredata.org/id/dataset/bcd15975-680c-47db-a062-ac0bb6e66816\",\n       \"@type\": \"Dataset\",\n       \"description\": \"Janus Thermal Conductivity for ocean drilling ...\"\n    }\n    &lt;/script&gt;\n    ...\n  &lt;/head&gt;\n  ...\n&lt;/html&gt;\n ```\n\n&lt;a id=\"context\"&gt;&lt;/a&gt;\n### Specifying the `context`\n\nThe `context` in a JSON-LD document defines the namespaces used in the document and their mappings to URIs when they\nare referenced using prefix notation. The JSON-LD 1.1 specification \n[provides many rules](https://www.w3.org/TR/json-ld11-api/#remote-document-and-context-retrieval) that impact how the context is\nloaded and how it is retrieved, but ultimately the goal is to define a context map with the namespace mappings for each\nvocabulary used in the document. For the schema.org vocabulary specifically, the official namespace is `http://schema.org/`\n(note this is not an `https` URI), but the context file for schema.org can be retrieved from the `https` web location at\n`https://schema.org` by following the JSON-LD processing rules. For providers, this translates to a few simple recommendations.\n\n1. We recommend retrieving the context file from it's `https` location using the following syntax:\n</code></pre> {   \"@context\": \"https://schema.org/\",    \"@type\": \"Dataset\",    \"name\": \"Example dataset title\",    ... } <pre><code>\nUsing that approach, the schema.org namespace will be set to `http` URIs. For example the `Dataset` type will be expanded to\n`http://schema.org/Dataset`.\n\n2. Should you need to define additional namespaces in your context, it can be done by expanding the context using a JSON array as follows:\n\n```json\n{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"prov\": \"http://www.w3.org/ns/prov#\"\n    }\n  ],\n  \"@type\": \"Dataset\",\n  \"name\": \"Example dataset title\",\n  \"prov:wasDerivedFrom\": {\n    \"@id\": \"https://doi.org/10.xxxx/Dataset-1\"\n  }\n}</code></pre></p> <p>Note the square brackets, in which the first entry is the URL of a context file to be retrieved, and the second value is a JSON object to be combined with the retrieved context. This approach still retrieves the context from the secure <code>https</code> URL at schema.org, but then adds an additional namespace for the <code>prov</code> vocabulary to the context. Now, terms from the PROV namespace can be referenced using prefix notation (e.g., <code>prov:wasDerivedFrom</code>).</p> <ol> <li>Additional approaches to defining the context are possible, but users should use care to ensure that the terms within schema.org use the <code>http://schema.org/</code> namespace as defined in the official schema.org context file. Because contributors to schema.org are working towards accepting <code>https://schema.org/</code> as an equivalent namespace URI for all terms, processors should treat schema.org terms in the http and https URI spaces as equivalent, but providers might find it safer to continue to use <code>http://schema.org/</code> as the official namespace for now. This particularly applies when defining a default vocabulary for un-prefixed terms, in which case we recommend using <code>\"@vocab\": \"http://schema.org/\"</code> if this is necessary. That said, most user's should not have need to define <code>@vocab</code> in typical usage.</li> </ol> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#provide-a-sitemapxml-file","title":"Provide a Sitemap.xml file","text":"<p>Many harvesters and aggregators depend on the existence of a <code>sitemap.xml</code> file on your site that lists all of the dataset landing pages from your site that you want to be harvested and indexed for search. Google Dataset Search, DataONE, and Geocodes all can make use of a sitemap to more efficiently harvest your site. A sitemap is a simple text file that lists each page that you want harvested. This can contain any webpage, but in this context we specifically want to list pages that contain a <code>schema:Dataset</code> entry to be harvested. Here's an example <code>sitemap.xml</code> file listing two Dataset landing pages, along with their <code>lastmod</code> date:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt;\n  &lt;url&gt;\n    &lt;loc&gt;https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2GH9BB0Q&lt;/loc&gt;\n    &lt;lastmod&gt;2021-12-06&lt;/lastmod&gt;\n  &lt;/url&gt;\n  &lt;url&gt;\n    &lt;loc&gt;https://arcticdata.io/catalog/view/doi%3A10.18739%2FA2ST7DZ2Q&lt;/loc&gt;\n    &lt;lastmod&gt;2021-12-07T12:15:05Z&lt;/lastmod&gt;\n  &lt;/url&gt;\n&lt;/urlset&gt;</code></pre> <p>Note the <code>&lt;lastmod&gt;</code> field, which indicates the last date on which the page was modified and is formatted as a W3C DateTime and may vary in precision. Most harvesters will use that date along with the HTTP <code>Last-modified</code> header to determine if a page has changed since the last time that a harvest was attempted. Keeping accurate <code>lastmod</code> values can massively improve the efficiency of indexing your catalog, as only the few items that have changed will need to be indexed.</p> <p>Location: The sitemap.xml file can be located anywhere on your site that is above the path in the hierachy in which your pages are listed. Typically, the sitemap.xml is places at the root of the site, but other locations can be used as well. A great way to indicate to harvesters where your sitemap is located would be to include it in your <code>robots.txt</code> file at the root of your web site, which is bascially an instruction manual for harvesters. For example, you might have a robots.txt file with the following contents:</p> <pre>\nUser-agent: *\nSitemap: https://arcticdata.io/sitemap1.xml\n</pre> <p>Sitemaps are limited to 50,000 records and 50MB, so if your site is larger than that you can break up your sitemap into multiple files, linked together using a sitemap index. Details about <code>sitemap-index.xml</code> and other aspects of sitemaps are provided in the https://sitemaps.org site, as well as from the Google sitemap documentation.</p> <p>By providing a sitemap and advertising its location, you make it simple for harvesters to find and index your Dataset listings.</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#data-types","title":"Data Types","text":"<p>For each schema.org type, such as Person or Event, there are fields that let you specify more information about that type. Each of these fields has an expected data type that is defined in the documentation as you can see from Figure 1..</p> <p></p> <p> Figure 1. schema.org field data types The expected data type for each field appears in the middle column. The left column is the name of the field, the middle column is the data type, and the right column is the field's description. </p> <p>Every data type is either a resource or a literal. Resources refer to other schema.org types. For example a Dataset type has a field called author of which the data type can be either a Person or an Organization. Because Person and Organization are other schema.org \"types\" who have their own fields, they are called resources. In JSON-LD, you specify resources by using curly brackets <code>{}</code>:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"author\": {\n    \"@type\": \"Person\",\n    \"name\": \"Jane Goodall\"\n  }\n}\n</pre> <p>In the JSON-LD above, the 'author' is a resource of type 'Person'. Fields that simply have a value are called literal data types. For examples, the 'Person' type above has a 'name' of \"Jane Goodall\" - a literal text value.</p> <p>Schema.org defines six literal, or primitive,  data types: Text, Number, Boolean, Date, DateTime, and Time. Text has two special variations: URL and how to specify when text is actually HTML.  </p> <p>When using schema.org, literal data types are not not specified using curly brackets <code>{}</code> as these are resrved for specifying 'objects' or 'resources' such as other schema.org types like <code>Person</code>, <code>Organization</code>, etc. First, let's see how to use a primitive data type by using fields of CreativeWork, the superclass for Dataset.</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#text","title":"Text","text":"<p>Imagine we want to say the name of our Creative Work is \"Passenger Manifest for H.M.S. Titanic\". The name field of CreativeWork specifies that it expects Text as the data type. We would use it in this way:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\"\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#number","title":"Number","text":"<p>Let's say we want to specify the version number of our manifest using the version field of CreativeWork which expects a Number. To specify numbers in JSON-LD, we omit the quotations surrounding the value:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#url","title":"URL","text":"<p>Now, let's specify the URL of our manifest using the url field of CreativeWork, an inheritied field from Thing. This fields expects a valid URL represented as Text:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\"\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#boolean","title":"Boolean","text":"<p>Using the Boolean value, we can speficy that our manifest is accessible for free using the field isAccessibleForFree by using the text <code>true</code> or <code>false</code> and omitting the quotes:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#date","title":"Date","text":"<p>To specify the datePublished, which allows either a Date or DateTime, as a Date, we can use any ISO 8601 date format by wrapping the date in double-quotes:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true,\n  \"datePublished\": \"2018-07-29\"\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#datetime","title":"DateTime","text":"<p>To specify the dateModified as a DateTime, as a Date, we must follow the ISO 8601  format for combining date and time representations using the form <code>[-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]</code>:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true,\n  \"datePublished\": \"2018-07-29\",\n  \"dateModified\": \"2018-07-30T14:30Z\"\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#time","title":"Time","text":"<p>Time is a rarely-used data type because it must represent a point in time recurring on multiple days following the XML Schema definition using the form <code>hh:mm:ss[Z|(+|-)hh:mm]</code> (see XML schema for details).</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true,\n  \"datePublished\": \"2018-07-29\",\n  \"dateModified\": \"2018-07-30T14:30Z\"\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#html","title":"HTML","text":"<p>The HTML data type is a special variation of the <code>Text</code> data type. In some cases where <code>Text</code> is the expected data type, our actual data type may be HTML (because we are dealing with web pages). In this case, the schema.org JSON-LD context defines <code>HTML</code> to mean rdf:HTML, the data type for specifying that a string of text should be interpreted as HTML. Let's say that we have a description of our manifest and want to use the description field, but we have HTML inside that text. Using the text field as we did above for the <code>name</code> field, we would specify the <code>description</code> as:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true,\n  \"datePublished\": \"2018-07-29\",\n  \"dateModified\": \"2018-07-30T14:30Z\",\n  \"description\": \"&lt;h3&gt;Acquisition&lt;/h3&gt;&lt;p&gt;The data was acquired from an office outside of &lt;a href\\\"https://en.wikipedia.org/wiki/New_York_City\\\"&gt;New York City&lt;/a&gt;.\"\n}\n</pre> <p>However, to specify that the <code>description</code> field should be interpreted as HTML, you specify <code>description</code> as a resource, setting the <code>@type</code> of that resource to \"HTML\" and placing the HTML string in a JSON-LD property <code>@value</code>:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"CreativeWork\",\n  \"name\": \"Passenger Manifest for H.M.S. Titanic\",\n  \"version\": 1,\n  \"url\": \"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\",\n  \"isAccessibleForFree\": true,\n  \"datePublished\": \"2018-07-29\",\n  \"dateModified\": \"2018-07-30T14:30Z\",\n  \"description\": {\n    \"@type\": \"HTML\",\n    \"@value\": \"&lt;h3&gt;Acquisition&lt;/h3&gt;&lt;p&gt;The data was acquired from an office outside of &lt;a href\\\"https://en.wikipedia.org/wiki/New_York_City\\\"&gt;New York City&lt;/a&gt;.\"\n  }\n}\n</pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#resource-types","title":"Resource Types","text":"<p>All schema.org resources should make use of the <code>@type</code> property which 'classifies' the resources as a specific type. For example, an un-typed resource would look like:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"name\": \"My Dataset\"\n}\n</pre> <p>Even though the above resource has a name of 'My Dataset' harvesters are unaware that your intent was to classify it as a Dataset. Un-typed resources are not valid schema.org resources, and so they require the <code>@type</code> property:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": \"Dataset\",\n  \"name\": \"My Dataset\"\n}\n</pre> <p>In some cases, it useful to multi-type a resource. One example of this may be a data repository. A data repositotry is typically functioning as noth an 'Organization' that employs people and has an address, but it also functions as a 'Service' to its user community. To multi-type a resource, we use JSON arrays:</p> <pre>\n{\n  \"@context\": \"https://schema.org/\",\n  \"@type\": [\"Organization\", \"Service\"],\n  \"name\": \"My Data Repository\"\n}\n</pre> <p>All schema.org types may be found here.</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#time-of-resource-modification","title":"Time of resource modification","text":"<p>An indication of when a resource was modified is valuable to a consumer for a variety of reasons.</p> <p>A consumer tracking changes in a collection of <code>SO:Dataset</code> or similar resources being advertised with a <code>sitemap.xml</code> or similar mechanism has at least three timestamps that can be examined to determine if an already retrieved resource may have been modified: the <code>schema.org/dateModified</code> property in the JSON-LD, the <code>Last-Modified</code> time reported by the web server, and the <code>&lt;lastmod&gt;</code> time that may be reported in a <code>sitemap.xml</code> document.</p> <p>The <code>schema.org/dateModified</code> value should be considered authoritative for indicating when the resource was modified. The <code>Last-Modified</code> header should reflect the corresponding <code>schema.org/dateModified</code> entry. This property provides an important hint for consumers as to whether a cached copy of a resource should be updated for example. Similarly the <code>&lt;lastmod&gt;</code> entry should reflect the <code>Last-Modified</code> header and the <code>schema.org/dateModified</code> value.</p> <p>A typical pattern for a consumer interesting in synchronizing a cache of resource is:</p> <ol> <li>Examine the sitemap for new or updated entries using hints from <code>&lt;lastmod&gt;</code></li> <li>Retrieve the resource directly or by previewing with a HTTP HEAD request. A    <code>Last-Modified</code> provides a hint as to whether the resource should be retrieved.</li> <li>Examine the <code>schema.org/dateModified</code> property of the resource(s) extracted from the    resource.</li> </ol> <p>Providing accurate hints early in the process can reduce requirements for effectively sharing data resources.</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#1-schemaorgdatemodified","title":"1. <code>schema.org/dateModified</code>","text":"<p>Each <code>schema.org</code> instance derived from <code>schema.org/CreativeWork</code> may have a <code>dateModified</code> property to indicate \"The date on which the CreativeWork was most recently modified or when the item's entry was modified within a DataFeed.\" This property should be provided with any instance of <code>schema.org/Dataset</code> or any other <code>schema.org</code> entity published in a landing page or though other mechanisms. The JSON spec does not include a built-in type for date time values, however the general consensus and a sensible practices is to represent a date time value as a time zone aware ISO 8601 formatted string. For example:</p> <pre><code>{\n  \"dateModified\": \"2018-12-10T13:45:00.000Z\"\n}</code></pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#2-http-last-modified-header","title":"2. HTTP <code>Last-Modified</code> Header","text":"<p>A schema.org instance is typically embedded in a landing page or may be accessed directly as a JSON-LD document over the HTTP protocol. HTTP resource providers (i.e. web servers) may include a <code>Last-Modified</code> header which contains the date and time at which the origin server believes the resource was last modified. The format for the date value follows the RFC 2616 specification. For example:</p> <pre><code>Last-Modified: Mon, 10 Dec 2018 13:45:00 GMT</code></pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#3-sitemapxml-lastmod-value","title":"3. <code>sitemap.xml lastmod</code> value","text":"<p>A <code>sitemap.xml</code> document provides a mechanism for a resource server to advertise available resources. Each <code>&lt;url&gt;</code> element may include a <code>&lt;lastmod&gt;</code> tag to indicate when the resource identified by the <code>&lt;url&gt;/&lt;loc&gt;</code> was last modified. The specification is fairly loose, indicating that date in the W3C Datetime format of <code>YYYY-MM-DD</code> may be sufficient. However, for the purposes of content synchronization, a higher precision is desireable, and should be provided where possible. For example:</p> <pre><code>2018-12-10T13:45:00.000Z</code></pre> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#json-ld-graph-techniques","title":"JSON-LD Graph Techniques","text":"<p>JSON-LD documents represent a graph model, even though at times that graph is implicit rather than being named. Here are some techniques that may be useful when constructing such graphs.</p> <p></p>"},{"location":"sos/guides/GETTING-STARTED/#ordering-items-with-json-ld-list","title":"Ordering items with JSON-LD <code>@list</code>","text":"<p>Unlike plain JSON, collections in JSON-LD are unordered [1, 2]. In cases where ordering of items needs to be preserved, we can use the <code>@list</code> keyword to specify that order should be preserved for a collection.  Ordered lists would be important, for example, when a list of authors or creators should be ordered as intended when rendering a view of the metadata, or when a list of bounding box coordinates in an array need to come in a particular order.</p> <p>In the following example, the list of <code>creator</code> items is not ordered, and so client tools could return the creator names in any order, and different tools may return them in different orders. This would be problematic for building a citation, for example.</p> <p>Example 1. Ordering for this list of creators will not be preserved: <pre><code>{\n  \"@context\": \"https://schema.org/\",\n  \"@id\": \"unordered_01\",\n  \"@type\": \"Dataset\",\n  \"creator\": [\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n      \"@type\": \"Person\",\n      \"name\": \"Dr Uta Passow\"\n    },\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person/50663\",\n      \"@type\": \"Person\",\n      \"name\": \"Dr Mark Brzezinski\"\n    }\n  ]\n}</code></pre></p> <p>To order a list, use the JSON-LD <code>@list</code> keyword`, as shown in Example 2:</p> <p>Example 2. Order will be preserved for this list of creators: <pre><code>{\n  \"@context\": \"https://schema.org/\",\n  \"@id\": \"order_01\",\n  \"@type\": \"Dataset\",\n  \"creator\": {\n    \"@list\": [\n      {\n        \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n        \"@type\": \"Person\",\n        \"name\": \"Dr Uta Passow\"\n      },\n      {\n        \"@id\": \"https://www.sample-data-repository.org/person/50663\",\n        \"@type\": \"Person\",\n        \"name\": \"Dr Mark Brzezinski\"\n      }\n    ]\n  }\n}</code></pre></p> <p>Ordering may be specified globally within the document by specifying the container type in a context. For example, after retrieving the context file from schema.org, we can define the <code>schema:creator</code> to be a list container globally in the document using the <code>@container</code> property:</p> <p>Example 3. Ordering of a list of creators is preserved anywhere such a list appears within the context. <pre><code>{\n  \"@context\": [\n    \"https://schema.org/\",\n    {\n      \"creator\": {\n        \"@container\": \"@list\"\n      }\n    }\n  ],\n  \"@id\": \"order_02\",\n  \"@type\": \"Dataset\",\n  \"creator\": [\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person/51317\",\n      \"@type\": \"Person\",\n      \"name\": \"Dr Uta Passow\"\n    },\n    {\n      \"@id\": \"https://www.sample-data-repository.org/person/50663\",\n      \"@type\": \"Person\",\n      \"name\": \"Dr Mark Brzezinski\"\n    }\n  ]\n}</code></pre></p> <p>With this technique, ordering can be set once in the context using <code>@list</code>, and then order will be preserved anytime that concept is used in the document.</p>"},{"location":"metadata/docs/geocodesSearchProfile/","title":"ECO GeoCODES Profile","text":"<p>This document will evolve to provide guidance to groups on how to address elements of their metadata policy and procedures to address indexing by GeoCODES.  To provide some context we can look at other activities and see their relationships.   </p> <p></p> <ul> <li>Google Dev Guidance helps guide use in Google Dataset Search</li> <li>ESIP Science on Schema guides use of Type Dataset (https://schema.org/Dataset) </li> <li>EarthCube GeoCODES provides guidance on the application GeoCODES search</li> </ul> <p>An element of this is the leveraging of the graph to provide connections to other resources.  In particular the resources in the EarthCube Resource Registry graph.  However, links to other external resources like the EarthCube Throughput project are being developed. </p> <p>This work will be done in a way that can be applied to other groups (present and future) as well.  </p> <p>An example of the current approach for linking to the Resource Registry follows.  This is evolving at present as we explore how to connect this through to the user interface. </p> <p></p> <p>GeoCODES team will work to refine and publish this guidance along with machine workflows to validate JSON-LD data graphs with.  Likely via SHACL shape graphs.  </p>"},{"location":"metadata/docs/geocodesSearchProfile/#preview-of-these-recommendations-include","title":"Preview of these recommendations include:","text":""},{"location":"metadata/docs/geocodesSearchProfile/#on-the-server-side","title":"On the server side:","text":"<p>The following are items of use to the GeoCODES harvest and indexing process.  </p> <p>Sitemap or Sitemap index</p> <p>You can leverage a sitemap (up to 50K resources) or a sitemap index to go beyond that.  An index will point to multiple sitemaps.  Note, you can use the index pattern to point to different resource types too.  For example, a sitemap index can point to various sitemaps for tools/software, people, dataset, etc.  This is also completely compatible with Google and other commercial site index systems.  </p> <p>Leverage Sitemap with dates</p> <p>Adding the dates node and updating it will be useful as we move to an automated indexing pattern with GeoCODES.   If present we will use this to guide the architecture to do faster and less burdensome indexing.   Sites that do not provide this date node will need to be periodically completely indexed to ensure updates to metadata records are found and indexed.  </p> <p>Content negotiate</p> <p>Though note required, we do support content negotiation for accessing your resources.  This can be both faster and less stressful on servers.  You do not need to provide any guidance back to EarthCube if you implement this.</p> <p>Headless vs static</p> <p>Both static and dynamic inclusion of the JSON-LD data graph into the page DOM is supported.  EarthCube (and all commercial indexes) support this \"headless\" rendering of pages for indexing.   At present we do not have a good standards based way to communicate this to EarthCube.  We are addressing approaches, both in the indexing code and via the web architecture to do this.   We will provide further guidance, if necessary, as it develops on this point.   Providers should feel free to select either approach as it addresses their own criteria.  </p>"},{"location":"metadata/docs/geocodesSearchProfile/#in-the-graph","title":"In the graph:","text":"<p>The following are either important in the search UI or (in the case of variable measured) items we have talked about leveraging.   It is important to know that these are items used by the search application.  In many ways this is a subset of Science on Schema of importance to us in providing the GeoCODES search UI/UX.  </p> <p>ID</p> <p>When generating your JSON-LD be sure to include a top level @id that points to the metadata record of the resource you are describing.  This can be a DOI like a DataCite metadata record but it can also be the URL of the landing page hosting your JSON-LD and describing the resource.</p> <p>Descriptive text</p> <p>The initial search is focused mostly on text searches across the literal strings of the graph, then we conduct other graph connections as the query is processed into a set of results.   So, while it's important to publish the graph elements it is also important to provide descriptive text and keywords at every level of the JSON-LD graph.</p> <p>Distribution url</p> <p>Follow ESIP Science on Schema guidance here.  </p> <p>Ref: https://github.com/ESIPFed/science-on-schema.org/blob/master/guides/Dataset.md#distributions </p> <p>Encoding format</p> <p>Follow ESIP Science on Schema guidance here.  </p> <p>Ref: https://github.com/ESIPFed/science-on-schema.org/blob/master/guides/Dataset.md#distributions </p> <p>Variable measured </p> <p>This is a developing area and we will implement ESIP Science on Schema recommendations here.</p> <p>Spatial</p> <p>Many ways to do it, but GeoCODES may propose geosparql WKT approach</p>"},{"location":"metadata/integrationtesting/","title":"Integration Testing","text":"<p>The goal of this is to see that the data is loaded</p> <p>This intially takes an effort to approve results.</p> <p>After this, if the code in gleaner changes, then it will take time to  again, approve the results</p>"},{"location":"metadata/integrationtesting/#approving-results","title":"Approving Results.","text":"<p>in the integration directory, theere is an approved_files directory</p> <p>Files with the extenstion, .approved.txt represent approved results. When you run a set of tests, if there is a difference, then a  result with .received. will appear.</p> <p>Examine this file, diff the changes, if needed, and if you think  the result is ok mv the file to .approved.txt</p>"},{"location":"metadata/integrationtesting/#running","title":"Running","text":""},{"location":"metadata/integrationtesting/#environment-variables-can-be-used-to-run-in-locally-or-in-ci-env","title":"Environment variables can be used to run in locally or in CI env","text":"<pre><code> GC_SITEMAP_URL\nGC_GLCON\nGC_NABUFILE\nGC_GLNRFILE\nGC_GRAPH\nGC_REPO</code></pre> <pre><code>   def setUpClass(cls):\n        cls.glcon = \"/Users/valentin/development/dev_earthcube/gleanerio/gleaner/glcon_darwin\"\n        cls.nabuFile = \"../resources/configs/geocodesintegration/nabu\"\n        graphendpoint, nabucfg = getNabu(cls.nabuFile)\n        cls.glnFile = \"../resources/configs/geocodesintegration/gleaner\"\n        s3endpoint, bucket, glncfg = getGleaner(cls.glnFile)\n        cls.s3 = minio.Minio(s3endpoint)\n        cls.glncfg = glncfg\n\n        cls.bucket = bucket\n        cls.repo = \"geocodes_demo_datasets\"\n        cls.nabucfg = nabucfg\n        ep = mg.graphFromEndpoint(graphendpoint)\n        cls.graphendpoint = ep\n        cls.graph = mg(ep, \"citesting\")\n</code></pre>"},{"location":"metadata/integrationtesting/#from-pycharm","title":"From pycharm","text":"<p>select metadata_approval_tests.py run</p> <p>This will run approval tests and put results of 'failed' tests with a name of received. If tests match the approved.txt, then no 'received.txt' files will be generated.</p>"},{"location":"scheduler/docs/eco_deploy/","title":"ECO Scheduler Notes","text":"<p>Note</p> <p>these will need to become the gleanerio scheduler documentation.  for now these are rough. Images and graphics need to be loaded</p> <pre><code>flowchart TB\nPostgres_Container-- defined by --&gt; compose_project\nDagit_UI_Container-- defined by --&gt; compose_project\nDagster_Container  -- defined by --&gt; compose_project\nHeadless_Container -- defined by --&gt; compose_project\nconfigs_volume_Container -- defined by --&gt; compose_project\ncompose_project -- deployed to --&gt; docker_portainer\n\nGleaner_container -- image manual add --&gt; docker_portainer\nNabu_container -- image manual add --&gt; docker_portainer\n\nGleaner_container -- deployed by --&gt; Dagster_Container\nNabu_container -- deployed by --&gt; Dagster_Container\n\nGleaner_container--  deployed to --&gt; docker_portainer\nNabu_container--  deployed to --&gt; docker_portainer\n\nDagit_UI_Container -- Created by --&gt; Github_action\nDagster_Cotnainer -- Created by --&gt; Github_action\n\nNabuConfig.tgz -- Archive to --&gt; Nabu_container\nGleanerConfig.tfz -- Archive to --&gt; Gleaner_container\n\nNabuConfig.tgz -- Stored in s3 --&gt; s3\nGleanerConfig.tfz -- Stored in s3 --&gt; s3\n\nconfigs_volume_Container -- populates volume --&gt; dagster-project\ndagster-project -- has --&gt; gleanerConfig.yaml\ndagster-project -- has --&gt; nabuConfig.yaml\n</code></pre>"},{"location":"scheduler/docs/eco_deploy/#deploy","title":"Deploy","text":""},{"location":"scheduler/docs/eco_deploy/#deploy-dagster-in-portainer","title":"Deploy Dagster in Portainer","text":"<p>You will need to deploy dagster contiainers to portainer, for a docker swarm</p> <ol> <li>Pull scheduler repo</li> <li>cd dagster/implnets/deployment</li> <li>create a copy of envFile.env and edit env variables</li> <li>pull images for nabu and gleaner    <code>GLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest</code> and     <code>GLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest</code></li> <li>as noted as noted in (Compose, Environment and Docker API Assets), deploy the configuration to s3. </li> <li>create network and volumes needed <code>dagster_setup_docker.sh</code></li> <li>create a stack, from compose_project.yaml, with the env variables you set</li> </ol>"},{"location":"scheduler/docs/eco_deploy/#deploy-local","title":"Deploy Local","text":"<p>this runs a local compose</p> <ol> <li>cd dagster/implnets</li> <li><code>make eco-clean</code>and <code>make eco-generate</code></li> <li>cd dagster/implnets/deployment </li> <li>run dagster_setup_docker.sh </li> <li>run dagster_localrun.sh</li> </ol>"},{"location":"scheduler/docs/eco_deploy/#implementation-network-builder","title":"Implementation network builder","text":"<p>The work for building the dagster containers for a given implementation network starts in  the directory <code>scheduler/dagster/implnets</code>.  This has been automated for CI For local development, At this time most of this can be driven by the Makefile</p>"},{"location":"scheduler/docs/eco_deploy/#automated-ci-build","title":"AUTOMATED CI Build","text":"<p>The containers are now built by a github action, and stored in the nsf earhtcube dockerhub</p> <ol> <li>Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where    NETWORK is your implmentation network like eco, iow, etc. </li> <li>if on a branch that you want to build, be sure the branch is in to github action</li> <li>increment VERSION, and commit. </li> <li>watch actions</li> </ol>"},{"location":"scheduler/docs/eco_deploy/#local-development","title":"LOCAL DEVELOPMENT","text":"<ol> <li>Make sure your gleanerconfig.yaml file is in the configs/NETWORK directory where NETWORK is your implmentation network like eco, iow, etc. </li> <li>Check the VERSION file and make sure it has a value you want in it to be tagged to the containers.</li> <li><code>make eco-clean</code>  will remove any existing generated code from the ./generatedCode/implnet-NETWORK directory</li> <li><code>make eco-generate</code> will build the code new.  Set the -d N in the makefile to a value N that is the number    of days you want the runs to cycle over.  So 30 would mean they run once every 30 days.  If you want some providers    to index at different rates you currently need to go in and edit the associated provider schedules file editing the    line <code>@schedule(cron_schedule=\"0 12 * * 6\", job=implnet_job_amgeo, execution_timezone=\"US/Central\")</code> with a     cron value you want.</li> <li><code>make eco-build</code> builds the Docker images following the build file ./build/Docker file.  Note this uses the     command line argument <code>--build-arg implnet=eco</code> to set the implementation NETWORK so that the correct build code     from generatedCode/NETWORK is copied over</li> </ol>"},{"location":"scheduler/docs/eco_deploy/#compose-environment-and-docker-api-assets","title":"Compose, Environment and Docker API Assets","text":"<ol> <li>You will need the (or need to make) the portainer access token      from your https://portainer.geocodes-aws-dev.earthcube.org/#!/account</li> <li>You will need a valid Gleaner configuration file named gleanerconfig.yaml and a nabu config named nabuconfig.yaml </li> <li>You will need the schema.org context files places in a directory assets  get each of the http and https versions</li> <li><code>wget https://schema.org/version/latest/schemaorg-current-https.jsonld</code></li> <li><code>wget https://schema.org/version/latest/schemaorg-current-http.jsonld</code></li> <li>Generate the archive files for Gleaner and Nabu.  Note the path to the context files should map with what is in the configuration files</li> <li><code>tar -zcf ./archives/NabuCfg.tgz ./nabuconfig.yaml ./assets</code></li> <li><code>tar -zcf ./archives/GleanerCfg.tgz ./gleanerconfig.yaml ./assets</code></li> <li>The archives .tgz files named NabuCfg.tgz and GleanerCfg.tgz need to be copied to the schedule prefix    in your bucket used for Gleaner</li> <li><code>mc cp GleanerCfg.tgz NabuCfg.tgz  gleaner/scheduler/configs</code></li> <li>Make sure GLEANERIO_NABU_ARCHIVE_OBJECT and GLEANERIO_GLEANER_ARCHIVE_OBJECT reflect this location in the .env file</li> <li>Next you will need to build the scheduler containers for your implementation network. Push these containers    to your container registry of choice and make sure the values are set in the .env file and that    the containers are available to Portainer or will get pulled on use.   These are the image files in the     compose file and also the images notes in the environment variables GLEANERIO_GLEANER_IMAGE and GLEANERIO_NABU_IMAGE    in the .env file.</li> </ol> <p>At this point you are ready to move to your Docker or Portainer environment and deploy the  compose and environment files.  </p>"},{"location":"scheduler/docs/eco_deploy/#notes","title":"Notes","text":"<ol> <li>I do not have the API call to ensure/check/pull and image used by the API, so these images need to be     pulled down manually at this time.  These are the images noted by the .env files at     <code>GLEANERIO_GLEANER_IMAGE=nsfearthcube/gleaner:latest</code> and     <code>GLEANERIO_NABU_IMAGE=nsfearthcube/nabu:latest</code></li> </ol>"}]}